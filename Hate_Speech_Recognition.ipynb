{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cLaXG_dGoRG0vCKgOMWg-VHa_9IbTPtL",
      "authorship_tag": "ABX9TyNydPAg2tjIF3StvpxWmnlp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khyati51/Hate-Speech-Recognition/blob/main/Hate_Speech_Recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sg-MaQcECNp3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "id": "CC8m9JxEO2lY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94e88cb0-5e6d-48e6-d31b-1722dc39a763"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "id": "w24b5F8ECYh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0acd987e-e81e-4c04-d66f-918c7e8947ef"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.14.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = ['Comment Text', 'Comment Type']\n",
        "train = pd.read_csv('/content/NLP_ass_train.tsv',sep='\\t',names=column_names)\n",
        "valid = pd.read_csv('/content/NLP_ass_valid.tsv',sep='\\t',names=column_names)\n",
        "test = pd.read_csv('/content/NLP_ass_test.tsv',sep='\\t',names=column_names)\n",
        "train"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IDhXwLb3Cj5_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "df668eb7-57ab-434e-cb7d-641c6789cb37"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Comment Text Comment Type\n",
              "0      we cannot continue calling ourselves feminists...       normal\n",
              "1                          nawt yall niggers ignoring me       normal\n",
              "2      <user> i am bit confused coz chinese ppl can n...   hatespeech\n",
              "3      and this is why i end up with nigger trainee d...   hatespeech\n",
              "4                      nogs jews and dykes how enriching    offensive\n",
              "...                                                  ...          ...\n",
              "15378  this vile racist ghetto ass dumber than a reta...    offensive\n",
              "15379  when i first got on here and said i hate trump...    offensive\n",
              "15380  was macht der moslem wenn der zion gegen seine...       normal\n",
              "15381  it is awful look at world demographics asians ...   hatespeech\n",
              "15382  the jewish globalist elite have only imported ...    offensive\n",
              "\n",
              "[15383 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-72754f56-58b2-4e44-bc5d-cbe358216cd4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment Text</th>\n",
              "      <th>Comment Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>we cannot continue calling ourselves feminists...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nawt yall niggers ignoring me</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>and this is why i end up with nigger trainee d...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nogs jews and dykes how enriching</td>\n",
              "      <td>offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15378</th>\n",
              "      <td>this vile racist ghetto ass dumber than a reta...</td>\n",
              "      <td>offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15379</th>\n",
              "      <td>when i first got on here and said i hate trump...</td>\n",
              "      <td>offensive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15380</th>\n",
              "      <td>was macht der moslem wenn der zion gegen seine...</td>\n",
              "      <td>normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15381</th>\n",
              "      <td>it is awful look at world demographics asians ...</td>\n",
              "      <td>hatespeech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15382</th>\n",
              "      <td>the jewish globalist elite have only imported ...</td>\n",
              "      <td>offensive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15383 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-72754f56-58b2-4e44-bc5d-cbe358216cd4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-72754f56-58b2-4e44-bc5d-cbe358216cd4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-72754f56-58b2-4e44-bc5d-cbe358216cd4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d1bb5629-fa33-49e4-a129-4a032a7269dc\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d1bb5629-fa33-49e4-a129-4a032a7269dc')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d1bb5629-fa33-49e4-a129-4a032a7269dc button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_e7149388-f98c-43a1-b25f-474c7240e8a8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e7149388-f98c-43a1-b25f-474c7240e8a8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 15383,\n  \"fields\": [\n    {\n      \"column\": \"Comment Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15355,\n        \"samples\": [\n          \"this should not be called or even considered womensmarch real women do not lend themselves or rely on this new feminist bullshit its superflous judgejudy said it best when she owned katie couric these girls have no love let alone respect for themselves in return they recieve none slagfest\",\n          \"weev is a kike and ricky is a soy nigger\",\n          \"iranians and arabs lived in sweden when you were a kid i did not know thought i thought sweden was fairly homogenous until recently\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Comment Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"normal\",\n          \"hatespeech\",\n          \"offensive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text Pre Processing"
      ],
      "metadata": {
        "id": "CST-OQilDCQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "TVGZZvfOC9H1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4jXOAL4ECtZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download required NLTK resources if not already downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "qzP5gEW-DRtR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6e09348-4c93-407d-f303-472801b9fa26"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    #print(\"Original Text:\", text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "\n",
        "    print(\"Text after removing HTML tags:\", text)\n",
        "\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    text = re.sub('[^a-zA-Z]', ' ', text).lower()\n",
        "\n",
        "    print(\"Text after removing non-alphabetic characters and converting to lowercase:\", text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    words = word_tokenize(text)\n",
        "\n",
        "    print(\"Text after tokenization:\", words)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    print(\"Text after removing stop words:\", words)\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "\n",
        "    print(\"Text after Lemmatization:\", words)\n",
        "\n",
        "    # Combine words back into a single string\n",
        "    preprocessed_text = ' '.join(words)\n",
        "\n",
        "    print(\"Final pre-processed text:\", preprocessed_text)\n",
        "\n",
        "    return preprocessed_text"
      ],
      "metadata": {
        "id": "VbNrynIyDSYi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_text(\"This is Khyati Gupta, I am making this project:, related to NLP. I WILL BE learning all the techniques used in NLP and about RNN.\")"
      ],
      "metadata": {
        "id": "JNXY6frKDdoe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "outputId": "658152e9-7470-420d-fc74-4d6346af6a0b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text after removing HTML tags: This is Khyati Gupta, I am making this project:, related to NLP. I WILL BE learning all the techniques used in NLP and about RNN.\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this is khyati gupta  i am making this project   related to nlp  i will be learning all the techniques used in nlp and about rnn \n",
            "Text after tokenization: ['this', 'is', 'khyati', 'gupta', 'i', 'am', 'making', 'this', 'project', 'related', 'to', 'nlp', 'i', 'will', 'be', 'learning', 'all', 'the', 'techniques', 'used', 'in', 'nlp', 'and', 'about', 'rnn']\n",
            "Text after removing stop words: ['khyati', 'gupta', 'making', 'project', 'related', 'nlp', 'learning', 'techniques', 'used', 'nlp', 'rnn']\n",
            "Text after Lemmatization: ['khyati', 'gupta', 'making', 'project', 'related', 'nlp', 'learning', 'technique', 'used', 'nlp', 'rnn']\n",
            "Final pre-processed text: khyati gupta making project related nlp learning technique used nlp rnn\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'khyati gupta making project related nlp learning technique used nlp rnn'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "train['preprocessed_text'] = train['Comment Text'].apply(preprocess_text)\n",
        "test['preprocessed_text'] = test['Comment Text'].apply(preprocess_text)\n",
        "valid['preprocessed_text'] = valid['Comment Text'].apply(preprocess_text)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "b5_zPyieD1i7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8678c2f9-2aab-4cb3-86f1-e1af28c54d27"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Text after removing HTML tags: i am such a shady hoe omg\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am such a shady hoe omg\n",
            "Text after tokenization: ['i', 'am', 'such', 'a', 'shady', 'hoe', 'omg']\n",
            "Text after removing stop words: ['shady', 'hoe', 'omg']\n",
            "Text after Lemmatization: ['shady', 'hoe', 'omg']\n",
            "Final pre-processed text: shady hoe omg\n",
            "Text after removing HTML tags:  damn he actually retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  damn he actually retarded\n",
            "Text after tokenization: ['damn', 'he', 'actually', 'retarded']\n",
            "Text after removing stop words: ['damn', 'actually', 'retarded']\n",
            "Text after Lemmatization: ['damn', 'actually', 'retarded']\n",
            "Final pre-processed text: damn actually retarded\n",
            "Text after removing HTML tags: this bitch kenny got my key i can ’ t emuss lock the damn door\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this bitch kenny got my key i can   t emuss lock the damn door\n",
            "Text after tokenization: ['this', 'bitch', 'kenny', 'got', 'my', 'key', 'i', 'can', 't', 'emuss', 'lock', 'the', 'damn', 'door']\n",
            "Text after removing stop words: ['bitch', 'kenny', 'got', 'key', 'emuss', 'lock', 'damn', 'door']\n",
            "Text after Lemmatization: ['bitch', 'kenny', 'got', 'key', 'emu', 'lock', 'damn', 'door']\n",
            "Final pre-processed text: bitch kenny got key emu lock damn door\n",
            "Text after removing HTML tags:  i don ’ t take self help or career advice from white men or most white women most are too blind to poorer dynamics\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i don   t take self help or career advice from white men or most white women most are too blind to poorer dynamics\n",
            "Text after tokenization: ['i', 'don', 't', 'take', 'self', 'help', 'or', 'career', 'advice', 'from', 'white', 'men', 'or', 'most', 'white', 'women', 'most', 'are', 'too', 'blind', 'to', 'poorer', 'dynamics']\n",
            "Text after removing stop words: ['take', 'self', 'help', 'career', 'advice', 'white', 'men', 'white', 'women', 'blind', 'poorer', 'dynamics']\n",
            "Text after Lemmatization: ['take', 'self', 'help', 'career', 'advice', 'white', 'men', 'white', 'woman', 'blind', 'poorer', 'dynamic']\n",
            "Final pre-processed text: take self help career advice white men white woman blind poorer dynamic\n",
            "Text after removing HTML tags:  wow islam side by side w jews only  case in the entire world so what happened to all the jews in kolkata\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  wow islam side by side w jews only  case in the entire world so what happened to all the jews in kolkata\n",
            "Text after tokenization: ['wow', 'islam', 'side', 'by', 'side', 'w', 'jews', 'only', 'case', 'in', 'the', 'entire', 'world', 'so', 'what', 'happened', 'to', 'all', 'the', 'jews', 'in', 'kolkata']\n",
            "Text after removing stop words: ['wow', 'islam', 'side', 'side', 'w', 'jews', 'case', 'entire', 'world', 'happened', 'jews', 'kolkata']\n",
            "Text after Lemmatization: ['wow', 'islam', 'side', 'side', 'w', 'jew', 'case', 'entire', 'world', 'happened', 'jew', 'kolkata']\n",
            "Final pre-processed text: wow islam side side w jew case entire world happened jew kolkata\n",
            "Text after removing HTML tags:   you retarded asl\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   you retarded asl\n",
            "Text after tokenization: ['you', 'retarded', 'asl']\n",
            "Text after removing stop words: ['retarded', 'asl']\n",
            "Text after Lemmatization: ['retarded', 'asl']\n",
            "Final pre-processed text: retarded asl\n",
            "Text after removing HTML tags:  why didnt he ask why ndp attracts ecosocailists who love debt charles is a fucking pussy\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  why didnt he ask why ndp attracts ecosocailists who love debt charles is a fucking pussy\n",
            "Text after tokenization: ['why', 'didnt', 'he', 'ask', 'why', 'ndp', 'attracts', 'ecosocailists', 'who', 'love', 'debt', 'charles', 'is', 'a', 'fucking', 'pussy']\n",
            "Text after removing stop words: ['didnt', 'ask', 'ndp', 'attracts', 'ecosocailists', 'love', 'debt', 'charles', 'fucking', 'pussy']\n",
            "Text after Lemmatization: ['didnt', 'ask', 'ndp', 'attracts', 'ecosocailists', 'love', 'debt', 'charles', 'fucking', 'pussy']\n",
            "Final pre-processed text: didnt ask ndp attracts ecosocailists love debt charles fucking pussy\n",
            "Text after removing HTML tags:   i learn islamic education in schools and im still gay honey\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   i learn islamic education in schools and im still gay honey\n",
            "Text after tokenization: ['i', 'learn', 'islamic', 'education', 'in', 'schools', 'and', 'im', 'still', 'gay', 'honey']\n",
            "Text after removing stop words: ['learn', 'islamic', 'education', 'schools', 'im', 'still', 'gay', 'honey']\n",
            "Text after Lemmatization: ['learn', 'islamic', 'education', 'school', 'im', 'still', 'gay', 'honey']\n",
            "Final pre-processed text: learn islamic education school im still gay honey\n",
            "Text after removing HTML tags: no more wrestlemania week in new york cause the ghetto thee ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: no more wrestlemania week in new york cause the ghetto thee ghetto\n",
            "Text after tokenization: ['no', 'more', 'wrestlemania', 'week', 'in', 'new', 'york', 'cause', 'the', 'ghetto', 'thee', 'ghetto']\n",
            "Text after removing stop words: ['wrestlemania', 'week', 'new', 'york', 'cause', 'ghetto', 'thee', 'ghetto']\n",
            "Text after Lemmatization: ['wrestlemania', 'week', 'new', 'york', 'cause', 'ghetto', 'thee', 'ghetto']\n",
            "Final pre-processed text: wrestlemania week new york cause ghetto thee ghetto\n",
            "Text after removing HTML tags:  people who are responding to this tweet are retarded he was  they all say stupid things give them hell winnington\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  people who are responding to this tweet are retarded he was  they all say stupid things give them hell winnington\n",
            "Text after tokenization: ['people', 'who', 'are', 'responding', 'to', 'this', 'tweet', 'are', 'retarded', 'he', 'was', 'they', 'all', 'say', 'stupid', 'things', 'give', 'them', 'hell', 'winnington']\n",
            "Text after removing stop words: ['people', 'responding', 'tweet', 'retarded', 'say', 'stupid', 'things', 'give', 'hell', 'winnington']\n",
            "Text after Lemmatization: ['people', 'responding', 'tweet', 'retarded', 'say', 'stupid', 'thing', 'give', 'hell', 'winnington']\n",
            "Final pre-processed text: people responding tweet retarded say stupid thing give hell winnington\n",
            "Text after removing HTML tags: pray that my mom white blood count and platelets come up and stay up in jesus mighty name\n",
            "Text after removing non-alphabetic characters and converting to lowercase: pray that my mom white blood count and platelets come up and stay up in jesus mighty name\n",
            "Text after tokenization: ['pray', 'that', 'my', 'mom', 'white', 'blood', 'count', 'and', 'platelets', 'come', 'up', 'and', 'stay', 'up', 'in', 'jesus', 'mighty', 'name']\n",
            "Text after removing stop words: ['pray', 'mom', 'white', 'blood', 'count', 'platelets', 'come', 'stay', 'jesus', 'mighty', 'name']\n",
            "Text after Lemmatization: ['pray', 'mom', 'white', 'blood', 'count', 'platelet', 'come', 'stay', 'jesus', 'mighty', 'name']\n",
            "Final pre-processed text: pray mom white blood count platelet come stay jesus mighty name\n",
            "Text after removing HTML tags:  no like they retarded 😭\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  no like they retarded  \n",
            "Text after tokenization: ['no', 'like', 'they', 'retarded']\n",
            "Text after removing stop words: ['like', 'retarded']\n",
            "Text after Lemmatization: ['like', 'retarded']\n",
            "Final pre-processed text: like retarded\n",
            "Text after removing HTML tags:  he already admitted to being a guy just say you are gay and let this delusion die\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  he already admitted to being a guy just say you are gay and let this delusion die\n",
            "Text after tokenization: ['he', 'already', 'admitted', 'to', 'being', 'a', 'guy', 'just', 'say', 'you', 'are', 'gay', 'and', 'let', 'this', 'delusion', 'die']\n",
            "Text after removing stop words: ['already', 'admitted', 'guy', 'say', 'gay', 'let', 'delusion', 'die']\n",
            "Text after Lemmatization: ['already', 'admitted', 'guy', 'say', 'gay', 'let', 'delusion', 'die']\n",
            "Final pre-processed text: already admitted guy say gay let delusion die\n",
            "Text after removing HTML tags:  i ’ m pretty sure that black dude said punch nazis not punch every single white advocate ever\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i   m pretty sure that black dude said punch nazis not punch every single white advocate ever\n",
            "Text after tokenization: ['i', 'm', 'pretty', 'sure', 'that', 'black', 'dude', 'said', 'punch', 'nazis', 'not', 'punch', 'every', 'single', 'white', 'advocate', 'ever']\n",
            "Text after removing stop words: ['pretty', 'sure', 'black', 'dude', 'said', 'punch', 'nazis', 'punch', 'every', 'single', 'white', 'advocate', 'ever']\n",
            "Text after Lemmatization: ['pretty', 'sure', 'black', 'dude', 'said', 'punch', 'nazi', 'punch', 'every', 'single', 'white', 'advocate', 'ever']\n",
            "Final pre-processed text: pretty sure black dude said punch nazi punch every single white advocate ever\n",
            "Text after removing HTML tags:  the guy should be sacked basically calling brexit voters nazi\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  the guy should be sacked basically calling brexit voters nazi\n",
            "Text after tokenization: ['the', 'guy', 'should', 'be', 'sacked', 'basically', 'calling', 'brexit', 'voters', 'nazi']\n",
            "Text after removing stop words: ['guy', 'sacked', 'basically', 'calling', 'brexit', 'voters', 'nazi']\n",
            "Text after Lemmatization: ['guy', 'sacked', 'basically', 'calling', 'brexit', 'voter', 'nazi']\n",
            "Final pre-processed text: guy sacked basically calling brexit voter nazi\n",
            "Text after removing HTML tags:  now now you naughty little slut\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  now now you naughty little slut\n",
            "Text after tokenization: ['now', 'now', 'you', 'naughty', 'little', 'slut']\n",
            "Text after removing stop words: ['naughty', 'little', 'slut']\n",
            "Text after Lemmatization: ['naughty', 'little', 'slut']\n",
            "Final pre-processed text: naughty little slut\n",
            "Text after removing HTML tags: so are we gonna get let ’ get married or is baby boy the closest thing to it we ’ ll ever get  ghettobaby\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so are we gonna get let   get married or is baby boy the closest thing to it we   ll ever get  ghettobaby\n",
            "Text after tokenization: ['so', 'are', 'we', 'gon', 'na', 'get', 'let', 'get', 'married', 'or', 'is', 'baby', 'boy', 'the', 'closest', 'thing', 'to', 'it', 'we', 'll', 'ever', 'get', 'ghettobaby']\n",
            "Text after removing stop words: ['gon', 'na', 'get', 'let', 'get', 'married', 'baby', 'boy', 'closest', 'thing', 'ever', 'get', 'ghettobaby']\n",
            "Text after Lemmatization: ['gon', 'na', 'get', 'let', 'get', 'married', 'baby', 'boy', 'closest', 'thing', 'ever', 'get', 'ghettobaby']\n",
            "Final pre-processed text: gon na get let get married baby boy closest thing ever get ghettobaby\n",
            "Text after removing HTML tags: god i hope he does and is not being talked out of it by his poisonous advisers or his bitch daughter\n",
            "Text after removing non-alphabetic characters and converting to lowercase: god i hope he does and is not being talked out of it by his poisonous advisers or his bitch daughter\n",
            "Text after tokenization: ['god', 'i', 'hope', 'he', 'does', 'and', 'is', 'not', 'being', 'talked', 'out', 'of', 'it', 'by', 'his', 'poisonous', 'advisers', 'or', 'his', 'bitch', 'daughter']\n",
            "Text after removing stop words: ['god', 'hope', 'talked', 'poisonous', 'advisers', 'bitch', 'daughter']\n",
            "Text after Lemmatization: ['god', 'hope', 'talked', 'poisonous', 'adviser', 'bitch', 'daughter']\n",
            "Final pre-processed text: god hope talked poisonous adviser bitch daughter\n",
            "Text after removing HTML tags: are not we muslims stopshiitevictimization\n",
            "Text after removing non-alphabetic characters and converting to lowercase: are not we muslims stopshiitevictimization\n",
            "Text after tokenization: ['are', 'not', 'we', 'muslims', 'stopshiitevictimization']\n",
            "Text after removing stop words: ['muslims', 'stopshiitevictimization']\n",
            "Text after Lemmatization: ['muslim', 'stopshiitevictimization']\n",
            "Final pre-processed text: muslim stopshiitevictimization\n",
            "Text after removing HTML tags: being in a goofy relationship is my kind of thing cause my ass retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase: being in a goofy relationship is my kind of thing cause my ass retarded\n",
            "Text after tokenization: ['being', 'in', 'a', 'goofy', 'relationship', 'is', 'my', 'kind', 'of', 'thing', 'cause', 'my', 'ass', 'retarded']\n",
            "Text after removing stop words: ['goofy', 'relationship', 'kind', 'thing', 'cause', 'ass', 'retarded']\n",
            "Text after Lemmatization: ['goofy', 'relationship', 'kind', 'thing', 'cause', 'as', 'retarded']\n",
            "Final pre-processed text: goofy relationship kind thing cause as retarded\n",
            "Text after removing HTML tags: talking to them hoes you steady gossip yeah that bitch shit you a bitch nigga on my momma 😤\n",
            "Text after removing non-alphabetic characters and converting to lowercase: talking to them hoes you steady gossip yeah that bitch shit you a bitch nigga on my momma  \n",
            "Text after tokenization: ['talking', 'to', 'them', 'hoes', 'you', 'steady', 'gossip', 'yeah', 'that', 'bitch', 'shit', 'you', 'a', 'bitch', 'nigga', 'on', 'my', 'momma']\n",
            "Text after removing stop words: ['talking', 'hoes', 'steady', 'gossip', 'yeah', 'bitch', 'shit', 'bitch', 'nigga', 'momma']\n",
            "Text after Lemmatization: ['talking', 'hoe', 'steady', 'gossip', 'yeah', 'bitch', 'shit', 'bitch', 'nigga', 'momma']\n",
            "Final pre-processed text: talking hoe steady gossip yeah bitch shit bitch nigga momma\n",
            "Text after removing HTML tags:   fucking kill yourself\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   fucking kill yourself\n",
            "Text after tokenization: ['fucking', 'kill', 'yourself']\n",
            "Text after removing stop words: ['fucking', 'kill']\n",
            "Text after Lemmatization: ['fucking', 'kill']\n",
            "Final pre-processed text: fucking kill\n",
            "Text after removing HTML tags:  fix your game stupid retarded bitches\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  fix your game stupid retarded bitches\n",
            "Text after tokenization: ['fix', 'your', 'game', 'stupid', 'retarded', 'bitches']\n",
            "Text after removing stop words: ['fix', 'game', 'stupid', 'retarded', 'bitches']\n",
            "Text after Lemmatization: ['fix', 'game', 'stupid', 'retarded', 'bitch']\n",
            "Final pre-processed text: fix game stupid retarded bitch\n",
            "Text after removing HTML tags: you cannot espouse the idea that women do not belong in politics because politics is the state and the state is violence women get too emotional and then cry about the violence being done fuck sake stop fanboying my timeline\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you cannot espouse the idea that women do not belong in politics because politics is the state and the state is violence women get too emotional and then cry about the violence being done fuck sake stop fanboying my timeline\n",
            "Text after tokenization: ['you', 'can', 'not', 'espouse', 'the', 'idea', 'that', 'women', 'do', 'not', 'belong', 'in', 'politics', 'because', 'politics', 'is', 'the', 'state', 'and', 'the', 'state', 'is', 'violence', 'women', 'get', 'too', 'emotional', 'and', 'then', 'cry', 'about', 'the', 'violence', 'being', 'done', 'fuck', 'sake', 'stop', 'fanboying', 'my', 'timeline']\n",
            "Text after removing stop words: ['espouse', 'idea', 'women', 'belong', 'politics', 'politics', 'state', 'state', 'violence', 'women', 'get', 'emotional', 'cry', 'violence', 'done', 'fuck', 'sake', 'stop', 'fanboying', 'timeline']\n",
            "Text after Lemmatization: ['espouse', 'idea', 'woman', 'belong', 'politics', 'politics', 'state', 'state', 'violence', 'woman', 'get', 'emotional', 'cry', 'violence', 'done', 'fuck', 'sake', 'stop', 'fanboying', 'timeline']\n",
            "Final pre-processed text: espouse idea woman belong politics politics state state violence woman get emotional cry violence done fuck sake stop fanboying timeline\n",
            "Text after removing HTML tags: billie eilish is the mother of all depressed gay kids\n",
            "Text after removing non-alphabetic characters and converting to lowercase: billie eilish is the mother of all depressed gay kids\n",
            "Text after tokenization: ['billie', 'eilish', 'is', 'the', 'mother', 'of', 'all', 'depressed', 'gay', 'kids']\n",
            "Text after removing stop words: ['billie', 'eilish', 'mother', 'depressed', 'gay', 'kids']\n",
            "Text after Lemmatization: ['billie', 'eilish', 'mother', 'depressed', 'gay', 'kid']\n",
            "Final pre-processed text: billie eilish mother depressed gay kid\n",
            "Text after removing HTML tags: women women women\n",
            "Text after removing non-alphabetic characters and converting to lowercase: women women women\n",
            "Text after tokenization: ['women', 'women', 'women']\n",
            "Text after removing stop words: ['women', 'women', 'women']\n",
            "Text after Lemmatization: ['woman', 'woman', 'woman']\n",
            "Final pre-processed text: woman woman woman\n",
            "Text after removing HTML tags: if a bitch start scratching amp pulling my hair during a fight omm imma stomp on a hoe head dumb ass bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if a bitch start scratching amp pulling my hair during a fight omm imma stomp on a hoe head dumb ass bitch\n",
            "Text after tokenization: ['if', 'a', 'bitch', 'start', 'scratching', 'amp', 'pulling', 'my', 'hair', 'during', 'a', 'fight', 'omm', 'imma', 'stomp', 'on', 'a', 'hoe', 'head', 'dumb', 'ass', 'bitch']\n",
            "Text after removing stop words: ['bitch', 'start', 'scratching', 'amp', 'pulling', 'hair', 'fight', 'omm', 'imma', 'stomp', 'hoe', 'head', 'dumb', 'ass', 'bitch']\n",
            "Text after Lemmatization: ['bitch', 'start', 'scratching', 'amp', 'pulling', 'hair', 'fight', 'omm', 'imma', 'stomp', 'hoe', 'head', 'dumb', 'as', 'bitch']\n",
            "Final pre-processed text: bitch start scratching amp pulling hair fight omm imma stomp hoe head dumb as bitch\n",
            "Text after removing HTML tags:  oh look the nacho nazi is up an tweeting\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  oh look the nacho nazi is up an tweeting\n",
            "Text after tokenization: ['oh', 'look', 'the', 'nacho', 'nazi', 'is', 'up', 'an', 'tweeting']\n",
            "Text after removing stop words: ['oh', 'look', 'nacho', 'nazi', 'tweeting']\n",
            "Text after Lemmatization: ['oh', 'look', 'nacho', 'nazi', 'tweeting']\n",
            "Final pre-processed text: oh look nacho nazi tweeting\n",
            "Text after removing HTML tags: ramadan kareem to all my mutuals muslims and all muslims around the world 💖\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ramadan kareem to all my mutuals muslims and all muslims around the world  \n",
            "Text after tokenization: ['ramadan', 'kareem', 'to', 'all', 'my', 'mutuals', 'muslims', 'and', 'all', 'muslims', 'around', 'the', 'world']\n",
            "Text after removing stop words: ['ramadan', 'kareem', 'mutuals', 'muslims', 'muslims', 'around', 'world']\n",
            "Text after Lemmatization: ['ramadan', 'kareem', 'mutuals', 'muslim', 'muslim', 'around', 'world']\n",
            "Final pre-processed text: ramadan kareem mutuals muslim muslim around world\n",
            "Text after removing HTML tags: famous saying “ i felt sorry for myself because i had ragged shoes until i met a man who had no feet ” showgratitude\n",
            "Text after removing non-alphabetic characters and converting to lowercase: famous saying   i felt sorry for myself because i had ragged shoes until i met a man who had no feet   showgratitude\n",
            "Text after tokenization: ['famous', 'saying', 'i', 'felt', 'sorry', 'for', 'myself', 'because', 'i', 'had', 'ragged', 'shoes', 'until', 'i', 'met', 'a', 'man', 'who', 'had', 'no', 'feet', 'showgratitude']\n",
            "Text after removing stop words: ['famous', 'saying', 'felt', 'sorry', 'ragged', 'shoes', 'met', 'man', 'feet', 'showgratitude']\n",
            "Text after Lemmatization: ['famous', 'saying', 'felt', 'sorry', 'ragged', 'shoe', 'met', 'man', 'foot', 'showgratitude']\n",
            "Final pre-processed text: famous saying felt sorry ragged shoe met man foot showgratitude\n",
            "Text after removing HTML tags: ramadan kareem to you all both muslims and non muslims may this month brings peace happiness and brings people together\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ramadan kareem to you all both muslims and non muslims may this month brings peace happiness and brings people together\n",
            "Text after tokenization: ['ramadan', 'kareem', 'to', 'you', 'all', 'both', 'muslims', 'and', 'non', 'muslims', 'may', 'this', 'month', 'brings', 'peace', 'happiness', 'and', 'brings', 'people', 'together']\n",
            "Text after removing stop words: ['ramadan', 'kareem', 'muslims', 'non', 'muslims', 'may', 'month', 'brings', 'peace', 'happiness', 'brings', 'people', 'together']\n",
            "Text after Lemmatization: ['ramadan', 'kareem', 'muslim', 'non', 'muslim', 'may', 'month', 'brings', 'peace', 'happiness', 'brings', 'people', 'together']\n",
            "Final pre-processed text: ramadan kareem muslim non muslim may month brings peace happiness brings people together\n",
            "Text after removing HTML tags:   fuck you die already you shit spewing pussy grabber i fucking hate you hope you choke on an egg mcmuffin\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   fuck you die already you shit spewing pussy grabber i fucking hate you hope you choke on an egg mcmuffin\n",
            "Text after tokenization: ['fuck', 'you', 'die', 'already', 'you', 'shit', 'spewing', 'pussy', 'grabber', 'i', 'fucking', 'hate', 'you', 'hope', 'you', 'choke', 'on', 'an', 'egg', 'mcmuffin']\n",
            "Text after removing stop words: ['fuck', 'die', 'already', 'shit', 'spewing', 'pussy', 'grabber', 'fucking', 'hate', 'hope', 'choke', 'egg', 'mcmuffin']\n",
            "Text after Lemmatization: ['fuck', 'die', 'already', 'shit', 'spewing', 'pussy', 'grabber', 'fucking', 'hate', 'hope', 'choke', 'egg', 'mcmuffin']\n",
            "Final pre-processed text: fuck die already shit spewing pussy grabber fucking hate hope choke egg mcmuffin\n",
            "Text after removing HTML tags:   rascism is not just for blacks browns muslims asians people who hate white people are rascist\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   rascism is not just for blacks browns muslims asians people who hate white people are rascist\n",
            "Text after tokenization: ['rascism', 'is', 'not', 'just', 'for', 'blacks', 'browns', 'muslims', 'asians', 'people', 'who', 'hate', 'white', 'people', 'are', 'rascist']\n",
            "Text after removing stop words: ['rascism', 'blacks', 'browns', 'muslims', 'asians', 'people', 'hate', 'white', 'people', 'rascist']\n",
            "Text after Lemmatization: ['rascism', 'black', 'brown', 'muslim', 'asian', 'people', 'hate', 'white', 'people', 'rascist']\n",
            "Final pre-processed text: rascism black brown muslim asian people hate white people rascist\n",
            "Text after removing HTML tags:  you will be killed one day by our muslims inshaallah\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  you will be killed one day by our muslims inshaallah\n",
            "Text after tokenization: ['you', 'will', 'be', 'killed', 'one', 'day', 'by', 'our', 'muslims', 'inshaallah']\n",
            "Text after removing stop words: ['killed', 'one', 'day', 'muslims', 'inshaallah']\n",
            "Text after Lemmatization: ['killed', 'one', 'day', 'muslim', 'inshaallah']\n",
            "Final pre-processed text: killed one day muslim inshaallah\n",
            "Text after removing HTML tags: and the domestic violence doer beat the racist and they both earned millions for a single match of a pointless sport now back to our regularly scheduled lives\n",
            "Text after removing non-alphabetic characters and converting to lowercase: and the domestic violence doer beat the racist and they both earned millions for a single match of a pointless sport now back to our regularly scheduled lives\n",
            "Text after tokenization: ['and', 'the', 'domestic', 'violence', 'doer', 'beat', 'the', 'racist', 'and', 'they', 'both', 'earned', 'millions', 'for', 'a', 'single', 'match', 'of', 'a', 'pointless', 'sport', 'now', 'back', 'to', 'our', 'regularly', 'scheduled', 'lives']\n",
            "Text after removing stop words: ['domestic', 'violence', 'doer', 'beat', 'racist', 'earned', 'millions', 'single', 'match', 'pointless', 'sport', 'back', 'regularly', 'scheduled', 'lives']\n",
            "Text after Lemmatization: ['domestic', 'violence', 'doer', 'beat', 'racist', 'earned', 'million', 'single', 'match', 'pointless', 'sport', 'back', 'regularly', 'scheduled', 'life']\n",
            "Final pre-processed text: domestic violence doer beat racist earned million single match pointless sport back regularly scheduled life\n",
            "Text after removing HTML tags: the irony is if you are a straight white male in decent shape they are going to label you a nazi anyway might as well rock it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the irony is if you are a straight white male in decent shape they are going to label you a nazi anyway might as well rock it\n",
            "Text after tokenization: ['the', 'irony', 'is', 'if', 'you', 'are', 'a', 'straight', 'white', 'male', 'in', 'decent', 'shape', 'they', 'are', 'going', 'to', 'label', 'you', 'a', 'nazi', 'anyway', 'might', 'as', 'well', 'rock', 'it']\n",
            "Text after removing stop words: ['irony', 'straight', 'white', 'male', 'decent', 'shape', 'going', 'label', 'nazi', 'anyway', 'might', 'well', 'rock']\n",
            "Text after Lemmatization: ['irony', 'straight', 'white', 'male', 'decent', 'shape', 'going', 'label', 'nazi', 'anyway', 'might', 'well', 'rock']\n",
            "Final pre-processed text: irony straight white male decent shape going label nazi anyway might well rock\n",
            "Text after removing HTML tags: i ’ ve never watch it from the looks of the preview it looks retarded asf\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i   ve never watch it from the looks of the preview it looks retarded asf\n",
            "Text after tokenization: ['i', 've', 'never', 'watch', 'it', 'from', 'the', 'looks', 'of', 'the', 'preview', 'it', 'looks', 'retarded', 'asf']\n",
            "Text after removing stop words: ['never', 'watch', 'looks', 'preview', 'looks', 'retarded', 'asf']\n",
            "Text after Lemmatization: ['never', 'watch', 'look', 'preview', 'look', 'retarded', 'asf']\n",
            "Final pre-processed text: never watch look preview look retarded asf\n",
            "Text after removing HTML tags:   wbk they ’ re muslim kings\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   wbk they   re muslim kings\n",
            "Text after tokenization: ['wbk', 'they', 're', 'muslim', 'kings']\n",
            "Text after removing stop words: ['wbk', 'muslim', 'kings']\n",
            "Text after Lemmatization: ['wbk', 'muslim', 'king']\n",
            "Final pre-processed text: wbk muslim king\n",
            "Text after removing HTML tags: i think when people say black community or hispanic community it really means ghetto i notice no matter what race all ghetto people have the same character traits and ignorant attitude they are all violent and easily lead the media is pandering to their ignorance\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i think when people say black community or hispanic community it really means ghetto i notice no matter what race all ghetto people have the same character traits and ignorant attitude they are all violent and easily lead the media is pandering to their ignorance\n",
            "Text after tokenization: ['i', 'think', 'when', 'people', 'say', 'black', 'community', 'or', 'hispanic', 'community', 'it', 'really', 'means', 'ghetto', 'i', 'notice', 'no', 'matter', 'what', 'race', 'all', 'ghetto', 'people', 'have', 'the', 'same', 'character', 'traits', 'and', 'ignorant', 'attitude', 'they', 'are', 'all', 'violent', 'and', 'easily', 'lead', 'the', 'media', 'is', 'pandering', 'to', 'their', 'ignorance']\n",
            "Text after removing stop words: ['think', 'people', 'say', 'black', 'community', 'hispanic', 'community', 'really', 'means', 'ghetto', 'notice', 'matter', 'race', 'ghetto', 'people', 'character', 'traits', 'ignorant', 'attitude', 'violent', 'easily', 'lead', 'media', 'pandering', 'ignorance']\n",
            "Text after Lemmatization: ['think', 'people', 'say', 'black', 'community', 'hispanic', 'community', 'really', 'mean', 'ghetto', 'notice', 'matter', 'race', 'ghetto', 'people', 'character', 'trait', 'ignorant', 'attitude', 'violent', 'easily', 'lead', 'medium', 'pandering', 'ignorance']\n",
            "Final pre-processed text: think people say black community hispanic community really mean ghetto notice matter race ghetto people character trait ignorant attitude violent easily lead medium pandering ignorance\n",
            "Text after removing HTML tags:  non muslims can follow on muslim laws in shariah\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  non muslims can follow on muslim laws in shariah\n",
            "Text after tokenization: ['non', 'muslims', 'can', 'follow', 'on', 'muslim', 'laws', 'in', 'shariah']\n",
            "Text after removing stop words: ['non', 'muslims', 'follow', 'muslim', 'laws', 'shariah']\n",
            "Text after Lemmatization: ['non', 'muslim', 'follow', 'muslim', 'law', 'shariah']\n",
            "Final pre-processed text: non muslim follow muslim law shariah\n",
            "Text after removing HTML tags:  liberal retards go eat more tide pods lmao you have reached your pinnacle of absurdity and derangement yet\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  liberal retards go eat more tide pods lmao you have reached your pinnacle of absurdity and derangement yet\n",
            "Text after tokenization: ['liberal', 'retards', 'go', 'eat', 'more', 'tide', 'pods', 'lmao', 'you', 'have', 'reached', 'your', 'pinnacle', 'of', 'absurdity', 'and', 'derangement', 'yet']\n",
            "Text after removing stop words: ['liberal', 'retards', 'go', 'eat', 'tide', 'pods', 'lmao', 'reached', 'pinnacle', 'absurdity', 'derangement', 'yet']\n",
            "Text after Lemmatization: ['liberal', 'retard', 'go', 'eat', 'tide', 'pod', 'lmao', 'reached', 'pinnacle', 'absurdity', 'derangement', 'yet']\n",
            "Final pre-processed text: liberal retard go eat tide pod lmao reached pinnacle absurdity derangement yet\n",
            "Text after removing HTML tags: lol go back to sucking muslim refugee cock retarded whore\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lol go back to sucking muslim refugee cock retarded whore\n",
            "Text after tokenization: ['lol', 'go', 'back', 'to', 'sucking', 'muslim', 'refugee', 'cock', 'retarded', 'whore']\n",
            "Text after removing stop words: ['lol', 'go', 'back', 'sucking', 'muslim', 'refugee', 'cock', 'retarded', 'whore']\n",
            "Text after Lemmatization: ['lol', 'go', 'back', 'sucking', 'muslim', 'refugee', 'cock', 'retarded', 'whore']\n",
            "Final pre-processed text: lol go back sucking muslim refugee cock retarded whore\n",
            "Text after removing HTML tags:  w why is he a nazi\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  w why is he a nazi\n",
            "Text after tokenization: ['w', 'why', 'is', 'he', 'a', 'nazi']\n",
            "Text after removing stop words: ['w', 'nazi']\n",
            "Text after Lemmatization: ['w', 'nazi']\n",
            "Final pre-processed text: w nazi\n",
            "Text after removing HTML tags:  you want me to give up the list i created to give to you that ’ even more ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  you want me to give up the list i created to give to you that   even more ghetto\n",
            "Text after tokenization: ['you', 'want', 'me', 'to', 'give', 'up', 'the', 'list', 'i', 'created', 'to', 'give', 'to', 'you', 'that', 'even', 'more', 'ghetto']\n",
            "Text after removing stop words: ['want', 'give', 'list', 'created', 'give', 'even', 'ghetto']\n",
            "Text after Lemmatization: ['want', 'give', 'list', 'created', 'give', 'even', 'ghetto']\n",
            "Final pre-processed text: want give list created give even ghetto\n",
            "Text after removing HTML tags:   go one better and let it out free charge to some middle eastern refugees\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   go one better and let it out free charge to some middle eastern refugees\n",
            "Text after tokenization: ['go', 'one', 'better', 'and', 'let', 'it', 'out', 'free', 'charge', 'to', 'some', 'middle', 'eastern', 'refugees']\n",
            "Text after removing stop words: ['go', 'one', 'better', 'let', 'free', 'charge', 'middle', 'eastern', 'refugees']\n",
            "Text after Lemmatization: ['go', 'one', 'better', 'let', 'free', 'charge', 'middle', 'eastern', 'refugee']\n",
            "Final pre-processed text: go one better let free charge middle eastern refugee\n",
            "Text after removing HTML tags:  shut the fuck up nazi\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  shut the fuck up nazi\n",
            "Text after tokenization: ['shut', 'the', 'fuck', 'up', 'nazi']\n",
            "Text after removing stop words: ['shut', 'fuck', 'nazi']\n",
            "Text after Lemmatization: ['shut', 'fuck', 'nazi']\n",
            "Final pre-processed text: shut fuck nazi\n",
            "Text after removing HTML tags: concentrate on white genocide jews elites corrupt politicians etc and leave christianity alone\n",
            "Text after removing non-alphabetic characters and converting to lowercase: concentrate on white genocide jews elites corrupt politicians etc and leave christianity alone\n",
            "Text after tokenization: ['concentrate', 'on', 'white', 'genocide', 'jews', 'elites', 'corrupt', 'politicians', 'etc', 'and', 'leave', 'christianity', 'alone']\n",
            "Text after removing stop words: ['concentrate', 'white', 'genocide', 'jews', 'elites', 'corrupt', 'politicians', 'etc', 'leave', 'christianity', 'alone']\n",
            "Text after Lemmatization: ['concentrate', 'white', 'genocide', 'jew', 'elite', 'corrupt', 'politician', 'etc', 'leave', 'christianity', 'alone']\n",
            "Final pre-processed text: concentrate white genocide jew elite corrupt politician etc leave christianity alone\n",
            "Text after removing HTML tags:   some retard already made a like farming meme about him\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   some retard already made a like farming meme about him\n",
            "Text after tokenization: ['some', 'retard', 'already', 'made', 'a', 'like', 'farming', 'meme', 'about', 'him']\n",
            "Text after removing stop words: ['retard', 'already', 'made', 'like', 'farming', 'meme']\n",
            "Text after Lemmatization: ['retard', 'already', 'made', 'like', 'farming', 'meme']\n",
            "Final pre-processed text: retard already made like farming meme\n",
            "Text after removing HTML tags:  just rent an illegal alien to translate\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  just rent an illegal alien to translate\n",
            "Text after tokenization: ['just', 'rent', 'an', 'illegal', 'alien', 'to', 'translate']\n",
            "Text after removing stop words: ['rent', 'illegal', 'alien', 'translate']\n",
            "Text after Lemmatization: ['rent', 'illegal', 'alien', 'translate']\n",
            "Final pre-processed text: rent illegal alien translate\n",
            "Text after removing HTML tags:  the logic is insane to me there ’ literally white dominicans that are racist to black dominicans lmao\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  the logic is insane to me there   literally white dominicans that are racist to black dominicans lmao\n",
            "Text after tokenization: ['the', 'logic', 'is', 'insane', 'to', 'me', 'there', 'literally', 'white', 'dominicans', 'that', 'are', 'racist', 'to', 'black', 'dominicans', 'lmao']\n",
            "Text after removing stop words: ['logic', 'insane', 'literally', 'white', 'dominicans', 'racist', 'black', 'dominicans', 'lmao']\n",
            "Text after Lemmatization: ['logic', 'insane', 'literally', 'white', 'dominican', 'racist', 'black', 'dominican', 'lmao']\n",
            "Final pre-processed text: logic insane literally white dominican racist black dominican lmao\n",
            "Text after removing HTML tags: new to nsfw twitter •  • us • soft hard dom • single • lesbian • she her lt  rt for mutual\n",
            "Text after removing non-alphabetic characters and converting to lowercase: new to nsfw twitter      us   soft hard dom   single   lesbian   she her lt  rt for mutual\n",
            "Text after tokenization: ['new', 'to', 'nsfw', 'twitter', 'us', 'soft', 'hard', 'dom', 'single', 'lesbian', 'she', 'her', 'lt', 'rt', 'for', 'mutual']\n",
            "Text after removing stop words: ['new', 'nsfw', 'twitter', 'us', 'soft', 'hard', 'dom', 'single', 'lesbian', 'lt', 'rt', 'mutual']\n",
            "Text after Lemmatization: ['new', 'nsfw', 'twitter', 'u', 'soft', 'hard', 'dom', 'single', 'lesbian', 'lt', 'rt', 'mutual']\n",
            "Final pre-processed text: new nsfw twitter u soft hard dom single lesbian lt rt mutual\n",
            "Text after removing HTML tags: anyone else having a hard time discerning trolls from serious people the left has gone so retard it hard to tell\n",
            "Text after removing non-alphabetic characters and converting to lowercase: anyone else having a hard time discerning trolls from serious people the left has gone so retard it hard to tell\n",
            "Text after tokenization: ['anyone', 'else', 'having', 'a', 'hard', 'time', 'discerning', 'trolls', 'from', 'serious', 'people', 'the', 'left', 'has', 'gone', 'so', 'retard', 'it', 'hard', 'to', 'tell']\n",
            "Text after removing stop words: ['anyone', 'else', 'hard', 'time', 'discerning', 'trolls', 'serious', 'people', 'left', 'gone', 'retard', 'hard', 'tell']\n",
            "Text after Lemmatization: ['anyone', 'else', 'hard', 'time', 'discerning', 'troll', 'serious', 'people', 'left', 'gone', 'retard', 'hard', 'tell']\n",
            "Final pre-processed text: anyone else hard time discerning troll serious people left gone retard hard tell\n",
            "Text after removing HTML tags: he was so dirty brought the girl to her work events whew caucasians are a different kind of petty\n",
            "Text after removing non-alphabetic characters and converting to lowercase: he was so dirty brought the girl to her work events whew caucasians are a different kind of petty\n",
            "Text after tokenization: ['he', 'was', 'so', 'dirty', 'brought', 'the', 'girl', 'to', 'her', 'work', 'events', 'whew', 'caucasians', 'are', 'a', 'different', 'kind', 'of', 'petty']\n",
            "Text after removing stop words: ['dirty', 'brought', 'girl', 'work', 'events', 'whew', 'caucasians', 'different', 'kind', 'petty']\n",
            "Text after Lemmatization: ['dirty', 'brought', 'girl', 'work', 'event', 'whew', 'caucasian', 'different', 'kind', 'petty']\n",
            "Final pre-processed text: dirty brought girl work event whew caucasian different kind petty\n",
            "Text after removing HTML tags: i called my boyfriend a broke bitch one time n he still won ’ t unblock me i hate you la niggas 😭 😭 😭\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i called my boyfriend a broke bitch one time n he still won   t unblock me i hate you la niggas      \n",
            "Text after tokenization: ['i', 'called', 'my', 'boyfriend', 'a', 'broke', 'bitch', 'one', 'time', 'n', 'he', 'still', 'won', 't', 'unblock', 'me', 'i', 'hate', 'you', 'la', 'niggas']\n",
            "Text after removing stop words: ['called', 'boyfriend', 'broke', 'bitch', 'one', 'time', 'n', 'still', 'unblock', 'hate', 'la', 'niggas']\n",
            "Text after Lemmatization: ['called', 'boyfriend', 'broke', 'bitch', 'one', 'time', 'n', 'still', 'unblock', 'hate', 'la', 'nigga']\n",
            "Final pre-processed text: called boyfriend broke bitch one time n still unblock hate la nigga\n",
            "Text after removing HTML tags: lmao i still love myself but i ’ m kinda retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lmao i still love myself but i   m kinda retarded\n",
            "Text after tokenization: ['lmao', 'i', 'still', 'love', 'myself', 'but', 'i', 'm', 'kinda', 'retarded']\n",
            "Text after removing stop words: ['lmao', 'still', 'love', 'kinda', 'retarded']\n",
            "Text after Lemmatization: ['lmao', 'still', 'love', 'kinda', 'retarded']\n",
            "Final pre-processed text: lmao still love kinda retarded\n",
            "Text after removing HTML tags: i wonder if the refugee will see the irony as he is about to behead this one\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i wonder if the refugee will see the irony as he is about to behead this one\n",
            "Text after tokenization: ['i', 'wonder', 'if', 'the', 'refugee', 'will', 'see', 'the', 'irony', 'as', 'he', 'is', 'about', 'to', 'behead', 'this', 'one']\n",
            "Text after removing stop words: ['wonder', 'refugee', 'see', 'irony', 'behead', 'one']\n",
            "Text after Lemmatization: ['wonder', 'refugee', 'see', 'irony', 'behead', 'one']\n",
            "Final pre-processed text: wonder refugee see irony behead one\n",
            "Text after removing HTML tags:  so brave standing against the imaginary nazi\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  so brave standing against the imaginary nazi\n",
            "Text after tokenization: ['so', 'brave', 'standing', 'against', 'the', 'imaginary', 'nazi']\n",
            "Text after removing stop words: ['brave', 'standing', 'imaginary', 'nazi']\n",
            "Text after Lemmatization: ['brave', 'standing', 'imaginary', 'nazi']\n",
            "Final pre-processed text: brave standing imaginary nazi\n",
            "Text after removing HTML tags: as a kid i really thought america was hollywood amp celebrities when in reality it ’ just bulletproof backpacks and legalized nazi behaviour\n",
            "Text after removing non-alphabetic characters and converting to lowercase: as a kid i really thought america was hollywood amp celebrities when in reality it   just bulletproof backpacks and legalized nazi behaviour\n",
            "Text after tokenization: ['as', 'a', 'kid', 'i', 'really', 'thought', 'america', 'was', 'hollywood', 'amp', 'celebrities', 'when', 'in', 'reality', 'it', 'just', 'bulletproof', 'backpacks', 'and', 'legalized', 'nazi', 'behaviour']\n",
            "Text after removing stop words: ['kid', 'really', 'thought', 'america', 'hollywood', 'amp', 'celebrities', 'reality', 'bulletproof', 'backpacks', 'legalized', 'nazi', 'behaviour']\n",
            "Text after Lemmatization: ['kid', 'really', 'thought', 'america', 'hollywood', 'amp', 'celebrity', 'reality', 'bulletproof', 'backpack', 'legalized', 'nazi', 'behaviour']\n",
            "Final pre-processed text: kid really thought america hollywood amp celebrity reality bulletproof backpack legalized nazi behaviour\n",
            "Text after removing HTML tags:   not every muslims has will to kill every non muslim all i am saying is we must stand with them\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   not every muslims has will to kill every non muslim all i am saying is we must stand with them\n",
            "Text after tokenization: ['not', 'every', 'muslims', 'has', 'will', 'to', 'kill', 'every', 'non', 'muslim', 'all', 'i', 'am', 'saying', 'is', 'we', 'must', 'stand', 'with', 'them']\n",
            "Text after removing stop words: ['every', 'muslims', 'kill', 'every', 'non', 'muslim', 'saying', 'must', 'stand']\n",
            "Text after Lemmatization: ['every', 'muslim', 'kill', 'every', 'non', 'muslim', 'saying', 'must', 'stand']\n",
            "Final pre-processed text: every muslim kill every non muslim saying must stand\n",
            "Text after removing HTML tags:  gas the jews\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  gas the jews\n",
            "Text after tokenization: ['gas', 'the', 'jews']\n",
            "Text after removing stop words: ['gas', 'jews']\n",
            "Text after Lemmatization: ['gas', 'jew']\n",
            "Final pre-processed text: gas jew\n",
            "Text after removing HTML tags:  he doesn ’ t really hate immigrants not white ones anyway he ’ just giving his base what they want\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  he doesn   t really hate immigrants not white ones anyway he   just giving his base what they want\n",
            "Text after tokenization: ['he', 'doesn', 't', 'really', 'hate', 'immigrants', 'not', 'white', 'ones', 'anyway', 'he', 'just', 'giving', 'his', 'base', 'what', 'they', 'want']\n",
            "Text after removing stop words: ['really', 'hate', 'immigrants', 'white', 'ones', 'anyway', 'giving', 'base', 'want']\n",
            "Text after Lemmatization: ['really', 'hate', 'immigrant', 'white', 'one', 'anyway', 'giving', 'base', 'want']\n",
            "Final pre-processed text: really hate immigrant white one anyway giving base want\n",
            "Text after removing HTML tags: people on here be like  communist  nazi  taliban  ancap  nazbol\n",
            "Text after removing non-alphabetic characters and converting to lowercase: people on here be like  communist  nazi  taliban  ancap  nazbol\n",
            "Text after tokenization: ['people', 'on', 'here', 'be', 'like', 'communist', 'nazi', 'taliban', 'ancap', 'nazbol']\n",
            "Text after removing stop words: ['people', 'like', 'communist', 'nazi', 'taliban', 'ancap', 'nazbol']\n",
            "Text after Lemmatization: ['people', 'like', 'communist', 'nazi', 'taliban', 'ancap', 'nazbol']\n",
            "Final pre-processed text: people like communist nazi taliban ancap nazbol\n",
            "Text after removing HTML tags:  cool now do jews\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  cool now do jews\n",
            "Text after tokenization: ['cool', 'now', 'do', 'jews']\n",
            "Text after removing stop words: ['cool', 'jews']\n",
            "Text after Lemmatization: ['cool', 'jew']\n",
            "Final pre-processed text: cool jew\n",
            "Text after removing HTML tags: all these niggas my boyfriends but i ’ m not their girlfriend that ’ ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: all these niggas my boyfriends but i   m not their girlfriend that   ghetto\n",
            "Text after tokenization: ['all', 'these', 'niggas', 'my', 'boyfriends', 'but', 'i', 'm', 'not', 'their', 'girlfriend', 'that', 'ghetto']\n",
            "Text after removing stop words: ['niggas', 'boyfriends', 'girlfriend', 'ghetto']\n",
            "Text after Lemmatization: ['nigga', 'boyfriend', 'girlfriend', 'ghetto']\n",
            "Final pre-processed text: nigga boyfriend girlfriend ghetto\n",
            "Text after removing HTML tags: the jews put gab at risk anglin uses the site the way it intended and the jews came down on it if anglin does not have the right to mock the dead then none of us have the right to do anything except bow to israel\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the jews put gab at risk anglin uses the site the way it intended and the jews came down on it if anglin does not have the right to mock the dead then none of us have the right to do anything except bow to israel\n",
            "Text after tokenization: ['the', 'jews', 'put', 'gab', 'at', 'risk', 'anglin', 'uses', 'the', 'site', 'the', 'way', 'it', 'intended', 'and', 'the', 'jews', 'came', 'down', 'on', 'it', 'if', 'anglin', 'does', 'not', 'have', 'the', 'right', 'to', 'mock', 'the', 'dead', 'then', 'none', 'of', 'us', 'have', 'the', 'right', 'to', 'do', 'anything', 'except', 'bow', 'to', 'israel']\n",
            "Text after removing stop words: ['jews', 'put', 'gab', 'risk', 'anglin', 'uses', 'site', 'way', 'intended', 'jews', 'came', 'anglin', 'right', 'mock', 'dead', 'none', 'us', 'right', 'anything', 'except', 'bow', 'israel']\n",
            "Text after Lemmatization: ['jew', 'put', 'gab', 'risk', 'anglin', 'us', 'site', 'way', 'intended', 'jew', 'came', 'anglin', 'right', 'mock', 'dead', 'none', 'u', 'right', 'anything', 'except', 'bow', 'israel']\n",
            "Final pre-processed text: jew put gab risk anglin us site way intended jew came anglin right mock dead none u right anything except bow israel\n",
            "Text after removing HTML tags:   ah you ’ re a retard now i understand best of luck\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   ah you   re a retard now i understand best of luck\n",
            "Text after tokenization: ['ah', 'you', 're', 'a', 'retard', 'now', 'i', 'understand', 'best', 'of', 'luck']\n",
            "Text after removing stop words: ['ah', 'retard', 'understand', 'best', 'luck']\n",
            "Text after Lemmatization: ['ah', 'retard', 'understand', 'best', 'luck']\n",
            "Final pre-processed text: ah retard understand best luck\n",
            "Text after removing HTML tags: hodgepodge superfest is gay girls only event they really make the japanese house and betty who headline their minds\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hodgepodge superfest is gay girls only event they really make the japanese house and betty who headline their minds\n",
            "Text after tokenization: ['hodgepodge', 'superfest', 'is', 'gay', 'girls', 'only', 'event', 'they', 'really', 'make', 'the', 'japanese', 'house', 'and', 'betty', 'who', 'headline', 'their', 'minds']\n",
            "Text after removing stop words: ['hodgepodge', 'superfest', 'gay', 'girls', 'event', 'really', 'make', 'japanese', 'house', 'betty', 'headline', 'minds']\n",
            "Text after Lemmatization: ['hodgepodge', 'superfest', 'gay', 'girl', 'event', 'really', 'make', 'japanese', 'house', 'betty', 'headline', 'mind']\n",
            "Final pre-processed text: hodgepodge superfest gay girl event really make japanese house betty headline mind\n",
            "Text after removing HTML tags:  i like immigrants\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i like immigrants\n",
            "Text after tokenization: ['i', 'like', 'immigrants']\n",
            "Text after removing stop words: ['like', 'immigrants']\n",
            "Text after Lemmatization: ['like', 'immigrant']\n",
            "Final pre-processed text: like immigrant\n",
            "Text after removing HTML tags: i hate hoes dese dayz dey getting brave for no reazon\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i hate hoes dese dayz dey getting brave for no reazon\n",
            "Text after tokenization: ['i', 'hate', 'hoes', 'dese', 'dayz', 'dey', 'getting', 'brave', 'for', 'no', 'reazon']\n",
            "Text after removing stop words: ['hate', 'hoes', 'dese', 'dayz', 'dey', 'getting', 'brave', 'reazon']\n",
            "Text after Lemmatization: ['hate', 'hoe', 'dese', 'dayz', 'dey', 'getting', 'brave', 'reazon']\n",
            "Final pre-processed text: hate hoe dese dayz dey getting brave reazon\n",
            "Text after removing HTML tags:   muslims out there in kashmir were either openly supporting their genocide or kept silence yes they all are terrorists\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   muslims out there in kashmir were either openly supporting their genocide or kept silence yes they all are terrorists\n",
            "Text after tokenization: ['muslims', 'out', 'there', 'in', 'kashmir', 'were', 'either', 'openly', 'supporting', 'their', 'genocide', 'or', 'kept', 'silence', 'yes', 'they', 'all', 'are', 'terrorists']\n",
            "Text after removing stop words: ['muslims', 'kashmir', 'either', 'openly', 'supporting', 'genocide', 'kept', 'silence', 'yes', 'terrorists']\n",
            "Text after Lemmatization: ['muslim', 'kashmir', 'either', 'openly', 'supporting', 'genocide', 'kept', 'silence', 'yes', 'terrorist']\n",
            "Final pre-processed text: muslim kashmir either openly supporting genocide kept silence yes terrorist\n",
            "Text after removing HTML tags:  prince is not gay like i am pretty sure i heard he was a homophobe\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  prince is not gay like i am pretty sure i heard he was a homophobe\n",
            "Text after tokenization: ['prince', 'is', 'not', 'gay', 'like', 'i', 'am', 'pretty', 'sure', 'i', 'heard', 'he', 'was', 'a', 'homophobe']\n",
            "Text after removing stop words: ['prince', 'gay', 'like', 'pretty', 'sure', 'heard', 'homophobe']\n",
            "Text after Lemmatization: ['prince', 'gay', 'like', 'pretty', 'sure', 'heard', 'homophobe']\n",
            "Final pre-processed text: prince gay like pretty sure heard homophobe\n",
            "Text after removing HTML tags: thread sur  aka le negrophobe\n",
            "Text after removing non-alphabetic characters and converting to lowercase: thread sur  aka le negrophobe\n",
            "Text after tokenization: ['thread', 'sur', 'aka', 'le', 'negrophobe']\n",
            "Text after removing stop words: ['thread', 'sur', 'aka', 'le', 'negrophobe']\n",
            "Text after Lemmatization: ['thread', 'sur', 'aka', 'le', 'negrophobe']\n",
            "Final pre-processed text: thread sur aka le negrophobe\n",
            "Text after removing HTML tags: hi please teach your kids that retard\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hi please teach your kids that retard\n",
            "Text after tokenization: ['hi', 'please', 'teach', 'your', 'kids', 'that', 'retard']\n",
            "Text after removing stop words: ['hi', 'please', 'teach', 'kids', 'retard']\n",
            "Text after Lemmatization: ['hi', 'please', 'teach', 'kid', 'retard']\n",
            "Final pre-processed text: hi please teach kid retard\n",
            "Text after removing HTML tags: tonight klopp is the goat when he releases his line up v burnley he a nazi thats how it works\n",
            "Text after removing non-alphabetic characters and converting to lowercase: tonight klopp is the goat when he releases his line up v burnley he a nazi thats how it works\n",
            "Text after tokenization: ['tonight', 'klopp', 'is', 'the', 'goat', 'when', 'he', 'releases', 'his', 'line', 'up', 'v', 'burnley', 'he', 'a', 'nazi', 'thats', 'how', 'it', 'works']\n",
            "Text after removing stop words: ['tonight', 'klopp', 'goat', 'releases', 'line', 'v', 'burnley', 'nazi', 'thats', 'works']\n",
            "Text after Lemmatization: ['tonight', 'klopp', 'goat', 'release', 'line', 'v', 'burnley', 'nazi', 'thats', 'work']\n",
            "Final pre-processed text: tonight klopp goat release line v burnley nazi thats work\n",
            "Text after removing HTML tags: chickens are bitches dude\n",
            "Text after removing non-alphabetic characters and converting to lowercase: chickens are bitches dude\n",
            "Text after tokenization: ['chickens', 'are', 'bitches', 'dude']\n",
            "Text after removing stop words: ['chickens', 'bitches', 'dude']\n",
            "Text after Lemmatization: ['chicken', 'bitch', 'dude']\n",
            "Final pre-processed text: chicken bitch dude\n",
            "Text after removing HTML tags:   there ’ a lot of blatant racism and pure ignorance in this thread the ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   there   a lot of blatant racism and pure ignorance in this thread the ghetto\n",
            "Text after tokenization: ['there', 'a', 'lot', 'of', 'blatant', 'racism', 'and', 'pure', 'ignorance', 'in', 'this', 'thread', 'the', 'ghetto']\n",
            "Text after removing stop words: ['lot', 'blatant', 'racism', 'pure', 'ignorance', 'thread', 'ghetto']\n",
            "Text after Lemmatization: ['lot', 'blatant', 'racism', 'pure', 'ignorance', 'thread', 'ghetto']\n",
            "Final pre-processed text: lot blatant racism pure ignorance thread ghetto\n",
            "Text after removing HTML tags: dom fat ass will buy me food just to eat it i really fucking hate it here\n",
            "Text after removing non-alphabetic characters and converting to lowercase: dom fat ass will buy me food just to eat it i really fucking hate it here\n",
            "Text after tokenization: ['dom', 'fat', 'ass', 'will', 'buy', 'me', 'food', 'just', 'to', 'eat', 'it', 'i', 'really', 'fucking', 'hate', 'it', 'here']\n",
            "Text after removing stop words: ['dom', 'fat', 'ass', 'buy', 'food', 'eat', 'really', 'fucking', 'hate']\n",
            "Text after Lemmatization: ['dom', 'fat', 'as', 'buy', 'food', 'eat', 'really', 'fucking', 'hate']\n",
            "Final pre-processed text: dom fat as buy food eat really fucking hate\n",
            "Text after removing HTML tags: hearing a bitch ass mosquito but not being able to see it 🤬 🤬 🤬 i be going cross eyed uu i wanna kill it soo bad\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hearing a bitch ass mosquito but not being able to see it       i be going cross eyed uu i wanna kill it soo bad\n",
            "Text after tokenization: ['hearing', 'a', 'bitch', 'ass', 'mosquito', 'but', 'not', 'being', 'able', 'to', 'see', 'it', 'i', 'be', 'going', 'cross', 'eyed', 'uu', 'i', 'wan', 'na', 'kill', 'it', 'soo', 'bad']\n",
            "Text after removing stop words: ['hearing', 'bitch', 'ass', 'mosquito', 'able', 'see', 'going', 'cross', 'eyed', 'uu', 'wan', 'na', 'kill', 'soo', 'bad']\n",
            "Text after Lemmatization: ['hearing', 'bitch', 'as', 'mosquito', 'able', 'see', 'going', 'cross', 'eyed', 'uu', 'wan', 'na', 'kill', 'soo', 'bad']\n",
            "Final pre-processed text: hearing bitch as mosquito able see going cross eyed uu wan na kill soo bad\n",
            "Text after removing HTML tags: i kno all the bad bitches should ’ ve ask me to give u  now u stuck w a rat lmao\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i kno all the bad bitches should   ve ask me to give u  now u stuck w a rat lmao\n",
            "Text after tokenization: ['i', 'kno', 'all', 'the', 'bad', 'bitches', 'should', 've', 'ask', 'me', 'to', 'give', 'u', 'now', 'u', 'stuck', 'w', 'a', 'rat', 'lmao']\n",
            "Text after removing stop words: ['kno', 'bad', 'bitches', 'ask', 'give', 'u', 'u', 'stuck', 'w', 'rat', 'lmao']\n",
            "Text after Lemmatization: ['kno', 'bad', 'bitch', 'ask', 'give', 'u', 'u', 'stuck', 'w', 'rat', 'lmao']\n",
            "Final pre-processed text: kno bad bitch ask give u u stuck w rat lmao\n",
            "Text after removing HTML tags:   and the government is supposed to protect from nazis\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   and the government is supposed to protect from nazis\n",
            "Text after tokenization: ['and', 'the', 'government', 'is', 'supposed', 'to', 'protect', 'from', 'nazis']\n",
            "Text after removing stop words: ['government', 'supposed', 'protect', 'nazis']\n",
            "Text after Lemmatization: ['government', 'supposed', 'protect', 'nazi']\n",
            "Final pre-processed text: government supposed protect nazi\n",
            "Text after removing HTML tags:  the fuck is a refugee drama romance and why is a white woman with rabid teeth in it\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  the fuck is a refugee drama romance and why is a white woman with rabid teeth in it\n",
            "Text after tokenization: ['the', 'fuck', 'is', 'a', 'refugee', 'drama', 'romance', 'and', 'why', 'is', 'a', 'white', 'woman', 'with', 'rabid', 'teeth', 'in', 'it']\n",
            "Text after removing stop words: ['fuck', 'refugee', 'drama', 'romance', 'white', 'woman', 'rabid', 'teeth']\n",
            "Text after Lemmatization: ['fuck', 'refugee', 'drama', 'romance', 'white', 'woman', 'rabid', 'teeth']\n",
            "Final pre-processed text: fuck refugee drama romance white woman rabid teeth\n",
            "Text after removing HTML tags:  no don ’ t mind belligerent retards\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  no don   t mind belligerent retards\n",
            "Text after tokenization: ['no', 'don', 't', 'mind', 'belligerent', 'retards']\n",
            "Text after removing stop words: ['mind', 'belligerent', 'retards']\n",
            "Text after Lemmatization: ['mind', 'belligerent', 'retard']\n",
            "Final pre-processed text: mind belligerent retard\n",
            "Text after removing HTML tags:  a born again muslim is that like new socialism\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  a born again muslim is that like new socialism\n",
            "Text after tokenization: ['a', 'born', 'again', 'muslim', 'is', 'that', 'like', 'new', 'socialism']\n",
            "Text after removing stop words: ['born', 'muslim', 'like', 'new', 'socialism']\n",
            "Text after Lemmatization: ['born', 'muslim', 'like', 'new', 'socialism']\n",
            "Final pre-processed text: born muslim like new socialism\n",
            "Text after removing HTML tags: y ’ all are so so sick i hate you niggas\n",
            "Text after removing non-alphabetic characters and converting to lowercase: y   all are so so sick i hate you niggas\n",
            "Text after tokenization: ['y', 'all', 'are', 'so', 'so', 'sick', 'i', 'hate', 'you', 'niggas']\n",
            "Text after removing stop words: ['sick', 'hate', 'niggas']\n",
            "Text after Lemmatization: ['sick', 'hate', 'nigga']\n",
            "Final pre-processed text: sick hate nigga\n",
            "Text after removing HTML tags:  bitch that ’ why you use wipes gross\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  bitch that   why you use wipes gross\n",
            "Text after tokenization: ['bitch', 'that', 'why', 'you', 'use', 'wipes', 'gross']\n",
            "Text after removing stop words: ['bitch', 'use', 'wipes', 'gross']\n",
            "Text after Lemmatization: ['bitch', 'use', 'wipe', 'gross']\n",
            "Final pre-processed text: bitch use wipe gross\n",
            "Text after removing HTML tags:  you know that muslims are real brits right\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  you know that muslims are real brits right\n",
            "Text after tokenization: ['you', 'know', 'that', 'muslims', 'are', 'real', 'brits', 'right']\n",
            "Text after removing stop words: ['know', 'muslims', 'real', 'brits', 'right']\n",
            "Text after Lemmatization: ['know', 'muslim', 'real', 'brit', 'right']\n",
            "Final pre-processed text: know muslim real brit right\n",
            "Text after removing HTML tags: that fine but there are a lot of refugees on gab who do care about race cuz internet duopolists want to thoughtpolice racial nationalism among other wrongthoughts they want erased from our minds even if you do not care about race but care about free speech conscience you should stand with us\n",
            "Text after removing non-alphabetic characters and converting to lowercase: that fine but there are a lot of refugees on gab who do care about race cuz internet duopolists want to thoughtpolice racial nationalism among other wrongthoughts they want erased from our minds even if you do not care about race but care about free speech conscience you should stand with us\n",
            "Text after tokenization: ['that', 'fine', 'but', 'there', 'are', 'a', 'lot', 'of', 'refugees', 'on', 'gab', 'who', 'do', 'care', 'about', 'race', 'cuz', 'internet', 'duopolists', 'want', 'to', 'thoughtpolice', 'racial', 'nationalism', 'among', 'other', 'wrongthoughts', 'they', 'want', 'erased', 'from', 'our', 'minds', 'even', 'if', 'you', 'do', 'not', 'care', 'about', 'race', 'but', 'care', 'about', 'free', 'speech', 'conscience', 'you', 'should', 'stand', 'with', 'us']\n",
            "Text after removing stop words: ['fine', 'lot', 'refugees', 'gab', 'care', 'race', 'cuz', 'internet', 'duopolists', 'want', 'thoughtpolice', 'racial', 'nationalism', 'among', 'wrongthoughts', 'want', 'erased', 'minds', 'even', 'care', 'race', 'care', 'free', 'speech', 'conscience', 'stand', 'us']\n",
            "Text after Lemmatization: ['fine', 'lot', 'refugee', 'gab', 'care', 'race', 'cuz', 'internet', 'duopolists', 'want', 'thoughtpolice', 'racial', 'nationalism', 'among', 'wrongthoughts', 'want', 'erased', 'mind', 'even', 'care', 'race', 'care', 'free', 'speech', 'conscience', 'stand', 'u']\n",
            "Final pre-processed text: fine lot refugee gab care race cuz internet duopolists want thoughtpolice racial nationalism among wrongthoughts want erased mind even care race care free speech conscience stand u\n",
            "Text after removing HTML tags: daca in a nutshell fake heroes using illegal immigrants to virtue signal about how unracist they are while purposely omitting the legal ways to become a citizen as a legal haitian american i fully support ending daca\n",
            "Text after removing non-alphabetic characters and converting to lowercase: daca in a nutshell fake heroes using illegal immigrants to virtue signal about how unracist they are while purposely omitting the legal ways to become a citizen as a legal haitian american i fully support ending daca\n",
            "Text after tokenization: ['daca', 'in', 'a', 'nutshell', 'fake', 'heroes', 'using', 'illegal', 'immigrants', 'to', 'virtue', 'signal', 'about', 'how', 'unracist', 'they', 'are', 'while', 'purposely', 'omitting', 'the', 'legal', 'ways', 'to', 'become', 'a', 'citizen', 'as', 'a', 'legal', 'haitian', 'american', 'i', 'fully', 'support', 'ending', 'daca']\n",
            "Text after removing stop words: ['daca', 'nutshell', 'fake', 'heroes', 'using', 'illegal', 'immigrants', 'virtue', 'signal', 'unracist', 'purposely', 'omitting', 'legal', 'ways', 'become', 'citizen', 'legal', 'haitian', 'american', 'fully', 'support', 'ending', 'daca']\n",
            "Text after Lemmatization: ['daca', 'nutshell', 'fake', 'hero', 'using', 'illegal', 'immigrant', 'virtue', 'signal', 'unracist', 'purposely', 'omitting', 'legal', 'way', 'become', 'citizen', 'legal', 'haitian', 'american', 'fully', 'support', 'ending', 'daca']\n",
            "Final pre-processed text: daca nutshell fake hero using illegal immigrant virtue signal unracist purposely omitting legal way become citizen legal haitian american fully support ending daca\n",
            "Text after removing HTML tags: we live in a age where cops are hated criminals are protected illegal immigrants and refugees are more important than american citizens and our safety our flag is burnt and disrespected all the while the media is promoting racial divide and civil disobedience\n",
            "Text after removing non-alphabetic characters and converting to lowercase: we live in a age where cops are hated criminals are protected illegal immigrants and refugees are more important than american citizens and our safety our flag is burnt and disrespected all the while the media is promoting racial divide and civil disobedience\n",
            "Text after tokenization: ['we', 'live', 'in', 'a', 'age', 'where', 'cops', 'are', 'hated', 'criminals', 'are', 'protected', 'illegal', 'immigrants', 'and', 'refugees', 'are', 'more', 'important', 'than', 'american', 'citizens', 'and', 'our', 'safety', 'our', 'flag', 'is', 'burnt', 'and', 'disrespected', 'all', 'the', 'while', 'the', 'media', 'is', 'promoting', 'racial', 'divide', 'and', 'civil', 'disobedience']\n",
            "Text after removing stop words: ['live', 'age', 'cops', 'hated', 'criminals', 'protected', 'illegal', 'immigrants', 'refugees', 'important', 'american', 'citizens', 'safety', 'flag', 'burnt', 'disrespected', 'media', 'promoting', 'racial', 'divide', 'civil', 'disobedience']\n",
            "Text after Lemmatization: ['live', 'age', 'cop', 'hated', 'criminal', 'protected', 'illegal', 'immigrant', 'refugee', 'important', 'american', 'citizen', 'safety', 'flag', 'burnt', 'disrespected', 'medium', 'promoting', 'racial', 'divide', 'civil', 'disobedience']\n",
            "Final pre-processed text: live age cop hated criminal protected illegal immigrant refugee important american citizen safety flag burnt disrespected medium promoting racial divide civil disobedience\n",
            "Text after removing HTML tags:  are you a fucking retard\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  are you a fucking retard\n",
            "Text after tokenization: ['are', 'you', 'a', 'fucking', 'retard']\n",
            "Text after removing stop words: ['fucking', 'retard']\n",
            "Text after Lemmatization: ['fucking', 'retard']\n",
            "Final pre-processed text: fucking retard\n",
            "Text after removing HTML tags: lindelof is such a poof i can not believe his defending what a fucking bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lindelof is such a poof i can not believe his defending what a fucking bitch\n",
            "Text after tokenization: ['lindelof', 'is', 'such', 'a', 'poof', 'i', 'can', 'not', 'believe', 'his', 'defending', 'what', 'a', 'fucking', 'bitch']\n",
            "Text after removing stop words: ['lindelof', 'poof', 'believe', 'defending', 'fucking', 'bitch']\n",
            "Text after Lemmatization: ['lindelof', 'poof', 'believe', 'defending', 'fucking', 'bitch']\n",
            "Final pre-processed text: lindelof poof believe defending fucking bitch\n",
            "Text after removing HTML tags: please vox quit the internet cut yourself off from the world except for the small fan base where you play dictator living in a ghetto will be awesome\n",
            "Text after removing non-alphabetic characters and converting to lowercase: please vox quit the internet cut yourself off from the world except for the small fan base where you play dictator living in a ghetto will be awesome\n",
            "Text after tokenization: ['please', 'vox', 'quit', 'the', 'internet', 'cut', 'yourself', 'off', 'from', 'the', 'world', 'except', 'for', 'the', 'small', 'fan', 'base', 'where', 'you', 'play', 'dictator', 'living', 'in', 'a', 'ghetto', 'will', 'be', 'awesome']\n",
            "Text after removing stop words: ['please', 'vox', 'quit', 'internet', 'cut', 'world', 'except', 'small', 'fan', 'base', 'play', 'dictator', 'living', 'ghetto', 'awesome']\n",
            "Text after Lemmatization: ['please', 'vox', 'quit', 'internet', 'cut', 'world', 'except', 'small', 'fan', 'base', 'play', 'dictator', 'living', 'ghetto', 'awesome']\n",
            "Final pre-processed text: please vox quit internet cut world except small fan base play dictator living ghetto awesome\n",
            "Text after removing HTML tags: ok where ’ the teaser trailer is this a fucking joke i ’ m so done\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ok where   the teaser trailer is this a fucking joke i   m so done\n",
            "Text after tokenization: ['ok', 'where', 'the', 'teaser', 'trailer', 'is', 'this', 'a', 'fucking', 'joke', 'i', 'm', 'so', 'done']\n",
            "Text after removing stop words: ['ok', 'teaser', 'trailer', 'fucking', 'joke', 'done']\n",
            "Text after Lemmatization: ['ok', 'teaser', 'trailer', 'fucking', 'joke', 'done']\n",
            "Final pre-processed text: ok teaser trailer fucking joke done\n",
            "Text after removing HTML tags: i been saving this bitch an ass whooping since i was pregnant pero ya anda preñada 🤦 🏻 ‍ ♀ ️\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i been saving this bitch an ass whooping since i was pregnant pero ya anda pre ada          \n",
            "Text after tokenization: ['i', 'been', 'saving', 'this', 'bitch', 'an', 'ass', 'whooping', 'since', 'i', 'was', 'pregnant', 'pero', 'ya', 'anda', 'pre', 'ada']\n",
            "Text after removing stop words: ['saving', 'bitch', 'ass', 'whooping', 'since', 'pregnant', 'pero', 'ya', 'anda', 'pre', 'ada']\n",
            "Text after Lemmatization: ['saving', 'bitch', 'as', 'whooping', 'since', 'pregnant', 'pero', 'ya', 'anda', 'pre', 'ada']\n",
            "Final pre-processed text: saving bitch as whooping since pregnant pero ya anda pre ada\n",
            "Text after removing HTML tags: this nigga do not even deserve me im just a dumb bitch tripping\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this nigga do not even deserve me im just a dumb bitch tripping\n",
            "Text after tokenization: ['this', 'nigga', 'do', 'not', 'even', 'deserve', 'me', 'im', 'just', 'a', 'dumb', 'bitch', 'tripping']\n",
            "Text after removing stop words: ['nigga', 'even', 'deserve', 'im', 'dumb', 'bitch', 'tripping']\n",
            "Text after Lemmatization: ['nigga', 'even', 'deserve', 'im', 'dumb', 'bitch', 'tripping']\n",
            "Final pre-processed text: nigga even deserve im dumb bitch tripping\n",
            "Text after removing HTML tags: lrt guys i swear im not nazis but them uniforms looking lit af\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lrt guys i swear im not nazis but them uniforms looking lit af\n",
            "Text after tokenization: ['lrt', 'guys', 'i', 'swear', 'im', 'not', 'nazis', 'but', 'them', 'uniforms', 'looking', 'lit', 'af']\n",
            "Text after removing stop words: ['lrt', 'guys', 'swear', 'im', 'nazis', 'uniforms', 'looking', 'lit', 'af']\n",
            "Text after Lemmatization: ['lrt', 'guy', 'swear', 'im', 'nazi', 'uniform', 'looking', 'lit', 'af']\n",
            "Final pre-processed text: lrt guy swear im nazi uniform looking lit af\n",
            "Text after removing HTML tags: go back to breitbart you sound like a retarded boomer\n",
            "Text after removing non-alphabetic characters and converting to lowercase: go back to breitbart you sound like a retarded boomer\n",
            "Text after tokenization: ['go', 'back', 'to', 'breitbart', 'you', 'sound', 'like', 'a', 'retarded', 'boomer']\n",
            "Text after removing stop words: ['go', 'back', 'breitbart', 'sound', 'like', 'retarded', 'boomer']\n",
            "Text after Lemmatization: ['go', 'back', 'breitbart', 'sound', 'like', 'retarded', 'boomer']\n",
            "Final pre-processed text: go back breitbart sound like retarded boomer\n",
            "Text after removing HTML tags:  hi pls kiss me full homo\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  hi pls kiss me full homo\n",
            "Text after tokenization: ['hi', 'pls', 'kiss', 'me', 'full', 'homo']\n",
            "Text after removing stop words: ['hi', 'pls', 'kiss', 'full', 'homo']\n",
            "Text after Lemmatization: ['hi', 'pls', 'kiss', 'full', 'homo']\n",
            "Final pre-processed text: hi pls kiss full homo\n",
            "Text after removing HTML tags:  no it is impossible for a jew to be a nazi its like calling a black person a white supremacist\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  no it is impossible for a jew to be a nazi its like calling a black person a white supremacist\n",
            "Text after tokenization: ['no', 'it', 'is', 'impossible', 'for', 'a', 'jew', 'to', 'be', 'a', 'nazi', 'its', 'like', 'calling', 'a', 'black', 'person', 'a', 'white', 'supremacist']\n",
            "Text after removing stop words: ['impossible', 'jew', 'nazi', 'like', 'calling', 'black', 'person', 'white', 'supremacist']\n",
            "Text after Lemmatization: ['impossible', 'jew', 'nazi', 'like', 'calling', 'black', 'person', 'white', 'supremacist']\n",
            "Final pre-processed text: impossible jew nazi like calling black person white supremacist\n",
            "Text after removing HTML tags: love when one of my favorite shows has a gay plot twist 🤓 😎 😍 🥰\n",
            "Text after removing non-alphabetic characters and converting to lowercase: love when one of my favorite shows has a gay plot twist        \n",
            "Text after tokenization: ['love', 'when', 'one', 'of', 'my', 'favorite', 'shows', 'has', 'a', 'gay', 'plot', 'twist']\n",
            "Text after removing stop words: ['love', 'one', 'favorite', 'shows', 'gay', 'plot', 'twist']\n",
            "Text after Lemmatization: ['love', 'one', 'favorite', 'show', 'gay', 'plot', 'twist']\n",
            "Final pre-processed text: love one favorite show gay plot twist\n",
            "Text after removing HTML tags:  normani and bts they hate you\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  normani and bts they hate you\n",
            "Text after tokenization: ['normani', 'and', 'bts', 'they', 'hate', 'you']\n",
            "Text after removing stop words: ['normani', 'bts', 'hate']\n",
            "Text after Lemmatization: ['normani', 'bts', 'hate']\n",
            "Final pre-processed text: normani bts hate\n",
            "Text after removing HTML tags: if you do not see the headline it was mit economist claims usa is slipping to developing nation status how retarded is reddit\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if you do not see the headline it was mit economist claims usa is slipping to developing nation status how retarded is reddit\n",
            "Text after tokenization: ['if', 'you', 'do', 'not', 'see', 'the', 'headline', 'it', 'was', 'mit', 'economist', 'claims', 'usa', 'is', 'slipping', 'to', 'developing', 'nation', 'status', 'how', 'retarded', 'is', 'reddit']\n",
            "Text after removing stop words: ['see', 'headline', 'mit', 'economist', 'claims', 'usa', 'slipping', 'developing', 'nation', 'status', 'retarded', 'reddit']\n",
            "Text after Lemmatization: ['see', 'headline', 'mit', 'economist', 'claim', 'usa', 'slipping', 'developing', 'nation', 'status', 'retarded', 'reddit']\n",
            "Final pre-processed text: see headline mit economist claim usa slipping developing nation status retarded reddit\n",
            "Text after removing HTML tags:   donate it to jews\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   donate it to jews\n",
            "Text after tokenization: ['donate', 'it', 'to', 'jews']\n",
            "Text after removing stop words: ['donate', 'jews']\n",
            "Text after Lemmatization: ['donate', 'jew']\n",
            "Final pre-processed text: donate jew\n",
            "Text after removing HTML tags: islam is mostly known for tolerance i know cause the lolbertarians told me yay freedoms\n",
            "Text after removing non-alphabetic characters and converting to lowercase: islam is mostly known for tolerance i know cause the lolbertarians told me yay freedoms\n",
            "Text after tokenization: ['islam', 'is', 'mostly', 'known', 'for', 'tolerance', 'i', 'know', 'cause', 'the', 'lolbertarians', 'told', 'me', 'yay', 'freedoms']\n",
            "Text after removing stop words: ['islam', 'mostly', 'known', 'tolerance', 'know', 'cause', 'lolbertarians', 'told', 'yay', 'freedoms']\n",
            "Text after Lemmatization: ['islam', 'mostly', 'known', 'tolerance', 'know', 'cause', 'lolbertarians', 'told', 'yay', 'freedom']\n",
            "Final pre-processed text: islam mostly known tolerance know cause lolbertarians told yay freedom\n",
            "Text after removing HTML tags:   new nazi movement\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   new nazi movement\n",
            "Text after tokenization: ['new', 'nazi', 'movement']\n",
            "Text after removing stop words: ['new', 'nazi', 'movement']\n",
            "Text after Lemmatization: ['new', 'nazi', 'movement']\n",
            "Final pre-processed text: new nazi movement\n",
            "Text after removing HTML tags: i can ’ t believe y ’ all really be sitting around eating spaghetti noodles separately from the meat sauce dry ass noodles how ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i can   t believe y   all really be sitting around eating spaghetti noodles separately from the meat sauce dry ass noodles how ghetto\n",
            "Text after tokenization: ['i', 'can', 't', 'believe', 'y', 'all', 'really', 'be', 'sitting', 'around', 'eating', 'spaghetti', 'noodles', 'separately', 'from', 'the', 'meat', 'sauce', 'dry', 'ass', 'noodles', 'how', 'ghetto']\n",
            "Text after removing stop words: ['believe', 'really', 'sitting', 'around', 'eating', 'spaghetti', 'noodles', 'separately', 'meat', 'sauce', 'dry', 'ass', 'noodles', 'ghetto']\n",
            "Text after Lemmatization: ['believe', 'really', 'sitting', 'around', 'eating', 'spaghetti', 'noodle', 'separately', 'meat', 'sauce', 'dry', 'as', 'noodle', 'ghetto']\n",
            "Final pre-processed text: believe really sitting around eating spaghetti noodle separately meat sauce dry as noodle ghetto\n",
            "Text after removing HTML tags:  wait a minute pissed like pee pee or something else i am retarded sorry\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  wait a minute pissed like pee pee or something else i am retarded sorry\n",
            "Text after tokenization: ['wait', 'a', 'minute', 'pissed', 'like', 'pee', 'pee', 'or', 'something', 'else', 'i', 'am', 'retarded', 'sorry']\n",
            "Text after removing stop words: ['wait', 'minute', 'pissed', 'like', 'pee', 'pee', 'something', 'else', 'retarded', 'sorry']\n",
            "Text after Lemmatization: ['wait', 'minute', 'pissed', 'like', 'pee', 'pee', 'something', 'else', 'retarded', 'sorry']\n",
            "Final pre-processed text: wait minute pissed like pee pee something else retarded sorry\n",
            "Text after removing HTML tags: someone in my class looks like bruce from finding nemo but he laughs like a retarded seth rogen\n",
            "Text after removing non-alphabetic characters and converting to lowercase: someone in my class looks like bruce from finding nemo but he laughs like a retarded seth rogen\n",
            "Text after tokenization: ['someone', 'in', 'my', 'class', 'looks', 'like', 'bruce', 'from', 'finding', 'nemo', 'but', 'he', 'laughs', 'like', 'a', 'retarded', 'seth', 'rogen']\n",
            "Text after removing stop words: ['someone', 'class', 'looks', 'like', 'bruce', 'finding', 'nemo', 'laughs', 'like', 'retarded', 'seth', 'rogen']\n",
            "Text after Lemmatization: ['someone', 'class', 'look', 'like', 'bruce', 'finding', 'nemo', 'laugh', 'like', 'retarded', 'seth', 'rogen']\n",
            "Final pre-processed text: someone class look like bruce finding nemo laugh like retarded seth rogen\n",
            "Text after removing HTML tags: by running away and proclaiming victory in his echo chamber his dumbfuck followers will likely consider it a masterful philosophical lol rebuke of the evil nahzi white supremists\n",
            "Text after removing non-alphabetic characters and converting to lowercase: by running away and proclaiming victory in his echo chamber his dumbfuck followers will likely consider it a masterful philosophical lol rebuke of the evil nahzi white supremists\n",
            "Text after tokenization: ['by', 'running', 'away', 'and', 'proclaiming', 'victory', 'in', 'his', 'echo', 'chamber', 'his', 'dumbfuck', 'followers', 'will', 'likely', 'consider', 'it', 'a', 'masterful', 'philosophical', 'lol', 'rebuke', 'of', 'the', 'evil', 'nahzi', 'white', 'supremists']\n",
            "Text after removing stop words: ['running', 'away', 'proclaiming', 'victory', 'echo', 'chamber', 'dumbfuck', 'followers', 'likely', 'consider', 'masterful', 'philosophical', 'lol', 'rebuke', 'evil', 'nahzi', 'white', 'supremists']\n",
            "Text after Lemmatization: ['running', 'away', 'proclaiming', 'victory', 'echo', 'chamber', 'dumbfuck', 'follower', 'likely', 'consider', 'masterful', 'philosophical', 'lol', 'rebuke', 'evil', 'nahzi', 'white', 'supremists']\n",
            "Final pre-processed text: running away proclaiming victory echo chamber dumbfuck follower likely consider masterful philosophical lol rebuke evil nahzi white supremists\n",
            "Text after removing HTML tags:  this is certainly a face of naziism adopted by rss surprising that they drink cow urine but beat an innocent child to death\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  this is certainly a face of naziism adopted by rss surprising that they drink cow urine but beat an innocent child to death\n",
            "Text after tokenization: ['this', 'is', 'certainly', 'a', 'face', 'of', 'naziism', 'adopted', 'by', 'rss', 'surprising', 'that', 'they', 'drink', 'cow', 'urine', 'but', 'beat', 'an', 'innocent', 'child', 'to', 'death']\n",
            "Text after removing stop words: ['certainly', 'face', 'naziism', 'adopted', 'rss', 'surprising', 'drink', 'cow', 'urine', 'beat', 'innocent', 'child', 'death']\n",
            "Text after Lemmatization: ['certainly', 'face', 'naziism', 'adopted', 'r', 'surprising', 'drink', 'cow', 'urine', 'beat', 'innocent', 'child', 'death']\n",
            "Final pre-processed text: certainly face naziism adopted r surprising drink cow urine beat innocent child death\n",
            "Text after removing HTML tags: there is nothing called as marital rape in islam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there is nothing called as marital rape in islam\n",
            "Text after tokenization: ['there', 'is', 'nothing', 'called', 'as', 'marital', 'rape', 'in', 'islam']\n",
            "Text after removing stop words: ['nothing', 'called', 'marital', 'rape', 'islam']\n",
            "Text after Lemmatization: ['nothing', 'called', 'marital', 'rape', 'islam']\n",
            "Final pre-processed text: nothing called marital rape islam\n",
            "Text after removing HTML tags: wow could u imagine bein such a bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase: wow could u imagine bein such a bitch\n",
            "Text after tokenization: ['wow', 'could', 'u', 'imagine', 'bein', 'such', 'a', 'bitch']\n",
            "Text after removing stop words: ['wow', 'could', 'u', 'imagine', 'bein', 'bitch']\n",
            "Text after Lemmatization: ['wow', 'could', 'u', 'imagine', 'bein', 'bitch']\n",
            "Final pre-processed text: wow could u imagine bein bitch\n",
            "Text after removing HTML tags:  but also get ready for my dyke fits sweethart 🥰\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  but also get ready for my dyke fits sweethart  \n",
            "Text after tokenization: ['but', 'also', 'get', 'ready', 'for', 'my', 'dyke', 'fits', 'sweethart']\n",
            "Text after removing stop words: ['also', 'get', 'ready', 'dyke', 'fits', 'sweethart']\n",
            "Text after Lemmatization: ['also', 'get', 'ready', 'dyke', 'fit', 'sweethart']\n",
            "Final pre-processed text: also get ready dyke fit sweethart\n",
            "Text after removing HTML tags: some people hope their ghetto fight ends up on worldstar hip hop but gab users we hope our epic shitpost ends up in a media matters top five list\n",
            "Text after removing non-alphabetic characters and converting to lowercase: some people hope their ghetto fight ends up on worldstar hip hop but gab users we hope our epic shitpost ends up in a media matters top five list\n",
            "Text after tokenization: ['some', 'people', 'hope', 'their', 'ghetto', 'fight', 'ends', 'up', 'on', 'worldstar', 'hip', 'hop', 'but', 'gab', 'users', 'we', 'hope', 'our', 'epic', 'shitpost', 'ends', 'up', 'in', 'a', 'media', 'matters', 'top', 'five', 'list']\n",
            "Text after removing stop words: ['people', 'hope', 'ghetto', 'fight', 'ends', 'worldstar', 'hip', 'hop', 'gab', 'users', 'hope', 'epic', 'shitpost', 'ends', 'media', 'matters', 'top', 'five', 'list']\n",
            "Text after Lemmatization: ['people', 'hope', 'ghetto', 'fight', 'end', 'worldstar', 'hip', 'hop', 'gab', 'user', 'hope', 'epic', 'shitpost', 'end', 'medium', 'matter', 'top', 'five', 'list']\n",
            "Final pre-processed text: people hope ghetto fight end worldstar hip hop gab user hope epic shitpost end medium matter top five list\n",
            "Text after removing HTML tags: i can ’ t believe some arabs sexualize wearing pants inside the house around the family embarrassing amp disgusting\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i can   t believe some arabs sexualize wearing pants inside the house around the family embarrassing amp disgusting\n",
            "Text after tokenization: ['i', 'can', 't', 'believe', 'some', 'arabs', 'sexualize', 'wearing', 'pants', 'inside', 'the', 'house', 'around', 'the', 'family', 'embarrassing', 'amp', 'disgusting']\n",
            "Text after removing stop words: ['believe', 'arabs', 'sexualize', 'wearing', 'pants', 'inside', 'house', 'around', 'family', 'embarrassing', 'amp', 'disgusting']\n",
            "Text after Lemmatization: ['believe', 'arab', 'sexualize', 'wearing', 'pant', 'inside', 'house', 'around', 'family', 'embarrassing', 'amp', 'disgusting']\n",
            "Final pre-processed text: believe arab sexualize wearing pant inside house around family embarrassing amp disgusting\n",
            "Text after removing HTML tags:  and the same with sexuality if your homophobic pick the straight one\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  and the same with sexuality if your homophobic pick the straight one\n",
            "Text after tokenization: ['and', 'the', 'same', 'with', 'sexuality', 'if', 'your', 'homophobic', 'pick', 'the', 'straight', 'one']\n",
            "Text after removing stop words: ['sexuality', 'homophobic', 'pick', 'straight', 'one']\n",
            "Text after Lemmatization: ['sexuality', 'homophobic', 'pick', 'straight', 'one']\n",
            "Final pre-processed text: sexuality homophobic pick straight one\n",
            "Text after removing HTML tags: its also how redguards are a vague brown race like all poc are like homogenous sad we cant have latinos\n",
            "Text after removing non-alphabetic characters and converting to lowercase: its also how redguards are a vague brown race like all poc are like homogenous sad we cant have latinos\n",
            "Text after tokenization: ['its', 'also', 'how', 'redguards', 'are', 'a', 'vague', 'brown', 'race', 'like', 'all', 'poc', 'are', 'like', 'homogenous', 'sad', 'we', 'cant', 'have', 'latinos']\n",
            "Text after removing stop words: ['also', 'redguards', 'vague', 'brown', 'race', 'like', 'poc', 'like', 'homogenous', 'sad', 'cant', 'latinos']\n",
            "Text after Lemmatization: ['also', 'redguards', 'vague', 'brown', 'race', 'like', 'poc', 'like', 'homogenous', 'sad', 'cant', 'latino']\n",
            "Final pre-processed text: also redguards vague brown race like poc like homogenous sad cant latino\n",
            "Text after removing HTML tags: i ’ m tired of waking up for school this shit is so ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i   m tired of waking up for school this shit is so ghetto\n",
            "Text after tokenization: ['i', 'm', 'tired', 'of', 'waking', 'up', 'for', 'school', 'this', 'shit', 'is', 'so', 'ghetto']\n",
            "Text after removing stop words: ['tired', 'waking', 'school', 'shit', 'ghetto']\n",
            "Text after Lemmatization: ['tired', 'waking', 'school', 'shit', 'ghetto']\n",
            "Final pre-processed text: tired waking school shit ghetto\n",
            "Text after removing HTML tags:  why does she sound like she speaking ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  why does she sound like she speaking ghetto\n",
            "Text after tokenization: ['why', 'does', 'she', 'sound', 'like', 'she', 'speaking', 'ghetto']\n",
            "Text after removing stop words: ['sound', 'like', 'speaking', 'ghetto']\n",
            "Text after Lemmatization: ['sound', 'like', 'speaking', 'ghetto']\n",
            "Final pre-processed text: sound like speaking ghetto\n",
            "Text after removing HTML tags:   last time i checked he has  career sacks vs the eagles layne johnson has made him his bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   last time i checked he has  career sacks vs the eagles layne johnson has made him his bitch\n",
            "Text after tokenization: ['last', 'time', 'i', 'checked', 'he', 'has', 'career', 'sacks', 'vs', 'the', 'eagles', 'layne', 'johnson', 'has', 'made', 'him', 'his', 'bitch']\n",
            "Text after removing stop words: ['last', 'time', 'checked', 'career', 'sacks', 'vs', 'eagles', 'layne', 'johnson', 'made', 'bitch']\n",
            "Text after Lemmatization: ['last', 'time', 'checked', 'career', 'sack', 'v', 'eagle', 'layne', 'johnson', 'made', 'bitch']\n",
            "Final pre-processed text: last time checked career sack v eagle layne johnson made bitch\n",
            "Text after removing HTML tags: child birth and the healing process after is the ghetto i would love ukulamanisa but wow child birth is low key traumatizing no lie 🤞 🏾\n",
            "Text after removing non-alphabetic characters and converting to lowercase: child birth and the healing process after is the ghetto i would love ukulamanisa but wow child birth is low key traumatizing no lie    \n",
            "Text after tokenization: ['child', 'birth', 'and', 'the', 'healing', 'process', 'after', 'is', 'the', 'ghetto', 'i', 'would', 'love', 'ukulamanisa', 'but', 'wow', 'child', 'birth', 'is', 'low', 'key', 'traumatizing', 'no', 'lie']\n",
            "Text after removing stop words: ['child', 'birth', 'healing', 'process', 'ghetto', 'would', 'love', 'ukulamanisa', 'wow', 'child', 'birth', 'low', 'key', 'traumatizing', 'lie']\n",
            "Text after Lemmatization: ['child', 'birth', 'healing', 'process', 'ghetto', 'would', 'love', 'ukulamanisa', 'wow', 'child', 'birth', 'low', 'key', 'traumatizing', 'lie']\n",
            "Final pre-processed text: child birth healing process ghetto would love ukulamanisa wow child birth low key traumatizing lie\n",
            "Text after removing HTML tags: petty betty back at it again this is my life how ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: petty betty back at it again this is my life how ghetto\n",
            "Text after tokenization: ['petty', 'betty', 'back', 'at', 'it', 'again', 'this', 'is', 'my', 'life', 'how', 'ghetto']\n",
            "Text after removing stop words: ['petty', 'betty', 'back', 'life', 'ghetto']\n",
            "Text after Lemmatization: ['petty', 'betty', 'back', 'life', 'ghetto']\n",
            "Final pre-processed text: petty betty back life ghetto\n",
            "Text after removing HTML tags:  lmao actually i take that back it depends on how much i like the person 🥴 but even then sucking dick is ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  lmao actually i take that back it depends on how much i like the person   but even then sucking dick is ghetto\n",
            "Text after tokenization: ['lmao', 'actually', 'i', 'take', 'that', 'back', 'it', 'depends', 'on', 'how', 'much', 'i', 'like', 'the', 'person', 'but', 'even', 'then', 'sucking', 'dick', 'is', 'ghetto']\n",
            "Text after removing stop words: ['lmao', 'actually', 'take', 'back', 'depends', 'much', 'like', 'person', 'even', 'sucking', 'dick', 'ghetto']\n",
            "Text after Lemmatization: ['lmao', 'actually', 'take', 'back', 'depends', 'much', 'like', 'person', 'even', 'sucking', 'dick', 'ghetto']\n",
            "Final pre-processed text: lmao actually take back depends much like person even sucking dick ghetto\n",
            "Text after removing HTML tags: top to bottom this club is a fucking joke more than  mil a year fuck off gordon you cunt\n",
            "Text after removing non-alphabetic characters and converting to lowercase: top to bottom this club is a fucking joke more than  mil a year fuck off gordon you cunt\n",
            "Text after tokenization: ['top', 'to', 'bottom', 'this', 'club', 'is', 'a', 'fucking', 'joke', 'more', 'than', 'mil', 'a', 'year', 'fuck', 'off', 'gordon', 'you', 'cunt']\n",
            "Text after removing stop words: ['top', 'bottom', 'club', 'fucking', 'joke', 'mil', 'year', 'fuck', 'gordon', 'cunt']\n",
            "Text after Lemmatization: ['top', 'bottom', 'club', 'fucking', 'joke', 'mil', 'year', 'fuck', 'gordon', 'cunt']\n",
            "Final pre-processed text: top bottom club fucking joke mil year fuck gordon cunt\n",
            "Text after removing HTML tags: i think you are missing that it is young white males especially that are waking up\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i think you are missing that it is young white males especially that are waking up\n",
            "Text after tokenization: ['i', 'think', 'you', 'are', 'missing', 'that', 'it', 'is', 'young', 'white', 'males', 'especially', 'that', 'are', 'waking', 'up']\n",
            "Text after removing stop words: ['think', 'missing', 'young', 'white', 'males', 'especially', 'waking']\n",
            "Text after Lemmatization: ['think', 'missing', 'young', 'white', 'male', 'especially', 'waking']\n",
            "Final pre-processed text: think missing young white male especially waking\n",
            "Text after removing HTML tags: how do i keep waking up single lol the ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: how do i keep waking up single lol the ghetto\n",
            "Text after tokenization: ['how', 'do', 'i', 'keep', 'waking', 'up', 'single', 'lol', 'the', 'ghetto']\n",
            "Text after removing stop words: ['keep', 'waking', 'single', 'lol', 'ghetto']\n",
            "Text after Lemmatization: ['keep', 'waking', 'single', 'lol', 'ghetto']\n",
            "Final pre-processed text: keep waking single lol ghetto\n",
            "Text after removing HTML tags:  i have seen many claimants to his syncophancy but no real evidence that they are actual human beings that goes treble for his admittedly queer followers if only there were one twentieth of this vehemence directed at milo and at his fag enabling followers if only\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i have seen many claimants to his syncophancy but no real evidence that they are actual human beings that goes treble for his admittedly queer followers if only there were one twentieth of this vehemence directed at milo and at his fag enabling followers if only\n",
            "Text after tokenization: ['i', 'have', 'seen', 'many', 'claimants', 'to', 'his', 'syncophancy', 'but', 'no', 'real', 'evidence', 'that', 'they', 'are', 'actual', 'human', 'beings', 'that', 'goes', 'treble', 'for', 'his', 'admittedly', 'queer', 'followers', 'if', 'only', 'there', 'were', 'one', 'twentieth', 'of', 'this', 'vehemence', 'directed', 'at', 'milo', 'and', 'at', 'his', 'fag', 'enabling', 'followers', 'if', 'only']\n",
            "Text after removing stop words: ['seen', 'many', 'claimants', 'syncophancy', 'real', 'evidence', 'actual', 'human', 'beings', 'goes', 'treble', 'admittedly', 'queer', 'followers', 'one', 'twentieth', 'vehemence', 'directed', 'milo', 'fag', 'enabling', 'followers']\n",
            "Text after Lemmatization: ['seen', 'many', 'claimant', 'syncophancy', 'real', 'evidence', 'actual', 'human', 'being', 'go', 'treble', 'admittedly', 'queer', 'follower', 'one', 'twentieth', 'vehemence', 'directed', 'milo', 'fag', 'enabling', 'follower']\n",
            "Final pre-processed text: seen many claimant syncophancy real evidence actual human being go treble admittedly queer follower one twentieth vehemence directed milo fag enabling follower\n",
            "Text after removing HTML tags:  she literally muslim\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  she literally muslim\n",
            "Text after tokenization: ['she', 'literally', 'muslim']\n",
            "Text after removing stop words: ['literally', 'muslim']\n",
            "Text after Lemmatization: ['literally', 'muslim']\n",
            "Final pre-processed text: literally muslim\n",
            "Text after removing HTML tags:  bitch you dont answer me\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  bitch you dont answer me\n",
            "Text after tokenization: ['bitch', 'you', 'dont', 'answer', 'me']\n",
            "Text after removing stop words: ['bitch', 'dont', 'answer']\n",
            "Text after Lemmatization: ['bitch', 'dont', 'answer']\n",
            "Final pre-processed text: bitch dont answer\n",
            "Text after removing HTML tags: whatever man starting to think this place will just belong to  and not  soon everyone will just leave it to you some of us do not trust liberal jews at all but we sure do not want them all dead and are getting tired of it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: whatever man starting to think this place will just belong to  and not  soon everyone will just leave it to you some of us do not trust liberal jews at all but we sure do not want them all dead and are getting tired of it\n",
            "Text after tokenization: ['whatever', 'man', 'starting', 'to', 'think', 'this', 'place', 'will', 'just', 'belong', 'to', 'and', 'not', 'soon', 'everyone', 'will', 'just', 'leave', 'it', 'to', 'you', 'some', 'of', 'us', 'do', 'not', 'trust', 'liberal', 'jews', 'at', 'all', 'but', 'we', 'sure', 'do', 'not', 'want', 'them', 'all', 'dead', 'and', 'are', 'getting', 'tired', 'of', 'it']\n",
            "Text after removing stop words: ['whatever', 'man', 'starting', 'think', 'place', 'belong', 'soon', 'everyone', 'leave', 'us', 'trust', 'liberal', 'jews', 'sure', 'want', 'dead', 'getting', 'tired']\n",
            "Text after Lemmatization: ['whatever', 'man', 'starting', 'think', 'place', 'belong', 'soon', 'everyone', 'leave', 'u', 'trust', 'liberal', 'jew', 'sure', 'want', 'dead', 'getting', 'tired']\n",
            "Final pre-processed text: whatever man starting think place belong soon everyone leave u trust liberal jew sure want dead getting tired\n",
            "Text after removing HTML tags:  wow fuck you too then damn\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  wow fuck you too then damn\n",
            "Text after tokenization: ['wow', 'fuck', 'you', 'too', 'then', 'damn']\n",
            "Text after removing stop words: ['wow', 'fuck', 'damn']\n",
            "Text after Lemmatization: ['wow', 'fuck', 'damn']\n",
            "Final pre-processed text: wow fuck damn\n",
            "Text after removing HTML tags:   calling something gay has nothing to do with power this has no logic\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   calling something gay has nothing to do with power this has no logic\n",
            "Text after tokenization: ['calling', 'something', 'gay', 'has', 'nothing', 'to', 'do', 'with', 'power', 'this', 'has', 'no', 'logic']\n",
            "Text after removing stop words: ['calling', 'something', 'gay', 'nothing', 'power', 'logic']\n",
            "Text after Lemmatization: ['calling', 'something', 'gay', 'nothing', 'power', 'logic']\n",
            "Final pre-processed text: calling something gay nothing power logic\n",
            "Text after removing HTML tags:   no blacks no jews attended the polish far right independence march for fear of their lives\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   no blacks no jews attended the polish far right independence march for fear of their lives\n",
            "Text after tokenization: ['no', 'blacks', 'no', 'jews', 'attended', 'the', 'polish', 'far', 'right', 'independence', 'march', 'for', 'fear', 'of', 'their', 'lives']\n",
            "Text after removing stop words: ['blacks', 'jews', 'attended', 'polish', 'far', 'right', 'independence', 'march', 'fear', 'lives']\n",
            "Text after Lemmatization: ['black', 'jew', 'attended', 'polish', 'far', 'right', 'independence', 'march', 'fear', 'life']\n",
            "Final pre-processed text: black jew attended polish far right independence march fear life\n",
            "Text after removing HTML tags:  i cited influences retard clearly of which yours in paint thinner and wood polish 😂 😂 😂\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i cited influences retard clearly of which yours in paint thinner and wood polish      \n",
            "Text after tokenization: ['i', 'cited', 'influences', 'retard', 'clearly', 'of', 'which', 'yours', 'in', 'paint', 'thinner', 'and', 'wood', 'polish']\n",
            "Text after removing stop words: ['cited', 'influences', 'retard', 'clearly', 'paint', 'thinner', 'wood', 'polish']\n",
            "Text after Lemmatization: ['cited', 'influence', 'retard', 'clearly', 'paint', 'thinner', 'wood', 'polish']\n",
            "Final pre-processed text: cited influence retard clearly paint thinner wood polish\n",
            "Text after removing HTML tags:  muslim migrants storming sweden for welfare\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  muslim migrants storming sweden for welfare\n",
            "Text after tokenization: ['muslim', 'migrants', 'storming', 'sweden', 'for', 'welfare']\n",
            "Text after removing stop words: ['muslim', 'migrants', 'storming', 'sweden', 'welfare']\n",
            "Text after Lemmatization: ['muslim', 'migrant', 'storming', 'sweden', 'welfare']\n",
            "Final pre-processed text: muslim migrant storming sweden welfare\n",
            "Text after removing HTML tags: george is a faggot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: george is a faggot\n",
            "Text after tokenization: ['george', 'is', 'a', 'faggot']\n",
            "Text after removing stop words: ['george', 'faggot']\n",
            "Text after Lemmatization: ['george', 'faggot']\n",
            "Final pre-processed text: george faggot\n",
            "Text after removing HTML tags: perrie really loves asians uno\n",
            "Text after removing non-alphabetic characters and converting to lowercase: perrie really loves asians uno\n",
            "Text after tokenization: ['perrie', 'really', 'loves', 'asians', 'uno']\n",
            "Text after removing stop words: ['perrie', 'really', 'loves', 'asians', 'uno']\n",
            "Text after Lemmatization: ['perrie', 'really', 'love', 'asian', 'uno']\n",
            "Final pre-processed text: perrie really love asian uno\n",
            "Text after removing HTML tags: canada refugee system is dysfunctional but neither the virtue signalling airhead aka pm the dishonest media nor the cowardservatives care canada justintrudeau cdnpoli cdnimm\n",
            "Text after removing non-alphabetic characters and converting to lowercase: canada refugee system is dysfunctional but neither the virtue signalling airhead aka pm the dishonest media nor the cowardservatives care canada justintrudeau cdnpoli cdnimm\n",
            "Text after tokenization: ['canada', 'refugee', 'system', 'is', 'dysfunctional', 'but', 'neither', 'the', 'virtue', 'signalling', 'airhead', 'aka', 'pm', 'the', 'dishonest', 'media', 'nor', 'the', 'cowardservatives', 'care', 'canada', 'justintrudeau', 'cdnpoli', 'cdnimm']\n",
            "Text after removing stop words: ['canada', 'refugee', 'system', 'dysfunctional', 'neither', 'virtue', 'signalling', 'airhead', 'aka', 'pm', 'dishonest', 'media', 'cowardservatives', 'care', 'canada', 'justintrudeau', 'cdnpoli', 'cdnimm']\n",
            "Text after Lemmatization: ['canada', 'refugee', 'system', 'dysfunctional', 'neither', 'virtue', 'signalling', 'airhead', 'aka', 'pm', 'dishonest', 'medium', 'cowardservatives', 'care', 'canada', 'justintrudeau', 'cdnpoli', 'cdnimm']\n",
            "Final pre-processed text: canada refugee system dysfunctional neither virtue signalling airhead aka pm dishonest medium cowardservatives care canada justintrudeau cdnpoli cdnimm\n",
            "Text after removing HTML tags:  sorry islam law lied everything\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  sorry islam law lied everything\n",
            "Text after tokenization: ['sorry', 'islam', 'law', 'lied', 'everything']\n",
            "Text after removing stop words: ['sorry', 'islam', 'law', 'lied', 'everything']\n",
            "Text after Lemmatization: ['sorry', 'islam', 'law', 'lied', 'everything']\n",
            "Final pre-processed text: sorry islam law lied everything\n",
            "Text after removing HTML tags:  islamophobia is not just in the mind islamist extremists are for real these are hardened moderate muslims\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  islamophobia is not just in the mind islamist extremists are for real these are hardened moderate muslims\n",
            "Text after tokenization: ['islamophobia', 'is', 'not', 'just', 'in', 'the', 'mind', 'islamist', 'extremists', 'are', 'for', 'real', 'these', 'are', 'hardened', 'moderate', 'muslims']\n",
            "Text after removing stop words: ['islamophobia', 'mind', 'islamist', 'extremists', 'real', 'hardened', 'moderate', 'muslims']\n",
            "Text after Lemmatization: ['islamophobia', 'mind', 'islamist', 'extremist', 'real', 'hardened', 'moderate', 'muslim']\n",
            "Final pre-processed text: islamophobia mind islamist extremist real hardened moderate muslim\n",
            "Text after removing HTML tags: all i want to do is gunshot gunshot gunshot and a cash register noise punch a nazi\n",
            "Text after removing non-alphabetic characters and converting to lowercase: all i want to do is gunshot gunshot gunshot and a cash register noise punch a nazi\n",
            "Text after tokenization: ['all', 'i', 'want', 'to', 'do', 'is', 'gunshot', 'gunshot', 'gunshot', 'and', 'a', 'cash', 'register', 'noise', 'punch', 'a', 'nazi']\n",
            "Text after removing stop words: ['want', 'gunshot', 'gunshot', 'gunshot', 'cash', 'register', 'noise', 'punch', 'nazi']\n",
            "Text after Lemmatization: ['want', 'gunshot', 'gunshot', 'gunshot', 'cash', 'register', 'noise', 'punch', 'nazi']\n",
            "Final pre-processed text: want gunshot gunshot gunshot cash register noise punch nazi\n",
            "Text after removing HTML tags: most western europeans would rather be killed or raped not a play on words or a euphemism or have their children killed or raped by evil muslim invaders then be called a racist bigot xenophobe or islamaphobe cultural marxism schooling has brain washed them into submissive spineless pussies\n",
            "Text after removing non-alphabetic characters and converting to lowercase: most western europeans would rather be killed or raped not a play on words or a euphemism or have their children killed or raped by evil muslim invaders then be called a racist bigot xenophobe or islamaphobe cultural marxism schooling has brain washed them into submissive spineless pussies\n",
            "Text after tokenization: ['most', 'western', 'europeans', 'would', 'rather', 'be', 'killed', 'or', 'raped', 'not', 'a', 'play', 'on', 'words', 'or', 'a', 'euphemism', 'or', 'have', 'their', 'children', 'killed', 'or', 'raped', 'by', 'evil', 'muslim', 'invaders', 'then', 'be', 'called', 'a', 'racist', 'bigot', 'xenophobe', 'or', 'islamaphobe', 'cultural', 'marxism', 'schooling', 'has', 'brain', 'washed', 'them', 'into', 'submissive', 'spineless', 'pussies']\n",
            "Text after removing stop words: ['western', 'europeans', 'would', 'rather', 'killed', 'raped', 'play', 'words', 'euphemism', 'children', 'killed', 'raped', 'evil', 'muslim', 'invaders', 'called', 'racist', 'bigot', 'xenophobe', 'islamaphobe', 'cultural', 'marxism', 'schooling', 'brain', 'washed', 'submissive', 'spineless', 'pussies']\n",
            "Text after Lemmatization: ['western', 'european', 'would', 'rather', 'killed', 'raped', 'play', 'word', 'euphemism', 'child', 'killed', 'raped', 'evil', 'muslim', 'invader', 'called', 'racist', 'bigot', 'xenophobe', 'islamaphobe', 'cultural', 'marxism', 'schooling', 'brain', 'washed', 'submissive', 'spineless', 'pussy']\n",
            "Final pre-processed text: western european would rather killed raped play word euphemism child killed raped evil muslim invader called racist bigot xenophobe islamaphobe cultural marxism schooling brain washed submissive spineless pussy\n",
            "Text after removing HTML tags: i need to stop using all my high speed data before it resets not being able to see photos is so ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i need to stop using all my high speed data before it resets not being able to see photos is so ghetto\n",
            "Text after tokenization: ['i', 'need', 'to', 'stop', 'using', 'all', 'my', 'high', 'speed', 'data', 'before', 'it', 'resets', 'not', 'being', 'able', 'to', 'see', 'photos', 'is', 'so', 'ghetto']\n",
            "Text after removing stop words: ['need', 'stop', 'using', 'high', 'speed', 'data', 'resets', 'able', 'see', 'photos', 'ghetto']\n",
            "Text after Lemmatization: ['need', 'stop', 'using', 'high', 'speed', 'data', 'reset', 'able', 'see', 'photo', 'ghetto']\n",
            "Final pre-processed text: need stop using high speed data reset able see photo ghetto\n",
            "Text after removing HTML tags: when most people hate your camera angle choice \n",
            "Text after removing non-alphabetic characters and converting to lowercase: when most people hate your camera angle choice \n",
            "Text after tokenization: ['when', 'most', 'people', 'hate', 'your', 'camera', 'angle', 'choice']\n",
            "Text after removing stop words: ['people', 'hate', 'camera', 'angle', 'choice']\n",
            "Text after Lemmatization: ['people', 'hate', 'camera', 'angle', 'choice']\n",
            "Final pre-processed text: people hate camera angle choice\n",
            "Text after removing HTML tags: shut up retard god damn shut up\n",
            "Text after removing non-alphabetic characters and converting to lowercase: shut up retard god damn shut up\n",
            "Text after tokenization: ['shut', 'up', 'retard', 'god', 'damn', 'shut', 'up']\n",
            "Text after removing stop words: ['shut', 'retard', 'god', 'damn', 'shut']\n",
            "Text after Lemmatization: ['shut', 'retard', 'god', 'damn', 'shut']\n",
            "Final pre-processed text: shut retard god damn shut\n",
            "Text after removing HTML tags: also jews always project their mentality onto their opponents before they attack them and so far your group is the only group who is doing the projection\n",
            "Text after removing non-alphabetic characters and converting to lowercase: also jews always project their mentality onto their opponents before they attack them and so far your group is the only group who is doing the projection\n",
            "Text after tokenization: ['also', 'jews', 'always', 'project', 'their', 'mentality', 'onto', 'their', 'opponents', 'before', 'they', 'attack', 'them', 'and', 'so', 'far', 'your', 'group', 'is', 'the', 'only', 'group', 'who', 'is', 'doing', 'the', 'projection']\n",
            "Text after removing stop words: ['also', 'jews', 'always', 'project', 'mentality', 'onto', 'opponents', 'attack', 'far', 'group', 'group', 'projection']\n",
            "Text after Lemmatization: ['also', 'jew', 'always', 'project', 'mentality', 'onto', 'opponent', 'attack', 'far', 'group', 'group', 'projection']\n",
            "Final pre-processed text: also jew always project mentality onto opponent attack far group group projection\n",
            "Text after removing HTML tags:  there should be no little mohadishu in america deport illegal muslims assimilate\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  there should be no little mohadishu in america deport illegal muslims assimilate\n",
            "Text after tokenization: ['there', 'should', 'be', 'no', 'little', 'mohadishu', 'in', 'america', 'deport', 'illegal', 'muslims', 'assimilate']\n",
            "Text after removing stop words: ['little', 'mohadishu', 'america', 'deport', 'illegal', 'muslims', 'assimilate']\n",
            "Text after Lemmatization: ['little', 'mohadishu', 'america', 'deport', 'illegal', 'muslim', 'assimilate']\n",
            "Final pre-processed text: little mohadishu america deport illegal muslim assimilate\n",
            "Text after removing HTML tags:  not pacsun though because im not a dyke 🚫 🏳 ️ ‍ 🌈 🚫\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  not pacsun though because im not a dyke            \n",
            "Text after tokenization: ['not', 'pacsun', 'though', 'because', 'im', 'not', 'a', 'dyke']\n",
            "Text after removing stop words: ['pacsun', 'though', 'im', 'dyke']\n",
            "Text after Lemmatization: ['pacsun', 'though', 'im', 'dyke']\n",
            "Final pre-processed text: pacsun though im dyke\n",
            "Text after removing HTML tags: how do niggas be  n gay like did you even try pussy my nigga\n",
            "Text after removing non-alphabetic characters and converting to lowercase: how do niggas be  n gay like did you even try pussy my nigga\n",
            "Text after tokenization: ['how', 'do', 'niggas', 'be', 'n', 'gay', 'like', 'did', 'you', 'even', 'try', 'pussy', 'my', 'nigga']\n",
            "Text after removing stop words: ['niggas', 'n', 'gay', 'like', 'even', 'try', 'pussy', 'nigga']\n",
            "Text after Lemmatization: ['nigga', 'n', 'gay', 'like', 'even', 'try', 'pussy', 'nigga']\n",
            "Final pre-processed text: nigga n gay like even try pussy nigga\n",
            "Text after removing HTML tags:  who cares which out burkha hijab its haram completely unislamic gunnah sharia law punishment according to hadiz\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  who cares which out burkha hijab its haram completely unislamic gunnah sharia law punishment according to hadiz\n",
            "Text after tokenization: ['who', 'cares', 'which', 'out', 'burkha', 'hijab', 'its', 'haram', 'completely', 'unislamic', 'gunnah', 'sharia', 'law', 'punishment', 'according', 'to', 'hadiz']\n",
            "Text after removing stop words: ['cares', 'burkha', 'hijab', 'haram', 'completely', 'unislamic', 'gunnah', 'sharia', 'law', 'punishment', 'according', 'hadiz']\n",
            "Text after Lemmatization: ['care', 'burkha', 'hijab', 'haram', 'completely', 'unislamic', 'gunnah', 'sharia', 'law', 'punishment', 'according', 'hadiz']\n",
            "Final pre-processed text: care burkha hijab haram completely unislamic gunnah sharia law punishment according hadiz\n",
            "Text after removing HTML tags:  frere t ’ es gay\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  frere t   es gay\n",
            "Text after tokenization: ['frere', 't', 'es', 'gay']\n",
            "Text after removing stop words: ['frere', 'es', 'gay']\n",
            "Text after Lemmatization: ['frere', 'e', 'gay']\n",
            "Final pre-processed text: frere e gay\n",
            "Text after removing HTML tags: especially helping the refugees where to even begin this is so sad i don ’ t want to look\n",
            "Text after removing non-alphabetic characters and converting to lowercase: especially helping the refugees where to even begin this is so sad i don   t want to look\n",
            "Text after tokenization: ['especially', 'helping', 'the', 'refugees', 'where', 'to', 'even', 'begin', 'this', 'is', 'so', 'sad', 'i', 'don', 't', 'want', 'to', 'look']\n",
            "Text after removing stop words: ['especially', 'helping', 'refugees', 'even', 'begin', 'sad', 'want', 'look']\n",
            "Text after Lemmatization: ['especially', 'helping', 'refugee', 'even', 'begin', 'sad', 'want', 'look']\n",
            "Final pre-processed text: especially helping refugee even begin sad want look\n",
            "Text after removing HTML tags: why i go around this white bitch going  mph in a  mph zone and she flips me off yeah ok i throw drinks bitch amp now you wet tf up\n",
            "Text after removing non-alphabetic characters and converting to lowercase: why i go around this white bitch going  mph in a  mph zone and she flips me off yeah ok i throw drinks bitch amp now you wet tf up\n",
            "Text after tokenization: ['why', 'i', 'go', 'around', 'this', 'white', 'bitch', 'going', 'mph', 'in', 'a', 'mph', 'zone', 'and', 'she', 'flips', 'me', 'off', 'yeah', 'ok', 'i', 'throw', 'drinks', 'bitch', 'amp', 'now', 'you', 'wet', 'tf', 'up']\n",
            "Text after removing stop words: ['go', 'around', 'white', 'bitch', 'going', 'mph', 'mph', 'zone', 'flips', 'yeah', 'ok', 'throw', 'drinks', 'bitch', 'amp', 'wet', 'tf']\n",
            "Text after Lemmatization: ['go', 'around', 'white', 'bitch', 'going', 'mph', 'mph', 'zone', 'flip', 'yeah', 'ok', 'throw', 'drink', 'bitch', 'amp', 'wet', 'tf']\n",
            "Final pre-processed text: go around white bitch going mph mph zone flip yeah ok throw drink bitch amp wet tf\n",
            "Text after removing HTML tags:  lol muslims hate both lgbt and jews no one questions them tho\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  lol muslims hate both lgbt and jews no one questions them tho\n",
            "Text after tokenization: ['lol', 'muslims', 'hate', 'both', 'lgbt', 'and', 'jews', 'no', 'one', 'questions', 'them', 'tho']\n",
            "Text after removing stop words: ['lol', 'muslims', 'hate', 'lgbt', 'jews', 'one', 'questions', 'tho']\n",
            "Text after Lemmatization: ['lol', 'muslim', 'hate', 'lgbt', 'jew', 'one', 'question', 'tho']\n",
            "Final pre-processed text: lol muslim hate lgbt jew one question tho\n",
            "Text after removing HTML tags:  they know it very well bro they want their immigrant terrorist brothers to have a free run in our country that it\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  they know it very well bro they want their immigrant terrorist brothers to have a free run in our country that it\n",
            "Text after tokenization: ['they', 'know', 'it', 'very', 'well', 'bro', 'they', 'want', 'their', 'immigrant', 'terrorist', 'brothers', 'to', 'have', 'a', 'free', 'run', 'in', 'our', 'country', 'that', 'it']\n",
            "Text after removing stop words: ['know', 'well', 'bro', 'want', 'immigrant', 'terrorist', 'brothers', 'free', 'run', 'country']\n",
            "Text after Lemmatization: ['know', 'well', 'bro', 'want', 'immigrant', 'terrorist', 'brother', 'free', 'run', 'country']\n",
            "Final pre-processed text: know well bro want immigrant terrorist brother free run country\n",
            "Text after removing HTML tags:   northeast understandable they have apoint why muslims\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   northeast understandable they have apoint why muslims\n",
            "Text after tokenization: ['northeast', 'understandable', 'they', 'have', 'apoint', 'why', 'muslims']\n",
            "Text after removing stop words: ['northeast', 'understandable', 'apoint', 'muslims']\n",
            "Text after Lemmatization: ['northeast', 'understandable', 'apoint', 'muslim']\n",
            "Final pre-processed text: northeast understandable apoint muslim\n",
            "Text after removing HTML tags:  lmao not hoe you have ptsd tho\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  lmao not hoe you have ptsd tho\n",
            "Text after tokenization: ['lmao', 'not', 'hoe', 'you', 'have', 'ptsd', 'tho']\n",
            "Text after removing stop words: ['lmao', 'hoe', 'ptsd', 'tho']\n",
            "Text after Lemmatization: ['lmao', 'hoe', 'ptsd', 'tho']\n",
            "Final pre-processed text: lmao hoe ptsd tho\n",
            "Text after removing HTML tags:  a muslim mp is being disciplined to act only as muslim representative and then ppl question why bjp does not give them enough ticket\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  a muslim mp is being disciplined to act only as muslim representative and then ppl question why bjp does not give them enough ticket\n",
            "Text after tokenization: ['a', 'muslim', 'mp', 'is', 'being', 'disciplined', 'to', 'act', 'only', 'as', 'muslim', 'representative', 'and', 'then', 'ppl', 'question', 'why', 'bjp', 'does', 'not', 'give', 'them', 'enough', 'ticket']\n",
            "Text after removing stop words: ['muslim', 'mp', 'disciplined', 'act', 'muslim', 'representative', 'ppl', 'question', 'bjp', 'give', 'enough', 'ticket']\n",
            "Text after Lemmatization: ['muslim', 'mp', 'disciplined', 'act', 'muslim', 'representative', 'ppl', 'question', 'bjp', 'give', 'enough', 'ticket']\n",
            "Final pre-processed text: muslim mp disciplined act muslim representative ppl question bjp give enough ticket\n",
            "Text after removing HTML tags: ha well it was obviously a dick move on the guy part but the uk funds antiwhite hate speech and of course mass immigration is a massive nativehobic antiwhite hate crime that unlike this tasteless joke will actually result in violence against jewish people\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ha well it was obviously a dick move on the guy part but the uk funds antiwhite hate speech and of course mass immigration is a massive nativehobic antiwhite hate crime that unlike this tasteless joke will actually result in violence against jewish people\n",
            "Text after tokenization: ['ha', 'well', 'it', 'was', 'obviously', 'a', 'dick', 'move', 'on', 'the', 'guy', 'part', 'but', 'the', 'uk', 'funds', 'antiwhite', 'hate', 'speech', 'and', 'of', 'course', 'mass', 'immigration', 'is', 'a', 'massive', 'nativehobic', 'antiwhite', 'hate', 'crime', 'that', 'unlike', 'this', 'tasteless', 'joke', 'will', 'actually', 'result', 'in', 'violence', 'against', 'jewish', 'people']\n",
            "Text after removing stop words: ['ha', 'well', 'obviously', 'dick', 'move', 'guy', 'part', 'uk', 'funds', 'antiwhite', 'hate', 'speech', 'course', 'mass', 'immigration', 'massive', 'nativehobic', 'antiwhite', 'hate', 'crime', 'unlike', 'tasteless', 'joke', 'actually', 'result', 'violence', 'jewish', 'people']\n",
            "Text after Lemmatization: ['ha', 'well', 'obviously', 'dick', 'move', 'guy', 'part', 'uk', 'fund', 'antiwhite', 'hate', 'speech', 'course', 'mass', 'immigration', 'massive', 'nativehobic', 'antiwhite', 'hate', 'crime', 'unlike', 'tasteless', 'joke', 'actually', 'result', 'violence', 'jewish', 'people']\n",
            "Final pre-processed text: ha well obviously dick move guy part uk fund antiwhite hate speech course mass immigration massive nativehobic antiwhite hate crime unlike tasteless joke actually result violence jewish people\n",
            "Text after removing HTML tags: no to broke girlfriends the ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: no to broke girlfriends the ghetto\n",
            "Text after tokenization: ['no', 'to', 'broke', 'girlfriends', 'the', 'ghetto']\n",
            "Text after removing stop words: ['broke', 'girlfriends', 'ghetto']\n",
            "Text after Lemmatization: ['broke', 'girlfriend', 'ghetto']\n",
            "Final pre-processed text: broke girlfriend ghetto\n",
            "Text after removing HTML tags: american sexy porn girls asians young fucking job wife ass ass\n",
            "Text after removing non-alphabetic characters and converting to lowercase: american sexy porn girls asians young fucking job wife ass ass\n",
            "Text after tokenization: ['american', 'sexy', 'porn', 'girls', 'asians', 'young', 'fucking', 'job', 'wife', 'ass', 'ass']\n",
            "Text after removing stop words: ['american', 'sexy', 'porn', 'girls', 'asians', 'young', 'fucking', 'job', 'wife', 'ass', 'ass']\n",
            "Text after Lemmatization: ['american', 'sexy', 'porn', 'girl', 'asian', 'young', 'fucking', 'job', 'wife', 'as', 'as']\n",
            "Final pre-processed text: american sexy porn girl asian young fucking job wife as as\n",
            "Text after removing HTML tags: y ’ all really on my tl harassing this girl cause her friend brought her sneakers the ghetto i swear y ’ all tell y ’ all worth everyday on here\n",
            "Text after removing non-alphabetic characters and converting to lowercase: y   all really on my tl harassing this girl cause her friend brought her sneakers the ghetto i swear y   all tell y   all worth everyday on here\n",
            "Text after tokenization: ['y', 'all', 'really', 'on', 'my', 'tl', 'harassing', 'this', 'girl', 'cause', 'her', 'friend', 'brought', 'her', 'sneakers', 'the', 'ghetto', 'i', 'swear', 'y', 'all', 'tell', 'y', 'all', 'worth', 'everyday', 'on', 'here']\n",
            "Text after removing stop words: ['really', 'tl', 'harassing', 'girl', 'cause', 'friend', 'brought', 'sneakers', 'ghetto', 'swear', 'tell', 'worth', 'everyday']\n",
            "Text after Lemmatization: ['really', 'tl', 'harassing', 'girl', 'cause', 'friend', 'brought', 'sneaker', 'ghetto', 'swear', 'tell', 'worth', 'everyday']\n",
            "Final pre-processed text: really tl harassing girl cause friend brought sneaker ghetto swear tell worth everyday\n",
            "Text after removing HTML tags: this sh*t  is so ghetto bro 🤦 🏽 ‍ ♀ ️\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this sh t  is so ghetto bro          \n",
            "Text after tokenization: ['this', 'sh', 't', 'is', 'so', 'ghetto', 'bro']\n",
            "Text after removing stop words: ['sh', 'ghetto', 'bro']\n",
            "Text after Lemmatization: ['sh', 'ghetto', 'bro']\n",
            "Final pre-processed text: sh ghetto bro\n",
            "Text after removing HTML tags: i dunno where does this h  on j3ws come from among msian muslims like to even idolise h tler im just speechless\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i dunno where does this h  on j ws come from among msian muslims like to even idolise h tler im just speechless\n",
            "Text after tokenization: ['i', 'dunno', 'where', 'does', 'this', 'h', 'on', 'j', 'ws', 'come', 'from', 'among', 'msian', 'muslims', 'like', 'to', 'even', 'idolise', 'h', 'tler', 'im', 'just', 'speechless']\n",
            "Text after removing stop words: ['dunno', 'h', 'j', 'ws', 'come', 'among', 'msian', 'muslims', 'like', 'even', 'idolise', 'h', 'tler', 'im', 'speechless']\n",
            "Text after Lemmatization: ['dunno', 'h', 'j', 'w', 'come', 'among', 'msian', 'muslim', 'like', 'even', 'idolise', 'h', 'tler', 'im', 'speechless']\n",
            "Final pre-processed text: dunno h j w come among msian muslim like even idolise h tler im speechless\n",
            "Text after removing HTML tags:  typical ghetto garbage\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  typical ghetto garbage\n",
            "Text after tokenization: ['typical', 'ghetto', 'garbage']\n",
            "Text after removing stop words: ['typical', 'ghetto', 'garbage']\n",
            "Text after Lemmatization: ['typical', 'ghetto', 'garbage']\n",
            "Final pre-processed text: typical ghetto garbage\n",
            "Text after removing HTML tags:   just like close friendship is basicly gay relationship\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   just like close friendship is basicly gay relationship\n",
            "Text after tokenization: ['just', 'like', 'close', 'friendship', 'is', 'basicly', 'gay', 'relationship']\n",
            "Text after removing stop words: ['like', 'close', 'friendship', 'basicly', 'gay', 'relationship']\n",
            "Text after Lemmatization: ['like', 'close', 'friendship', 'basicly', 'gay', 'relationship']\n",
            "Final pre-processed text: like close friendship basicly gay relationship\n",
            "Text after removing HTML tags:  ackshully all arabs are white and islam is technically white imperialism\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  ackshully all arabs are white and islam is technically white imperialism\n",
            "Text after tokenization: ['ackshully', 'all', 'arabs', 'are', 'white', 'and', 'islam', 'is', 'technically', 'white', 'imperialism']\n",
            "Text after removing stop words: ['ackshully', 'arabs', 'white', 'islam', 'technically', 'white', 'imperialism']\n",
            "Text after Lemmatization: ['ackshully', 'arab', 'white', 'islam', 'technically', 'white', 'imperialism']\n",
            "Final pre-processed text: ackshully arab white islam technically white imperialism\n",
            "Text after removing HTML tags:  graysons the ghetto currently\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  graysons the ghetto currently\n",
            "Text after tokenization: ['graysons', 'the', 'ghetto', 'currently']\n",
            "Text after removing stop words: ['graysons', 'ghetto', 'currently']\n",
            "Text after Lemmatization: ['graysons', 'ghetto', 'currently']\n",
            "Final pre-processed text: graysons ghetto currently\n",
            "Text after removing HTML tags: i need some airpods bc i ’ m tired of not being able to listen to music and charge my phone at the same time lmao how ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i need some airpods bc i   m tired of not being able to listen to music and charge my phone at the same time lmao how ghetto\n",
            "Text after tokenization: ['i', 'need', 'some', 'airpods', 'bc', 'i', 'm', 'tired', 'of', 'not', 'being', 'able', 'to', 'listen', 'to', 'music', 'and', 'charge', 'my', 'phone', 'at', 'the', 'same', 'time', 'lmao', 'how', 'ghetto']\n",
            "Text after removing stop words: ['need', 'airpods', 'bc', 'tired', 'able', 'listen', 'music', 'charge', 'phone', 'time', 'lmao', 'ghetto']\n",
            "Text after Lemmatization: ['need', 'airpods', 'bc', 'tired', 'able', 'listen', 'music', 'charge', 'phone', 'time', 'lmao', 'ghetto']\n",
            "Final pre-processed text: need airpods bc tired able listen music charge phone time lmao ghetto\n",
            "Text after removing HTML tags:  let not forget him shielding zaush not banning nazi imagery and cub porn until forced still allowing zoophilic content feral porn\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  let not forget him shielding zaush not banning nazi imagery and cub porn until forced still allowing zoophilic content feral porn\n",
            "Text after tokenization: ['let', 'not', 'forget', 'him', 'shielding', 'zaush', 'not', 'banning', 'nazi', 'imagery', 'and', 'cub', 'porn', 'until', 'forced', 'still', 'allowing', 'zoophilic', 'content', 'feral', 'porn']\n",
            "Text after removing stop words: ['let', 'forget', 'shielding', 'zaush', 'banning', 'nazi', 'imagery', 'cub', 'porn', 'forced', 'still', 'allowing', 'zoophilic', 'content', 'feral', 'porn']\n",
            "Text after Lemmatization: ['let', 'forget', 'shielding', 'zaush', 'banning', 'nazi', 'imagery', 'cub', 'porn', 'forced', 'still', 'allowing', 'zoophilic', 'content', 'feral', 'porn']\n",
            "Final pre-processed text: let forget shielding zaush banning nazi imagery cub porn forced still allowing zoophilic content feral porn\n",
            "Text after removing HTML tags: over the last week i was thinking that altretard pedo nazi faggot would become like the ultimate insult and then pewdiepie said nigger and the world came to an end no no apparently even the feelings of the altleft are retarded and unimaginataive\n",
            "Text after removing non-alphabetic characters and converting to lowercase: over the last week i was thinking that altretard pedo nazi faggot would become like the ultimate insult and then pewdiepie said nigger and the world came to an end no no apparently even the feelings of the altleft are retarded and unimaginataive\n",
            "Text after tokenization: ['over', 'the', 'last', 'week', 'i', 'was', 'thinking', 'that', 'altretard', 'pedo', 'nazi', 'faggot', 'would', 'become', 'like', 'the', 'ultimate', 'insult', 'and', 'then', 'pewdiepie', 'said', 'nigger', 'and', 'the', 'world', 'came', 'to', 'an', 'end', 'no', 'no', 'apparently', 'even', 'the', 'feelings', 'of', 'the', 'altleft', 'are', 'retarded', 'and', 'unimaginataive']\n",
            "Text after removing stop words: ['last', 'week', 'thinking', 'altretard', 'pedo', 'nazi', 'faggot', 'would', 'become', 'like', 'ultimate', 'insult', 'pewdiepie', 'said', 'nigger', 'world', 'came', 'end', 'apparently', 'even', 'feelings', 'altleft', 'retarded', 'unimaginataive']\n",
            "Text after Lemmatization: ['last', 'week', 'thinking', 'altretard', 'pedo', 'nazi', 'faggot', 'would', 'become', 'like', 'ultimate', 'insult', 'pewdiepie', 'said', 'nigger', 'world', 'came', 'end', 'apparently', 'even', 'feeling', 'altleft', 'retarded', 'unimaginataive']\n",
            "Final pre-processed text: last week thinking altretard pedo nazi faggot would become like ultimate insult pewdiepie said nigger world came end apparently even feeling altleft retarded unimaginataive\n",
            "Text after removing HTML tags: never try to prove yourself to anyone if they want to hate you they will just enjoy it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: never try to prove yourself to anyone if they want to hate you they will just enjoy it\n",
            "Text after tokenization: ['never', 'try', 'to', 'prove', 'yourself', 'to', 'anyone', 'if', 'they', 'want', 'to', 'hate', 'you', 'they', 'will', 'just', 'enjoy', 'it']\n",
            "Text after removing stop words: ['never', 'try', 'prove', 'anyone', 'want', 'hate', 'enjoy']\n",
            "Text after Lemmatization: ['never', 'try', 'prove', 'anyone', 'want', 'hate', 'enjoy']\n",
            "Final pre-processed text: never try prove anyone want hate enjoy\n",
            "Text after removing HTML tags: am i a bad person for reading cut refugees in   and i wondered whether they were going to do it vertically or horizontally\n",
            "Text after removing non-alphabetic characters and converting to lowercase: am i a bad person for reading cut refugees in   and i wondered whether they were going to do it vertically or horizontally\n",
            "Text after tokenization: ['am', 'i', 'a', 'bad', 'person', 'for', 'reading', 'cut', 'refugees', 'in', 'and', 'i', 'wondered', 'whether', 'they', 'were', 'going', 'to', 'do', 'it', 'vertically', 'or', 'horizontally']\n",
            "Text after removing stop words: ['bad', 'person', 'reading', 'cut', 'refugees', 'wondered', 'whether', 'going', 'vertically', 'horizontally']\n",
            "Text after Lemmatization: ['bad', 'person', 'reading', 'cut', 'refugee', 'wondered', 'whether', 'going', 'vertically', 'horizontally']\n",
            "Final pre-processed text: bad person reading cut refugee wondered whether going vertically horizontally\n",
            "Text after removing HTML tags: only thing i want to hear accept cut refugees to zero\n",
            "Text after removing non-alphabetic characters and converting to lowercase: only thing i want to hear accept cut refugees to zero\n",
            "Text after tokenization: ['only', 'thing', 'i', 'want', 'to', 'hear', 'accept', 'cut', 'refugees', 'to', 'zero']\n",
            "Text after removing stop words: ['thing', 'want', 'hear', 'accept', 'cut', 'refugees', 'zero']\n",
            "Text after Lemmatization: ['thing', 'want', 'hear', 'accept', 'cut', 'refugee', 'zero']\n",
            "Final pre-processed text: thing want hear accept cut refugee zero\n",
            "Text after removing HTML tags: wii dear iran it was us we don ’ t like you sand niggers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: wii dear iran it was us we don   t like you sand niggers\n",
            "Text after tokenization: ['wii', 'dear', 'iran', 'it', 'was', 'us', 'we', 'don', 't', 'like', 'you', 'sand', 'niggers']\n",
            "Text after removing stop words: ['wii', 'dear', 'iran', 'us', 'like', 'sand', 'niggers']\n",
            "Text after Lemmatization: ['wii', 'dear', 'iran', 'u', 'like', 'sand', 'nigger']\n",
            "Final pre-processed text: wii dear iran u like sand nigger\n",
            "Text after removing HTML tags: yesterday i was drunk asl sorry for the retarded tweets\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yesterday i was drunk asl sorry for the retarded tweets\n",
            "Text after tokenization: ['yesterday', 'i', 'was', 'drunk', 'asl', 'sorry', 'for', 'the', 'retarded', 'tweets']\n",
            "Text after removing stop words: ['yesterday', 'drunk', 'asl', 'sorry', 'retarded', 'tweets']\n",
            "Text after Lemmatization: ['yesterday', 'drunk', 'asl', 'sorry', 'retarded', 'tweet']\n",
            "Final pre-processed text: yesterday drunk asl sorry retarded tweet\n",
            "Text after removing HTML tags: gays y ’ all be dappin up ur guy friends or giving em hugs 😳 😳 😳\n",
            "Text after removing non-alphabetic characters and converting to lowercase: gays y   all be dappin up ur guy friends or giving em hugs      \n",
            "Text after tokenization: ['gays', 'y', 'all', 'be', 'dappin', 'up', 'ur', 'guy', 'friends', 'or', 'giving', 'em', 'hugs']\n",
            "Text after removing stop words: ['gays', 'dappin', 'ur', 'guy', 'friends', 'giving', 'em', 'hugs']\n",
            "Text after Lemmatization: ['gay', 'dappin', 'ur', 'guy', 'friend', 'giving', 'em', 'hug']\n",
            "Final pre-processed text: gay dappin ur guy friend giving em hug\n",
            "Text after removing HTML tags: all the statues of dead white men and burning the books white nationalists will be removed from the internet the borders will be thrown open further to decrease the waiting for the end game of ghettoizing  of white americans as has been done in south africa\n",
            "Text after removing non-alphabetic characters and converting to lowercase: all the statues of dead white men and burning the books white nationalists will be removed from the internet the borders will be thrown open further to decrease the waiting for the end game of ghettoizing  of white americans as has been done in south africa\n",
            "Text after tokenization: ['all', 'the', 'statues', 'of', 'dead', 'white', 'men', 'and', 'burning', 'the', 'books', 'white', 'nationalists', 'will', 'be', 'removed', 'from', 'the', 'internet', 'the', 'borders', 'will', 'be', 'thrown', 'open', 'further', 'to', 'decrease', 'the', 'waiting', 'for', 'the', 'end', 'game', 'of', 'ghettoizing', 'of', 'white', 'americans', 'as', 'has', 'been', 'done', 'in', 'south', 'africa']\n",
            "Text after removing stop words: ['statues', 'dead', 'white', 'men', 'burning', 'books', 'white', 'nationalists', 'removed', 'internet', 'borders', 'thrown', 'open', 'decrease', 'waiting', 'end', 'game', 'ghettoizing', 'white', 'americans', 'done', 'south', 'africa']\n",
            "Text after Lemmatization: ['statue', 'dead', 'white', 'men', 'burning', 'book', 'white', 'nationalist', 'removed', 'internet', 'border', 'thrown', 'open', 'decrease', 'waiting', 'end', 'game', 'ghettoizing', 'white', 'american', 'done', 'south', 'africa']\n",
            "Final pre-processed text: statue dead white men burning book white nationalist removed internet border thrown open decrease waiting end game ghettoizing white american done south africa\n",
            "Text after removing HTML tags: that why they call it germoney those fuckin muslim faggots\n",
            "Text after removing non-alphabetic characters and converting to lowercase: that why they call it germoney those fuckin muslim faggots\n",
            "Text after tokenization: ['that', 'why', 'they', 'call', 'it', 'germoney', 'those', 'fuckin', 'muslim', 'faggots']\n",
            "Text after removing stop words: ['call', 'germoney', 'fuckin', 'muslim', 'faggots']\n",
            "Text after Lemmatization: ['call', 'germoney', 'fuckin', 'muslim', 'faggot']\n",
            "Final pre-processed text: call germoney fuckin muslim faggot\n",
            "Text after removing HTML tags: antifa are retarded we could not have picked better enemies\n",
            "Text after removing non-alphabetic characters and converting to lowercase: antifa are retarded we could not have picked better enemies\n",
            "Text after tokenization: ['antifa', 'are', 'retarded', 'we', 'could', 'not', 'have', 'picked', 'better', 'enemies']\n",
            "Text after removing stop words: ['antifa', 'retarded', 'could', 'picked', 'better', 'enemies']\n",
            "Text after Lemmatization: ['antifa', 'retarded', 'could', 'picked', 'better', 'enemy']\n",
            "Final pre-processed text: antifa retarded could picked better enemy\n",
            "Text after removing HTML tags: how can you let someone like that go through so much pain i hate you  omg rot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: how can you let someone like that go through so much pain i hate you  omg rot\n",
            "Text after tokenization: ['how', 'can', 'you', 'let', 'someone', 'like', 'that', 'go', 'through', 'so', 'much', 'pain', 'i', 'hate', 'you', 'omg', 'rot']\n",
            "Text after removing stop words: ['let', 'someone', 'like', 'go', 'much', 'pain', 'hate', 'omg', 'rot']\n",
            "Text after Lemmatization: ['let', 'someone', 'like', 'go', 'much', 'pain', 'hate', 'omg', 'rot']\n",
            "Final pre-processed text: let someone like go much pain hate omg rot\n",
            "Text after removing HTML tags:  calling a thing a thing does not make it a thing it just makes you look retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  calling a thing a thing does not make it a thing it just makes you look retarded\n",
            "Text after tokenization: ['calling', 'a', 'thing', 'a', 'thing', 'does', 'not', 'make', 'it', 'a', 'thing', 'it', 'just', 'makes', 'you', 'look', 'retarded']\n",
            "Text after removing stop words: ['calling', 'thing', 'thing', 'make', 'thing', 'makes', 'look', 'retarded']\n",
            "Text after Lemmatization: ['calling', 'thing', 'thing', 'make', 'thing', 'make', 'look', 'retarded']\n",
            "Final pre-processed text: calling thing thing make thing make look retarded\n",
            "Text after removing HTML tags: the only way you can tell these sand niggers apart is by the size of the ashtrays on their heads\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the only way you can tell these sand niggers apart is by the size of the ashtrays on their heads\n",
            "Text after tokenization: ['the', 'only', 'way', 'you', 'can', 'tell', 'these', 'sand', 'niggers', 'apart', 'is', 'by', 'the', 'size', 'of', 'the', 'ashtrays', 'on', 'their', 'heads']\n",
            "Text after removing stop words: ['way', 'tell', 'sand', 'niggers', 'apart', 'size', 'ashtrays', 'heads']\n",
            "Text after Lemmatization: ['way', 'tell', 'sand', 'nigger', 'apart', 'size', 'ashtray', 'head']\n",
            "Final pre-processed text: way tell sand nigger apart size ashtray head\n",
            "Text after removing HTML tags: son i fucking hate being single 😂\n",
            "Text after removing non-alphabetic characters and converting to lowercase: son i fucking hate being single  \n",
            "Text after tokenization: ['son', 'i', 'fucking', 'hate', 'being', 'single']\n",
            "Text after removing stop words: ['son', 'fucking', 'hate', 'single']\n",
            "Text after Lemmatization: ['son', 'fucking', 'hate', 'single']\n",
            "Final pre-processed text: son fucking hate single\n",
            "Text after removing HTML tags:  boy gtfo talkin bout used to be 😂 if you stop acting retarded 🙄\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  boy gtfo talkin bout used to be   if you stop acting retarded  \n",
            "Text after tokenization: ['boy', 'gtfo', 'talkin', 'bout', 'used', 'to', 'be', 'if', 'you', 'stop', 'acting', 'retarded']\n",
            "Text after removing stop words: ['boy', 'gtfo', 'talkin', 'bout', 'used', 'stop', 'acting', 'retarded']\n",
            "Text after Lemmatization: ['boy', 'gtfo', 'talkin', 'bout', 'used', 'stop', 'acting', 'retarded']\n",
            "Final pre-processed text: boy gtfo talkin bout used stop acting retarded\n",
            "Text after removing HTML tags:  i remember when r stood for republican since  it stands for retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i remember when r stood for republican since  it stands for retarded\n",
            "Text after tokenization: ['i', 'remember', 'when', 'r', 'stood', 'for', 'republican', 'since', 'it', 'stands', 'for', 'retarded']\n",
            "Text after removing stop words: ['remember', 'r', 'stood', 'republican', 'since', 'stands', 'retarded']\n",
            "Text after Lemmatization: ['remember', 'r', 'stood', 'republican', 'since', 'stand', 'retarded']\n",
            "Final pre-processed text: remember r stood republican since stand retarded\n",
            "Text after removing HTML tags: i don ’ t wanna go to your homophobic fucking prom anyway\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i don   t wanna go to your homophobic fucking prom anyway\n",
            "Text after tokenization: ['i', 'don', 't', 'wan', 'na', 'go', 'to', 'your', 'homophobic', 'fucking', 'prom', 'anyway']\n",
            "Text after removing stop words: ['wan', 'na', 'go', 'homophobic', 'fucking', 'prom', 'anyway']\n",
            "Text after Lemmatization: ['wan', 'na', 'go', 'homophobic', 'fucking', 'prom', 'anyway']\n",
            "Final pre-processed text: wan na go homophobic fucking prom anyway\n",
            "Text after removing HTML tags: i only want to go to coachella for one day to see my king and queen frank ocean and lana del rey this is violence \n",
            "Text after removing non-alphabetic characters and converting to lowercase: i only want to go to coachella for one day to see my king and queen frank ocean and lana del rey this is violence \n",
            "Text after tokenization: ['i', 'only', 'want', 'to', 'go', 'to', 'coachella', 'for', 'one', 'day', 'to', 'see', 'my', 'king', 'and', 'queen', 'frank', 'ocean', 'and', 'lana', 'del', 'rey', 'this', 'is', 'violence']\n",
            "Text after removing stop words: ['want', 'go', 'coachella', 'one', 'day', 'see', 'king', 'queen', 'frank', 'ocean', 'lana', 'del', 'rey', 'violence']\n",
            "Text after Lemmatization: ['want', 'go', 'coachella', 'one', 'day', 'see', 'king', 'queen', 'frank', 'ocean', 'lana', 'del', 'rey', 'violence']\n",
            "Final pre-processed text: want go coachella one day see king queen frank ocean lana del rey violence\n",
            "Text after removing HTML tags: is twiiter is banned for muslims twitterstayfair\n",
            "Text after removing non-alphabetic characters and converting to lowercase: is twiiter is banned for muslims twitterstayfair\n",
            "Text after tokenization: ['is', 'twiiter', 'is', 'banned', 'for', 'muslims', 'twitterstayfair']\n",
            "Text after removing stop words: ['twiiter', 'banned', 'muslims', 'twitterstayfair']\n",
            "Text after Lemmatization: ['twiiter', 'banned', 'muslim', 'twitterstayfair']\n",
            "Final pre-processed text: twiiter banned muslim twitterstayfair\n",
            "Text after removing HTML tags:  at least a good percent don ’ t like it because “ ew it ’ like a cowboy movie ” yeah u fuckin retard that ’ the whole point\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  at least a good percent don   t like it because   ew it   like a cowboy movie   yeah u fuckin retard that   the whole point\n",
            "Text after tokenization: ['at', 'least', 'a', 'good', 'percent', 'don', 't', 'like', 'it', 'because', 'ew', 'it', 'like', 'a', 'cowboy', 'movie', 'yeah', 'u', 'fuckin', 'retard', 'that', 'the', 'whole', 'point']\n",
            "Text after removing stop words: ['least', 'good', 'percent', 'like', 'ew', 'like', 'cowboy', 'movie', 'yeah', 'u', 'fuckin', 'retard', 'whole', 'point']\n",
            "Text after Lemmatization: ['least', 'good', 'percent', 'like', 'ew', 'like', 'cowboy', 'movie', 'yeah', 'u', 'fuckin', 'retard', 'whole', 'point']\n",
            "Final pre-processed text: least good percent like ew like cowboy movie yeah u fuckin retard whole point\n",
            "Text after removing HTML tags: darrell revis took ya  receiver and made sure than nigga ain ’ t get  yards\n",
            "Text after removing non-alphabetic characters and converting to lowercase: darrell revis took ya  receiver and made sure than nigga ain   t get  yards\n",
            "Text after tokenization: ['darrell', 'revis', 'took', 'ya', 'receiver', 'and', 'made', 'sure', 'than', 'nigga', 'ain', 't', 'get', 'yards']\n",
            "Text after removing stop words: ['darrell', 'revis', 'took', 'ya', 'receiver', 'made', 'sure', 'nigga', 'get', 'yards']\n",
            "Text after Lemmatization: ['darrell', 'revis', 'took', 'ya', 'receiver', 'made', 'sure', 'nigga', 'get', 'yard']\n",
            "Final pre-processed text: darrell revis took ya receiver made sure nigga get yard\n",
            "Text after removing HTML tags:   ching chong nobody gives a fuck you atheist\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   ching chong nobody gives a fuck you atheist\n",
            "Text after tokenization: ['ching', 'chong', 'nobody', 'gives', 'a', 'fuck', 'you', 'atheist']\n",
            "Text after removing stop words: ['ching', 'chong', 'nobody', 'gives', 'fuck', 'atheist']\n",
            "Text after Lemmatization: ['ching', 'chong', 'nobody', 'give', 'fuck', 'atheist']\n",
            "Final pre-processed text: ching chong nobody give fuck atheist\n",
            "Text after removing HTML tags: notice how unattractive and thick most women are in politics it a man job it dirty it requires loose morals and it corrupt as hell a woman by definition is alien to such an environment so the worst of them go into it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: notice how unattractive and thick most women are in politics it a man job it dirty it requires loose morals and it corrupt as hell a woman by definition is alien to such an environment so the worst of them go into it\n",
            "Text after tokenization: ['notice', 'how', 'unattractive', 'and', 'thick', 'most', 'women', 'are', 'in', 'politics', 'it', 'a', 'man', 'job', 'it', 'dirty', 'it', 'requires', 'loose', 'morals', 'and', 'it', 'corrupt', 'as', 'hell', 'a', 'woman', 'by', 'definition', 'is', 'alien', 'to', 'such', 'an', 'environment', 'so', 'the', 'worst', 'of', 'them', 'go', 'into', 'it']\n",
            "Text after removing stop words: ['notice', 'unattractive', 'thick', 'women', 'politics', 'man', 'job', 'dirty', 'requires', 'loose', 'morals', 'corrupt', 'hell', 'woman', 'definition', 'alien', 'environment', 'worst', 'go']\n",
            "Text after Lemmatization: ['notice', 'unattractive', 'thick', 'woman', 'politics', 'man', 'job', 'dirty', 'requires', 'loose', 'moral', 'corrupt', 'hell', 'woman', 'definition', 'alien', 'environment', 'worst', 'go']\n",
            "Final pre-processed text: notice unattractive thick woman politics man job dirty requires loose moral corrupt hell woman definition alien environment worst go\n",
            "Text after removing HTML tags:  wait until it ’ too hot to live here we will be climate refugees auspol\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  wait until it   too hot to live here we will be climate refugees auspol\n",
            "Text after tokenization: ['wait', 'until', 'it', 'too', 'hot', 'to', 'live', 'here', 'we', 'will', 'be', 'climate', 'refugees', 'auspol']\n",
            "Text after removing stop words: ['wait', 'hot', 'live', 'climate', 'refugees', 'auspol']\n",
            "Text after Lemmatization: ['wait', 'hot', 'live', 'climate', 'refugee', 'auspol']\n",
            "Final pre-processed text: wait hot live climate refugee auspol\n",
            "Text after removing HTML tags:   remember when you collaborated with neo nazis on a seth rich conspiracy documentary\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   remember when you collaborated with neo nazis on a seth rich conspiracy documentary\n",
            "Text after tokenization: ['remember', 'when', 'you', 'collaborated', 'with', 'neo', 'nazis', 'on', 'a', 'seth', 'rich', 'conspiracy', 'documentary']\n",
            "Text after removing stop words: ['remember', 'collaborated', 'neo', 'nazis', 'seth', 'rich', 'conspiracy', 'documentary']\n",
            "Text after Lemmatization: ['remember', 'collaborated', 'neo', 'nazi', 'seth', 'rich', 'conspiracy', 'documentary']\n",
            "Final pre-processed text: remember collaborated neo nazi seth rich conspiracy documentary\n",
            "Text after removing HTML tags: the next cracker who makes some racist joke on asians getting coughed on\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the next cracker who makes some racist joke on asians getting coughed on\n",
            "Text after tokenization: ['the', 'next', 'cracker', 'who', 'makes', 'some', 'racist', 'joke', 'on', 'asians', 'getting', 'coughed', 'on']\n",
            "Text after removing stop words: ['next', 'cracker', 'makes', 'racist', 'joke', 'asians', 'getting', 'coughed']\n",
            "Text after Lemmatization: ['next', 'cracker', 'make', 'racist', 'joke', 'asian', 'getting', 'coughed']\n",
            "Final pre-processed text: next cracker make racist joke asian getting coughed\n",
            "Text after removing HTML tags: “ i identify as a transgender i just don ’ t have the money to do a sex change ” no lol you ’ re just a broke faggot 🤣 🤣\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   i identify as a transgender i just don   t have the money to do a sex change   no lol you   re just a broke faggot    \n",
            "Text after tokenization: ['i', 'identify', 'as', 'a', 'transgender', 'i', 'just', 'don', 't', 'have', 'the', 'money', 'to', 'do', 'a', 'sex', 'change', 'no', 'lol', 'you', 're', 'just', 'a', 'broke', 'faggot']\n",
            "Text after removing stop words: ['identify', 'transgender', 'money', 'sex', 'change', 'lol', 'broke', 'faggot']\n",
            "Text after Lemmatization: ['identify', 'transgender', 'money', 'sex', 'change', 'lol', 'broke', 'faggot']\n",
            "Final pre-processed text: identify transgender money sex change lol broke faggot\n",
            "Text after removing HTML tags:  were you or your relatives refugees on this boat\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  were you or your relatives refugees on this boat\n",
            "Text after tokenization: ['were', 'you', 'or', 'your', 'relatives', 'refugees', 'on', 'this', 'boat']\n",
            "Text after removing stop words: ['relatives', 'refugees', 'boat']\n",
            "Text after Lemmatization: ['relative', 'refugee', 'boat']\n",
            "Final pre-processed text: relative refugee boat\n",
            "Text after removing HTML tags:   stfu you retarded scumbag did you really verify the news or you spreading false information\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   stfu you retarded scumbag did you really verify the news or you spreading false information\n",
            "Text after tokenization: ['stfu', 'you', 'retarded', 'scumbag', 'did', 'you', 'really', 'verify', 'the', 'news', 'or', 'you', 'spreading', 'false', 'information']\n",
            "Text after removing stop words: ['stfu', 'retarded', 'scumbag', 'really', 'verify', 'news', 'spreading', 'false', 'information']\n",
            "Text after Lemmatization: ['stfu', 'retarded', 'scumbag', 'really', 'verify', 'news', 'spreading', 'false', 'information']\n",
            "Final pre-processed text: stfu retarded scumbag really verify news spreading false information\n",
            "Text after removing HTML tags: saying “ my kids ” “ my bd ” is so ghetto this man violated 🤦 🏽 ‍ ♀ ️ 😂\n",
            "Text after removing non-alphabetic characters and converting to lowercase: saying   my kids     my bd   is so ghetto this man violated            \n",
            "Text after tokenization: ['saying', 'my', 'kids', 'my', 'bd', 'is', 'so', 'ghetto', 'this', 'man', 'violated']\n",
            "Text after removing stop words: ['saying', 'kids', 'bd', 'ghetto', 'man', 'violated']\n",
            "Text after Lemmatization: ['saying', 'kid', 'bd', 'ghetto', 'man', 'violated']\n",
            "Final pre-processed text: saying kid bd ghetto man violated\n",
            "Text after removing HTML tags:   nah nazis are evil\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   nah nazis are evil\n",
            "Text after tokenization: ['nah', 'nazis', 'are', 'evil']\n",
            "Text after removing stop words: ['nah', 'nazis', 'evil']\n",
            "Text after Lemmatization: ['nah', 'nazi', 'evil']\n",
            "Final pre-processed text: nah nazi evil\n",
            "Text after removing HTML tags: i hate you ghetto hoes i hate oakland bro\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i hate you ghetto hoes i hate oakland bro\n",
            "Text after tokenization: ['i', 'hate', 'you', 'ghetto', 'hoes', 'i', 'hate', 'oakland', 'bro']\n",
            "Text after removing stop words: ['hate', 'ghetto', 'hoes', 'hate', 'oakland', 'bro']\n",
            "Text after Lemmatization: ['hate', 'ghetto', 'hoe', 'hate', 'oakland', 'bro']\n",
            "Final pre-processed text: hate ghetto hoe hate oakland bro\n",
            "Text after removing HTML tags: oh my god i fucking hate mardi gras traffic why the fuck do some many snowboomers think they ’ re entitled to the entirety of my fucking city\n",
            "Text after removing non-alphabetic characters and converting to lowercase: oh my god i fucking hate mardi gras traffic why the fuck do some many snowboomers think they   re entitled to the entirety of my fucking city\n",
            "Text after tokenization: ['oh', 'my', 'god', 'i', 'fucking', 'hate', 'mardi', 'gras', 'traffic', 'why', 'the', 'fuck', 'do', 'some', 'many', 'snowboomers', 'think', 'they', 're', 'entitled', 'to', 'the', 'entirety', 'of', 'my', 'fucking', 'city']\n",
            "Text after removing stop words: ['oh', 'god', 'fucking', 'hate', 'mardi', 'gras', 'traffic', 'fuck', 'many', 'snowboomers', 'think', 'entitled', 'entirety', 'fucking', 'city']\n",
            "Text after Lemmatization: ['oh', 'god', 'fucking', 'hate', 'mardi', 'gras', 'traffic', 'fuck', 'many', 'snowboomers', 'think', 'entitled', 'entirety', 'fucking', 'city']\n",
            "Final pre-processed text: oh god fucking hate mardi gras traffic fuck many snowboomers think entitled entirety fucking city\n",
            "Text after removing HTML tags: i tweet for the dykes and the dykes alone\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i tweet for the dykes and the dykes alone\n",
            "Text after tokenization: ['i', 'tweet', 'for', 'the', 'dykes', 'and', 'the', 'dykes', 'alone']\n",
            "Text after removing stop words: ['tweet', 'dykes', 'dykes', 'alone']\n",
            "Text after Lemmatization: ['tweet', 'dyke', 'dyke', 'alone']\n",
            "Final pre-processed text: tweet dyke dyke alone\n",
            "Text after removing HTML tags: they turned cancun ’ into the ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: they turned cancun   into the ghetto\n",
            "Text after tokenization: ['they', 'turned', 'cancun', 'into', 'the', 'ghetto']\n",
            "Text after removing stop words: ['turned', 'cancun', 'ghetto']\n",
            "Text after Lemmatization: ['turned', 'cancun', 'ghetto']\n",
            "Final pre-processed text: turned cancun ghetto\n",
            "Text after removing HTML tags:   i hate you so much im choking over here 🤣 🤣 🤣 🤣\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   i hate you so much im choking over here        \n",
            "Text after tokenization: ['i', 'hate', 'you', 'so', 'much', 'im', 'choking', 'over', 'here']\n",
            "Text after removing stop words: ['hate', 'much', 'im', 'choking']\n",
            "Text after Lemmatization: ['hate', 'much', 'im', 'choking']\n",
            "Final pre-processed text: hate much im choking\n",
            "Text after removing HTML tags: punk ass manly bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase: punk ass manly bitch\n",
            "Text after tokenization: ['punk', 'ass', 'manly', 'bitch']\n",
            "Text after removing stop words: ['punk', 'ass', 'manly', 'bitch']\n",
            "Text after Lemmatization: ['punk', 'as', 'manly', 'bitch']\n",
            "Final pre-processed text: punk as manly bitch\n",
            "Text after removing HTML tags: bouta get real ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: bouta get real ghetto\n",
            "Text after tokenization: ['bouta', 'get', 'real', 'ghetto']\n",
            "Text after removing stop words: ['bouta', 'get', 'real', 'ghetto']\n",
            "Text after Lemmatization: ['bouta', 'get', 'real', 'ghetto']\n",
            "Final pre-processed text: bouta get real ghetto\n",
            "Text after removing HTML tags:   i was going for the  muslims killed in the genocide rather than the year\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   i was going for the  muslims killed in the genocide rather than the year\n",
            "Text after tokenization: ['i', 'was', 'going', 'for', 'the', 'muslims', 'killed', 'in', 'the', 'genocide', 'rather', 'than', 'the', 'year']\n",
            "Text after removing stop words: ['going', 'muslims', 'killed', 'genocide', 'rather', 'year']\n",
            "Text after Lemmatization: ['going', 'muslim', 'killed', 'genocide', 'rather', 'year']\n",
            "Final pre-processed text: going muslim killed genocide rather year\n",
            "Text after removing HTML tags:  its in malaysia and indonesia too are all of those islam ke dushman\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  its in malaysia and indonesia too are all of those islam ke dushman\n",
            "Text after tokenization: ['its', 'in', 'malaysia', 'and', 'indonesia', 'too', 'are', 'all', 'of', 'those', 'islam', 'ke', 'dushman']\n",
            "Text after removing stop words: ['malaysia', 'indonesia', 'islam', 'ke', 'dushman']\n",
            "Text after Lemmatization: ['malaysia', 'indonesia', 'islam', 'ke', 'dushman']\n",
            "Final pre-processed text: malaysia indonesia islam ke dushman\n",
            "Text after removing HTML tags: fuck allah that wanna beef shit nigga lets make a mill 💰\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fuck allah that wanna beef shit nigga lets make a mill  \n",
            "Text after tokenization: ['fuck', 'allah', 'that', 'wan', 'na', 'beef', 'shit', 'nigga', 'lets', 'make', 'a', 'mill']\n",
            "Text after removing stop words: ['fuck', 'allah', 'wan', 'na', 'beef', 'shit', 'nigga', 'lets', 'make', 'mill']\n",
            "Text after Lemmatization: ['fuck', 'allah', 'wan', 'na', 'beef', 'shit', 'nigga', 'let', 'make', 'mill']\n",
            "Final pre-processed text: fuck allah wan na beef shit nigga let make mill\n",
            "Text after removing HTML tags: hate how impulsive my decisions are when things aren ’ t going well like that bitch was not me idk her 😔 😔\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hate how impulsive my decisions are when things aren   t going well like that bitch was not me idk her    \n",
            "Text after tokenization: ['hate', 'how', 'impulsive', 'my', 'decisions', 'are', 'when', 'things', 'aren', 't', 'going', 'well', 'like', 'that', 'bitch', 'was', 'not', 'me', 'idk', 'her']\n",
            "Text after removing stop words: ['hate', 'impulsive', 'decisions', 'things', 'going', 'well', 'like', 'bitch', 'idk']\n",
            "Text after Lemmatization: ['hate', 'impulsive', 'decision', 'thing', 'going', 'well', 'like', 'bitch', 'idk']\n",
            "Final pre-processed text: hate impulsive decision thing going well like bitch idk\n",
            "Text after removing HTML tags:   left pseudo seculars indirectly and the madarsa jihadis they are everywhere watch for them\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   left pseudo seculars indirectly and the madarsa jihadis they are everywhere watch for them\n",
            "Text after tokenization: ['left', 'pseudo', 'seculars', 'indirectly', 'and', 'the', 'madarsa', 'jihadis', 'they', 'are', 'everywhere', 'watch', 'for', 'them']\n",
            "Text after removing stop words: ['left', 'pseudo', 'seculars', 'indirectly', 'madarsa', 'jihadis', 'everywhere', 'watch']\n",
            "Text after Lemmatization: ['left', 'pseudo', 'secular', 'indirectly', 'madarsa', 'jihadis', 'everywhere', 'watch']\n",
            "Final pre-processed text: left pseudo secular indirectly madarsa jihadis everywhere watch\n",
            "Text after removing HTML tags:  u couldn ’ t pay me to be wit that ghetto ass bitch josaline 😂 i ’ d hit once and dip\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  u couldn   t pay me to be wit that ghetto ass bitch josaline   i   d hit once and dip\n",
            "Text after tokenization: ['u', 'couldn', 't', 'pay', 'me', 'to', 'be', 'wit', 'that', 'ghetto', 'ass', 'bitch', 'josaline', 'i', 'd', 'hit', 'once', 'and', 'dip']\n",
            "Text after removing stop words: ['u', 'pay', 'wit', 'ghetto', 'ass', 'bitch', 'josaline', 'hit', 'dip']\n",
            "Text after Lemmatization: ['u', 'pay', 'wit', 'ghetto', 'as', 'bitch', 'josaline', 'hit', 'dip']\n",
            "Final pre-processed text: u pay wit ghetto as bitch josaline hit dip\n",
            "Text after removing HTML tags:  you ’ re correct soros was a nazi\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  you   re correct soros was a nazi\n",
            "Text after tokenization: ['you', 're', 'correct', 'soros', 'was', 'a', 'nazi']\n",
            "Text after removing stop words: ['correct', 'soros', 'nazi']\n",
            "Text after Lemmatization: ['correct', 'soros', 'nazi']\n",
            "Final pre-processed text: correct soros nazi\n",
            "Text after removing HTML tags: i only hang out with guys because women make me malfunction and do and say dumb things because i am so unbelievably homosexual\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i only hang out with guys because women make me malfunction and do and say dumb things because i am so unbelievably homosexual\n",
            "Text after tokenization: ['i', 'only', 'hang', 'out', 'with', 'guys', 'because', 'women', 'make', 'me', 'malfunction', 'and', 'do', 'and', 'say', 'dumb', 'things', 'because', 'i', 'am', 'so', 'unbelievably', 'homosexual']\n",
            "Text after removing stop words: ['hang', 'guys', 'women', 'make', 'malfunction', 'say', 'dumb', 'things', 'unbelievably', 'homosexual']\n",
            "Text after Lemmatization: ['hang', 'guy', 'woman', 'make', 'malfunction', 'say', 'dumb', 'thing', 'unbelievably', 'homosexual']\n",
            "Final pre-processed text: hang guy woman make malfunction say dumb thing unbelievably homosexual\n",
            "Text after removing HTML tags: follow  and get follow back shehnazians bhai ka old account suspend ho gya plz follow kro sare westandbyyousana\n",
            "Text after removing non-alphabetic characters and converting to lowercase: follow  and get follow back shehnazians bhai ka old account suspend ho gya plz follow kro sare westandbyyousana\n",
            "Text after tokenization: ['follow', 'and', 'get', 'follow', 'back', 'shehnazians', 'bhai', 'ka', 'old', 'account', 'suspend', 'ho', 'gya', 'plz', 'follow', 'kro', 'sare', 'westandbyyousana']\n",
            "Text after removing stop words: ['follow', 'get', 'follow', 'back', 'shehnazians', 'bhai', 'ka', 'old', 'account', 'suspend', 'ho', 'gya', 'plz', 'follow', 'kro', 'sare', 'westandbyyousana']\n",
            "Text after Lemmatization: ['follow', 'get', 'follow', 'back', 'shehnazians', 'bhai', 'ka', 'old', 'account', 'suspend', 'ho', 'gya', 'plz', 'follow', 'kro', 'sare', 'westandbyyousana']\n",
            "Final pre-processed text: follow get follow back shehnazians bhai ka old account suspend ho gya plz follow kro sare westandbyyousana\n",
            "Text after removing HTML tags: corona virus trying to kill all of us and people still out here sad and depressed like u better enjoy life while you still can 🤣 💀 💀\n",
            "Text after removing non-alphabetic characters and converting to lowercase: corona virus trying to kill all of us and people still out here sad and depressed like u better enjoy life while you still can      \n",
            "Text after tokenization: ['corona', 'virus', 'trying', 'to', 'kill', 'all', 'of', 'us', 'and', 'people', 'still', 'out', 'here', 'sad', 'and', 'depressed', 'like', 'u', 'better', 'enjoy', 'life', 'while', 'you', 'still', 'can']\n",
            "Text after removing stop words: ['corona', 'virus', 'trying', 'kill', 'us', 'people', 'still', 'sad', 'depressed', 'like', 'u', 'better', 'enjoy', 'life', 'still']\n",
            "Text after Lemmatization: ['corona', 'virus', 'trying', 'kill', 'u', 'people', 'still', 'sad', 'depressed', 'like', 'u', 'better', 'enjoy', 'life', 'still']\n",
            "Final pre-processed text: corona virus trying kill u people still sad depressed like u better enjoy life still\n",
            "Text after removing HTML tags: how yall niggas find out she was cheating lmaoo\n",
            "Text after removing non-alphabetic characters and converting to lowercase: how yall niggas find out she was cheating lmaoo\n",
            "Text after tokenization: ['how', 'yall', 'niggas', 'find', 'out', 'she', 'was', 'cheating', 'lmaoo']\n",
            "Text after removing stop words: ['yall', 'niggas', 'find', 'cheating', 'lmaoo']\n",
            "Text after Lemmatization: ['yall', 'nigga', 'find', 'cheating', 'lmaoo']\n",
            "Final pre-processed text: yall nigga find cheating lmaoo\n",
            "Text after removing HTML tags:  “ asians are the cause of everything bad ”\n",
            "Text after removing non-alphabetic characters and converting to lowercase:    asians are the cause of everything bad  \n",
            "Text after tokenization: ['asians', 'are', 'the', 'cause', 'of', 'everything', 'bad']\n",
            "Text after removing stop words: ['asians', 'cause', 'everything', 'bad']\n",
            "Text after Lemmatization: ['asian', 'cause', 'everything', 'bad']\n",
            "Final pre-processed text: asian cause everything bad\n",
            "Text after removing HTML tags:  we all are jihadis 😂\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  we all are jihadis  \n",
            "Text after tokenization: ['we', 'all', 'are', 'jihadis']\n",
            "Text after removing stop words: ['jihadis']\n",
            "Text after Lemmatization: ['jihadis']\n",
            "Final pre-processed text: jihadis\n",
            "Text after removing HTML tags: non violent resistance only works if the government is unwilling to use violence against said resistance\n",
            "Text after removing non-alphabetic characters and converting to lowercase: non violent resistance only works if the government is unwilling to use violence against said resistance\n",
            "Text after tokenization: ['non', 'violent', 'resistance', 'only', 'works', 'if', 'the', 'government', 'is', 'unwilling', 'to', 'use', 'violence', 'against', 'said', 'resistance']\n",
            "Text after removing stop words: ['non', 'violent', 'resistance', 'works', 'government', 'unwilling', 'use', 'violence', 'said', 'resistance']\n",
            "Text after Lemmatization: ['non', 'violent', 'resistance', 'work', 'government', 'unwilling', 'use', 'violence', 'said', 'resistance']\n",
            "Final pre-processed text: non violent resistance work government unwilling use violence said resistance\n",
            "Text after removing HTML tags: i have jew relations but they are not friends zero jew friends i was raised in the south and i always point out how their retard politics made it very hard for a right wing mischling kid with a military dad raised in the south\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i have jew relations but they are not friends zero jew friends i was raised in the south and i always point out how their retard politics made it very hard for a right wing mischling kid with a military dad raised in the south\n",
            "Text after tokenization: ['i', 'have', 'jew', 'relations', 'but', 'they', 'are', 'not', 'friends', 'zero', 'jew', 'friends', 'i', 'was', 'raised', 'in', 'the', 'south', 'and', 'i', 'always', 'point', 'out', 'how', 'their', 'retard', 'politics', 'made', 'it', 'very', 'hard', 'for', 'a', 'right', 'wing', 'mischling', 'kid', 'with', 'a', 'military', 'dad', 'raised', 'in', 'the', 'south']\n",
            "Text after removing stop words: ['jew', 'relations', 'friends', 'zero', 'jew', 'friends', 'raised', 'south', 'always', 'point', 'retard', 'politics', 'made', 'hard', 'right', 'wing', 'mischling', 'kid', 'military', 'dad', 'raised', 'south']\n",
            "Text after Lemmatization: ['jew', 'relation', 'friend', 'zero', 'jew', 'friend', 'raised', 'south', 'always', 'point', 'retard', 'politics', 'made', 'hard', 'right', 'wing', 'mischling', 'kid', 'military', 'dad', 'raised', 'south']\n",
            "Final pre-processed text: jew relation friend zero jew friend raised south always point retard politics made hard right wing mischling kid military dad raised south\n",
            "Text after removing HTML tags: you ugly fat nigger what are you doing in a white internet zone your balls will be kk ed for this in real life do you understand what you did wrong me nigga\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you ugly fat nigger what are you doing in a white internet zone your balls will be kk ed for this in real life do you understand what you did wrong me nigga\n",
            "Text after tokenization: ['you', 'ugly', 'fat', 'nigger', 'what', 'are', 'you', 'doing', 'in', 'a', 'white', 'internet', 'zone', 'your', 'balls', 'will', 'be', 'kk', 'ed', 'for', 'this', 'in', 'real', 'life', 'do', 'you', 'understand', 'what', 'you', 'did', 'wrong', 'me', 'nigga']\n",
            "Text after removing stop words: ['ugly', 'fat', 'nigger', 'white', 'internet', 'zone', 'balls', 'kk', 'ed', 'real', 'life', 'understand', 'wrong', 'nigga']\n",
            "Text after Lemmatization: ['ugly', 'fat', 'nigger', 'white', 'internet', 'zone', 'ball', 'kk', 'ed', 'real', 'life', 'understand', 'wrong', 'nigga']\n",
            "Final pre-processed text: ugly fat nigger white internet zone ball kk ed real life understand wrong nigga\n",
            "Text after removing HTML tags: oopsies bernie sanders said that illegal immigrants taking our jobs 👀 👀 👀\n",
            "Text after removing non-alphabetic characters and converting to lowercase: oopsies bernie sanders said that illegal immigrants taking our jobs      \n",
            "Text after tokenization: ['oopsies', 'bernie', 'sanders', 'said', 'that', 'illegal', 'immigrants', 'taking', 'our', 'jobs']\n",
            "Text after removing stop words: ['oopsies', 'bernie', 'sanders', 'said', 'illegal', 'immigrants', 'taking', 'jobs']\n",
            "Text after Lemmatization: ['oopsies', 'bernie', 'sander', 'said', 'illegal', 'immigrant', 'taking', 'job']\n",
            "Final pre-processed text: oopsies bernie sander said illegal immigrant taking job\n",
            "Text after removing HTML tags: whoever is free they can trend for  use the tagline bhuladungawithsid bhuladungawithsid\n",
            "Text after removing non-alphabetic characters and converting to lowercase: whoever is free they can trend for  use the tagline bhuladungawithsid bhuladungawithsid\n",
            "Text after tokenization: ['whoever', 'is', 'free', 'they', 'can', 'trend', 'for', 'use', 'the', 'tagline', 'bhuladungawithsid', 'bhuladungawithsid']\n",
            "Text after removing stop words: ['whoever', 'free', 'trend', 'use', 'tagline', 'bhuladungawithsid', 'bhuladungawithsid']\n",
            "Text after Lemmatization: ['whoever', 'free', 'trend', 'use', 'tagline', 'bhuladungawithsid', 'bhuladungawithsid']\n",
            "Final pre-processed text: whoever free trend use tagline bhuladungawithsid bhuladungawithsid\n",
            "Text after removing HTML tags: so we all just house niggas huh\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so we all just house niggas huh\n",
            "Text after tokenization: ['so', 'we', 'all', 'just', 'house', 'niggas', 'huh']\n",
            "Text after removing stop words: ['house', 'niggas', 'huh']\n",
            "Text after Lemmatization: ['house', 'nigga', 'huh']\n",
            "Final pre-processed text: house nigga huh\n",
            "Text after removing HTML tags: these dumbass lahori and islamabadi fuckers are gonna get us all killed\n",
            "Text after removing non-alphabetic characters and converting to lowercase: these dumbass lahori and islamabadi fuckers are gonna get us all killed\n",
            "Text after tokenization: ['these', 'dumbass', 'lahori', 'and', 'islamabadi', 'fuckers', 'are', 'gon', 'na', 'get', 'us', 'all', 'killed']\n",
            "Text after removing stop words: ['dumbass', 'lahori', 'islamabadi', 'fuckers', 'gon', 'na', 'get', 'us', 'killed']\n",
            "Text after Lemmatization: ['dumbass', 'lahori', 'islamabadi', 'fucker', 'gon', 'na', 'get', 'u', 'killed']\n",
            "Final pre-processed text: dumbass lahori islamabadi fucker gon na get u killed\n",
            "Text after removing HTML tags:  im sorry 🥺 🥺 🥺 i literally be forgetting to send u my pictures cause i literally keep them for myself so ghetto 🥴\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  im sorry       i literally be forgetting to send u my pictures cause i literally keep them for myself so ghetto  \n",
            "Text after tokenization: ['im', 'sorry', 'i', 'literally', 'be', 'forgetting', 'to', 'send', 'u', 'my', 'pictures', 'cause', 'i', 'literally', 'keep', 'them', 'for', 'myself', 'so', 'ghetto']\n",
            "Text after removing stop words: ['im', 'sorry', 'literally', 'forgetting', 'send', 'u', 'pictures', 'cause', 'literally', 'keep', 'ghetto']\n",
            "Text after Lemmatization: ['im', 'sorry', 'literally', 'forgetting', 'send', 'u', 'picture', 'cause', 'literally', 'keep', 'ghetto']\n",
            "Final pre-processed text: im sorry literally forgetting send u picture cause literally keep ghetto\n",
            "Text after removing HTML tags: if they make fun of a religious preacher for crying then i can also make fun a homosexual for being a fucking faggot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if they make fun of a religious preacher for crying then i can also make fun a homosexual for being a fucking faggot\n",
            "Text after tokenization: ['if', 'they', 'make', 'fun', 'of', 'a', 'religious', 'preacher', 'for', 'crying', 'then', 'i', 'can', 'also', 'make', 'fun', 'a', 'homosexual', 'for', 'being', 'a', 'fucking', 'faggot']\n",
            "Text after removing stop words: ['make', 'fun', 'religious', 'preacher', 'crying', 'also', 'make', 'fun', 'homosexual', 'fucking', 'faggot']\n",
            "Text after Lemmatization: ['make', 'fun', 'religious', 'preacher', 'cry', 'also', 'make', 'fun', 'homosexual', 'fucking', 'faggot']\n",
            "Final pre-processed text: make fun religious preacher cry also make fun homosexual fucking faggot\n",
            "Text after removing HTML tags:   i can not tell if you are trolling retarded or afraid of losing your shitty job maybe all\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   i can not tell if you are trolling retarded or afraid of losing your shitty job maybe all\n",
            "Text after tokenization: ['i', 'can', 'not', 'tell', 'if', 'you', 'are', 'trolling', 'retarded', 'or', 'afraid', 'of', 'losing', 'your', 'shitty', 'job', 'maybe', 'all']\n",
            "Text after removing stop words: ['tell', 'trolling', 'retarded', 'afraid', 'losing', 'shitty', 'job', 'maybe']\n",
            "Text after Lemmatization: ['tell', 'trolling', 'retarded', 'afraid', 'losing', 'shitty', 'job', 'maybe']\n",
            "Final pre-processed text: tell trolling retarded afraid losing shitty job maybe\n",
            "Text after removing HTML tags:   tell her to fuck off what the hell is she doing posing as a rabbi vile common shit bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   tell her to fuck off what the hell is she doing posing as a rabbi vile common shit bitch\n",
            "Text after tokenization: ['tell', 'her', 'to', 'fuck', 'off', 'what', 'the', 'hell', 'is', 'she', 'doing', 'posing', 'as', 'a', 'rabbi', 'vile', 'common', 'shit', 'bitch']\n",
            "Text after removing stop words: ['tell', 'fuck', 'hell', 'posing', 'rabbi', 'vile', 'common', 'shit', 'bitch']\n",
            "Text after Lemmatization: ['tell', 'fuck', 'hell', 'posing', 'rabbi', 'vile', 'common', 'shit', 'bitch']\n",
            "Final pre-processed text: tell fuck hell posing rabbi vile common shit bitch\n",
            "Text after removing HTML tags: if it makes kicking arabs and the blickety blecks out easier than i am all for it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if it makes kicking arabs and the blickety blecks out easier than i am all for it\n",
            "Text after tokenization: ['if', 'it', 'makes', 'kicking', 'arabs', 'and', 'the', 'blickety', 'blecks', 'out', 'easier', 'than', 'i', 'am', 'all', 'for', 'it']\n",
            "Text after removing stop words: ['makes', 'kicking', 'arabs', 'blickety', 'blecks', 'easier']\n",
            "Text after Lemmatization: ['make', 'kicking', 'arab', 'blickety', 'blecks', 'easier']\n",
            "Final pre-processed text: make kicking arab blickety blecks easier\n",
            "Text after removing HTML tags: moslem ghettos breeding more terrorists pedo rapists whilst poncing off the brit taxpayer  moslem breeding will eventually bankrupt uk  moslem breeding will lead to civil war  civil war will mean moohamad will become most common name of dead people\n",
            "Text after removing non-alphabetic characters and converting to lowercase: moslem ghettos breeding more terrorists pedo rapists whilst poncing off the brit taxpayer  moslem breeding will eventually bankrupt uk  moslem breeding will lead to civil war  civil war will mean moohamad will become most common name of dead people\n",
            "Text after tokenization: ['moslem', 'ghettos', 'breeding', 'more', 'terrorists', 'pedo', 'rapists', 'whilst', 'poncing', 'off', 'the', 'brit', 'taxpayer', 'moslem', 'breeding', 'will', 'eventually', 'bankrupt', 'uk', 'moslem', 'breeding', 'will', 'lead', 'to', 'civil', 'war', 'civil', 'war', 'will', 'mean', 'moohamad', 'will', 'become', 'most', 'common', 'name', 'of', 'dead', 'people']\n",
            "Text after removing stop words: ['moslem', 'ghettos', 'breeding', 'terrorists', 'pedo', 'rapists', 'whilst', 'poncing', 'brit', 'taxpayer', 'moslem', 'breeding', 'eventually', 'bankrupt', 'uk', 'moslem', 'breeding', 'lead', 'civil', 'war', 'civil', 'war', 'mean', 'moohamad', 'become', 'common', 'name', 'dead', 'people']\n",
            "Text after Lemmatization: ['moslem', 'ghetto', 'breeding', 'terrorist', 'pedo', 'rapist', 'whilst', 'poncing', 'brit', 'taxpayer', 'moslem', 'breeding', 'eventually', 'bankrupt', 'uk', 'moslem', 'breeding', 'lead', 'civil', 'war', 'civil', 'war', 'mean', 'moohamad', 'become', 'common', 'name', 'dead', 'people']\n",
            "Final pre-processed text: moslem ghetto breeding terrorist pedo rapist whilst poncing brit taxpayer moslem breeding eventually bankrupt uk moslem breeding lead civil war civil war mean moohamad become common name dead people\n",
            "Text after removing HTML tags: ah yes the pseudo penis estrus rite of swinging wildly while some marxist racist white bitch teacher stands in the middle dance nicely caught on an african safari the cameraman must have set up for days behind a blind to catch that shot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ah yes the pseudo penis estrus rite of swinging wildly while some marxist racist white bitch teacher stands in the middle dance nicely caught on an african safari the cameraman must have set up for days behind a blind to catch that shot\n",
            "Text after tokenization: ['ah', 'yes', 'the', 'pseudo', 'penis', 'estrus', 'rite', 'of', 'swinging', 'wildly', 'while', 'some', 'marxist', 'racist', 'white', 'bitch', 'teacher', 'stands', 'in', 'the', 'middle', 'dance', 'nicely', 'caught', 'on', 'an', 'african', 'safari', 'the', 'cameraman', 'must', 'have', 'set', 'up', 'for', 'days', 'behind', 'a', 'blind', 'to', 'catch', 'that', 'shot']\n",
            "Text after removing stop words: ['ah', 'yes', 'pseudo', 'penis', 'estrus', 'rite', 'swinging', 'wildly', 'marxist', 'racist', 'white', 'bitch', 'teacher', 'stands', 'middle', 'dance', 'nicely', 'caught', 'african', 'safari', 'cameraman', 'must', 'set', 'days', 'behind', 'blind', 'catch', 'shot']\n",
            "Text after Lemmatization: ['ah', 'yes', 'pseudo', 'penis', 'estrus', 'rite', 'swinging', 'wildly', 'marxist', 'racist', 'white', 'bitch', 'teacher', 'stand', 'middle', 'dance', 'nicely', 'caught', 'african', 'safari', 'cameraman', 'must', 'set', 'day', 'behind', 'blind', 'catch', 'shot']\n",
            "Final pre-processed text: ah yes pseudo penis estrus rite swinging wildly marxist racist white bitch teacher stand middle dance nicely caught african safari cameraman must set day behind blind catch shot\n",
            "Text after removing HTML tags:   attacking me for my appearance doesn ’ t work when i got hoes in the inbox sending titties faggot\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   attacking me for my appearance doesn   t work when i got hoes in the inbox sending titties faggot\n",
            "Text after tokenization: ['attacking', 'me', 'for', 'my', 'appearance', 'doesn', 't', 'work', 'when', 'i', 'got', 'hoes', 'in', 'the', 'inbox', 'sending', 'titties', 'faggot']\n",
            "Text after removing stop words: ['attacking', 'appearance', 'work', 'got', 'hoes', 'inbox', 'sending', 'titties', 'faggot']\n",
            "Text after Lemmatization: ['attacking', 'appearance', 'work', 'got', 'hoe', 'inbox', 'sending', 'titty', 'faggot']\n",
            "Final pre-processed text: attacking appearance work got hoe inbox sending titty faggot\n",
            "Text after removing HTML tags: the hoes would hate me before i ever hate them cause ew the ghetto 😂\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the hoes would hate me before i ever hate them cause ew the ghetto  \n",
            "Text after tokenization: ['the', 'hoes', 'would', 'hate', 'me', 'before', 'i', 'ever', 'hate', 'them', 'cause', 'ew', 'the', 'ghetto']\n",
            "Text after removing stop words: ['hoes', 'would', 'hate', 'ever', 'hate', 'cause', 'ew', 'ghetto']\n",
            "Text after Lemmatization: ['hoe', 'would', 'hate', 'ever', 'hate', 'cause', 'ew', 'ghetto']\n",
            "Final pre-processed text: hoe would hate ever hate cause ew ghetto\n",
            "Text after removing HTML tags:  chalo jo bhi hua sahi hua for them only islam is important nothing else\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  chalo jo bhi hua sahi hua for them only islam is important nothing else\n",
            "Text after tokenization: ['chalo', 'jo', 'bhi', 'hua', 'sahi', 'hua', 'for', 'them', 'only', 'islam', 'is', 'important', 'nothing', 'else']\n",
            "Text after removing stop words: ['chalo', 'jo', 'bhi', 'hua', 'sahi', 'hua', 'islam', 'important', 'nothing', 'else']\n",
            "Text after Lemmatization: ['chalo', 'jo', 'bhi', 'hua', 'sahi', 'hua', 'islam', 'important', 'nothing', 'else']\n",
            "Final pre-processed text: chalo jo bhi hua sahi hua islam important nothing else\n",
            "Text after removing HTML tags: soo what countries taking american refugees because this system is acrumbling\n",
            "Text after removing non-alphabetic characters and converting to lowercase: soo what countries taking american refugees because this system is acrumbling\n",
            "Text after tokenization: ['soo', 'what', 'countries', 'taking', 'american', 'refugees', 'because', 'this', 'system', 'is', 'acrumbling']\n",
            "Text after removing stop words: ['soo', 'countries', 'taking', 'american', 'refugees', 'system', 'acrumbling']\n",
            "Text after Lemmatization: ['soo', 'country', 'taking', 'american', 'refugee', 'system', 'acrumbling']\n",
            "Final pre-processed text: soo country taking american refugee system acrumbling\n",
            "Text after removing HTML tags:  i heard asians got some fire though\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i heard asians got some fire though\n",
            "Text after tokenization: ['i', 'heard', 'asians', 'got', 'some', 'fire', 'though']\n",
            "Text after removing stop words: ['heard', 'asians', 'got', 'fire', 'though']\n",
            "Text after Lemmatization: ['heard', 'asian', 'got', 'fire', 'though']\n",
            "Final pre-processed text: heard asian got fire though\n",
            "Text after removing HTML tags: you have got to be a special kind of stupid to get into one of these rape victim transport systems piloted by goathumping sandniggers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you have got to be a special kind of stupid to get into one of these rape victim transport systems piloted by goathumping sandniggers\n",
            "Text after tokenization: ['you', 'have', 'got', 'to', 'be', 'a', 'special', 'kind', 'of', 'stupid', 'to', 'get', 'into', 'one', 'of', 'these', 'rape', 'victim', 'transport', 'systems', 'piloted', 'by', 'goathumping', 'sandniggers']\n",
            "Text after removing stop words: ['got', 'special', 'kind', 'stupid', 'get', 'one', 'rape', 'victim', 'transport', 'systems', 'piloted', 'goathumping', 'sandniggers']\n",
            "Text after Lemmatization: ['got', 'special', 'kind', 'stupid', 'get', 'one', 'rape', 'victim', 'transport', 'system', 'piloted', 'goathumping', 'sandniggers']\n",
            "Final pre-processed text: got special kind stupid get one rape victim transport system piloted goathumping sandniggers\n",
            "Text after removing HTML tags:  i literally hate you\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i literally hate you\n",
            "Text after tokenization: ['i', 'literally', 'hate', 'you']\n",
            "Text after removing stop words: ['literally', 'hate']\n",
            "Text after Lemmatization: ['literally', 'hate']\n",
            "Final pre-processed text: literally hate\n",
            "Text after removing HTML tags:  mf really pinned that bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  mf really pinned that bitch\n",
            "Text after tokenization: ['mf', 'really', 'pinned', 'that', 'bitch']\n",
            "Text after removing stop words: ['mf', 'really', 'pinned', 'bitch']\n",
            "Text after Lemmatization: ['mf', 'really', 'pinned', 'bitch']\n",
            "Final pre-processed text: mf really pinned bitch\n",
            "Text after removing HTML tags:  you are retarded get back inside\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  you are retarded get back inside\n",
            "Text after tokenization: ['you', 'are', 'retarded', 'get', 'back', 'inside']\n",
            "Text after removing stop words: ['retarded', 'get', 'back', 'inside']\n",
            "Text after Lemmatization: ['retarded', 'get', 'back', 'inside']\n",
            "Final pre-processed text: retarded get back inside\n",
            "Text after removing HTML tags:   nah it ’ just retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   nah it   just retarded\n",
            "Text after tokenization: ['nah', 'it', 'just', 'retarded']\n",
            "Text after removing stop words: ['nah', 'retarded']\n",
            "Text after Lemmatization: ['nah', 'retarded']\n",
            "Final pre-processed text: nah retarded\n",
            "Text after removing HTML tags:  having a girlfriend super gay\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  having a girlfriend super gay\n",
            "Text after tokenization: ['having', 'a', 'girlfriend', 'super', 'gay']\n",
            "Text after removing stop words: ['girlfriend', 'super', 'gay']\n",
            "Text after Lemmatization: ['girlfriend', 'super', 'gay']\n",
            "Final pre-processed text: girlfriend super gay\n",
            "Text after removing HTML tags:  life of few people islam islam islam islam islam muslim muslim muslim and die\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  life of few people islam islam islam islam islam muslim muslim muslim and die\n",
            "Text after tokenization: ['life', 'of', 'few', 'people', 'islam', 'islam', 'islam', 'islam', 'islam', 'muslim', 'muslim', 'muslim', 'and', 'die']\n",
            "Text after removing stop words: ['life', 'people', 'islam', 'islam', 'islam', 'islam', 'islam', 'muslim', 'muslim', 'muslim', 'die']\n",
            "Text after Lemmatization: ['life', 'people', 'islam', 'islam', 'islam', 'islam', 'islam', 'muslim', 'muslim', 'muslim', 'die']\n",
            "Final pre-processed text: life people islam islam islam islam islam muslim muslim muslim die\n",
            "Text after removing HTML tags: sie look to nazis forget it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sie look to nazis forget it\n",
            "Text after tokenization: ['sie', 'look', 'to', 'nazis', 'forget', 'it']\n",
            "Text after removing stop words: ['sie', 'look', 'nazis', 'forget']\n",
            "Text after Lemmatization: ['sie', 'look', 'nazi', 'forget']\n",
            "Final pre-processed text: sie look nazi forget\n",
            "Text after removing HTML tags: just seen this nigga micheal jackson in the gulag\n",
            "Text after removing non-alphabetic characters and converting to lowercase: just seen this nigga micheal jackson in the gulag\n",
            "Text after tokenization: ['just', 'seen', 'this', 'nigga', 'micheal', 'jackson', 'in', 'the', 'gulag']\n",
            "Text after removing stop words: ['seen', 'nigga', 'micheal', 'jackson', 'gulag']\n",
            "Text after Lemmatization: ['seen', 'nigga', 'micheal', 'jackson', 'gulag']\n",
            "Final pre-processed text: seen nigga micheal jackson gulag\n",
            "Text after removing HTML tags:  put yourself in her shoes see debt pilling up to me oh everybody was at fault the family taraji and the guy\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  put yourself in her shoes see debt pilling up to me oh everybody was at fault the family taraji and the guy\n",
            "Text after tokenization: ['put', 'yourself', 'in', 'her', 'shoes', 'see', 'debt', 'pilling', 'up', 'to', 'me', 'oh', 'everybody', 'was', 'at', 'fault', 'the', 'family', 'taraji', 'and', 'the', 'guy']\n",
            "Text after removing stop words: ['put', 'shoes', 'see', 'debt', 'pilling', 'oh', 'everybody', 'fault', 'family', 'taraji', 'guy']\n",
            "Text after Lemmatization: ['put', 'shoe', 'see', 'debt', 'pilling', 'oh', 'everybody', 'fault', 'family', 'taraji', 'guy']\n",
            "Final pre-processed text: put shoe see debt pilling oh everybody fault family taraji guy\n",
            "Text after removing HTML tags:  you go by gay f word as well right\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  you go by gay f word as well right\n",
            "Text after tokenization: ['you', 'go', 'by', 'gay', 'f', 'word', 'as', 'well', 'right']\n",
            "Text after removing stop words: ['go', 'gay', 'f', 'word', 'well', 'right']\n",
            "Text after Lemmatization: ['go', 'gay', 'f', 'word', 'well', 'right']\n",
            "Final pre-processed text: go gay f word well right\n",
            "Text after removing HTML tags:  why are you smiling like you hate yourself\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  why are you smiling like you hate yourself\n",
            "Text after tokenization: ['why', 'are', 'you', 'smiling', 'like', 'you', 'hate', 'yourself']\n",
            "Text after removing stop words: ['smiling', 'like', 'hate']\n",
            "Text after Lemmatization: ['smiling', 'like', 'hate']\n",
            "Final pre-processed text: smiling like hate\n",
            "Text after removing HTML tags: i refuse to become homosexual but sometimes i really fucking hate these african american hood rat nigger bitches 🐐 goat\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i refuse to become homosexual but sometimes i really fucking hate these african american hood rat nigger bitches   goat\n",
            "Text after tokenization: ['i', 'refuse', 'to', 'become', 'homosexual', 'but', 'sometimes', 'i', 'really', 'fucking', 'hate', 'these', 'african', 'american', 'hood', 'rat', 'nigger', 'bitches', 'goat']\n",
            "Text after removing stop words: ['refuse', 'become', 'homosexual', 'sometimes', 'really', 'fucking', 'hate', 'african', 'american', 'hood', 'rat', 'nigger', 'bitches', 'goat']\n",
            "Text after Lemmatization: ['refuse', 'become', 'homosexual', 'sometimes', 'really', 'fucking', 'hate', 'african', 'american', 'hood', 'rat', 'nigger', 'bitch', 'goat']\n",
            "Final pre-processed text: refuse become homosexual sometimes really fucking hate african american hood rat nigger bitch goat\n",
            "Text after removing HTML tags:  guys i think stephen miller is a nazi just like other stephen miller is fuck off you white supremacist piece of shit\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  guys i think stephen miller is a nazi just like other stephen miller is fuck off you white supremacist piece of shit\n",
            "Text after tokenization: ['guys', 'i', 'think', 'stephen', 'miller', 'is', 'a', 'nazi', 'just', 'like', 'other', 'stephen', 'miller', 'is', 'fuck', 'off', 'you', 'white', 'supremacist', 'piece', 'of', 'shit']\n",
            "Text after removing stop words: ['guys', 'think', 'stephen', 'miller', 'nazi', 'like', 'stephen', 'miller', 'fuck', 'white', 'supremacist', 'piece', 'shit']\n",
            "Text after Lemmatization: ['guy', 'think', 'stephen', 'miller', 'nazi', 'like', 'stephen', 'miller', 'fuck', 'white', 'supremacist', 'piece', 'shit']\n",
            "Final pre-processed text: guy think stephen miller nazi like stephen miller fuck white supremacist piece shit\n",
            "Text after removing HTML tags: ghetto luv by peruzzi n dremo is a jam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ghetto luv by peruzzi n dremo is a jam\n",
            "Text after tokenization: ['ghetto', 'luv', 'by', 'peruzzi', 'n', 'dremo', 'is', 'a', 'jam']\n",
            "Text after removing stop words: ['ghetto', 'luv', 'peruzzi', 'n', 'dremo', 'jam']\n",
            "Text after Lemmatization: ['ghetto', 'luv', 'peruzzi', 'n', 'dremo', 'jam']\n",
            "Final pre-processed text: ghetto luv peruzzi n dremo jam\n",
            "Text after removing HTML tags: these ain t triple bitch these white lows\n",
            "Text after removing non-alphabetic characters and converting to lowercase: these ain t triple bitch these white lows\n",
            "Text after tokenization: ['these', 'ain', 't', 'triple', 'bitch', 'these', 'white', 'lows']\n",
            "Text after removing stop words: ['triple', 'bitch', 'white', 'lows']\n",
            "Text after Lemmatization: ['triple', 'bitch', 'white', 'low']\n",
            "Final pre-processed text: triple bitch white low\n",
            "Text after removing HTML tags:  disagree with me you should die sounds a like you agree with some of those nazi policies you like so much\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  disagree with me you should die sounds a like you agree with some of those nazi policies you like so much\n",
            "Text after tokenization: ['disagree', 'with', 'me', 'you', 'should', 'die', 'sounds', 'a', 'like', 'you', 'agree', 'with', 'some', 'of', 'those', 'nazi', 'policies', 'you', 'like', 'so', 'much']\n",
            "Text after removing stop words: ['disagree', 'die', 'sounds', 'like', 'agree', 'nazi', 'policies', 'like', 'much']\n",
            "Text after Lemmatization: ['disagree', 'die', 'sound', 'like', 'agree', 'nazi', 'policy', 'like', 'much']\n",
            "Final pre-processed text: disagree die sound like agree nazi policy like much\n",
            "Text after removing HTML tags: yesterday i went shopping and i did not buy a white blouse growth\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yesterday i went shopping and i did not buy a white blouse growth\n",
            "Text after tokenization: ['yesterday', 'i', 'went', 'shopping', 'and', 'i', 'did', 'not', 'buy', 'a', 'white', 'blouse', 'growth']\n",
            "Text after removing stop words: ['yesterday', 'went', 'shopping', 'buy', 'white', 'blouse', 'growth']\n",
            "Text after Lemmatization: ['yesterday', 'went', 'shopping', 'buy', 'white', 'blouse', 'growth']\n",
            "Final pre-processed text: yesterday went shopping buy white blouse growth\n",
            "Text after removing HTML tags: that because u blacks are eight times more likely to attack a hispanic person than a hispanic person attacking a black person blacks are twenty seven times more likely to attack a white person white nfl players should be kneeling to bring awareness to the massive black on white violence\n",
            "Text after removing non-alphabetic characters and converting to lowercase: that because u blacks are eight times more likely to attack a hispanic person than a hispanic person attacking a black person blacks are twenty seven times more likely to attack a white person white nfl players should be kneeling to bring awareness to the massive black on white violence\n",
            "Text after tokenization: ['that', 'because', 'u', 'blacks', 'are', 'eight', 'times', 'more', 'likely', 'to', 'attack', 'a', 'hispanic', 'person', 'than', 'a', 'hispanic', 'person', 'attacking', 'a', 'black', 'person', 'blacks', 'are', 'twenty', 'seven', 'times', 'more', 'likely', 'to', 'attack', 'a', 'white', 'person', 'white', 'nfl', 'players', 'should', 'be', 'kneeling', 'to', 'bring', 'awareness', 'to', 'the', 'massive', 'black', 'on', 'white', 'violence']\n",
            "Text after removing stop words: ['u', 'blacks', 'eight', 'times', 'likely', 'attack', 'hispanic', 'person', 'hispanic', 'person', 'attacking', 'black', 'person', 'blacks', 'twenty', 'seven', 'times', 'likely', 'attack', 'white', 'person', 'white', 'nfl', 'players', 'kneeling', 'bring', 'awareness', 'massive', 'black', 'white', 'violence']\n",
            "Text after Lemmatization: ['u', 'black', 'eight', 'time', 'likely', 'attack', 'hispanic', 'person', 'hispanic', 'person', 'attacking', 'black', 'person', 'black', 'twenty', 'seven', 'time', 'likely', 'attack', 'white', 'person', 'white', 'nfl', 'player', 'kneeling', 'bring', 'awareness', 'massive', 'black', 'white', 'violence']\n",
            "Final pre-processed text: u black eight time likely attack hispanic person hispanic person attacking black person black twenty seven time likely attack white person white nfl player kneeling bring awareness massive black white violence\n",
            "Text after removing HTML tags:  yeah white women brown refugee cured\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  yeah white women brown refugee cured\n",
            "Text after tokenization: ['yeah', 'white', 'women', 'brown', 'refugee', 'cured']\n",
            "Text after removing stop words: ['yeah', 'white', 'women', 'brown', 'refugee', 'cured']\n",
            "Text after Lemmatization: ['yeah', 'white', 'woman', 'brown', 'refugee', 'cured']\n",
            "Final pre-processed text: yeah white woman brown refugee cured\n",
            "Text after removing HTML tags:  saying anything shaming is the province of women and male feminists\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  saying anything shaming is the province of women and male feminists\n",
            "Text after tokenization: ['saying', 'anything', 'shaming', 'is', 'the', 'province', 'of', 'women', 'and', 'male', 'feminists']\n",
            "Text after removing stop words: ['saying', 'anything', 'shaming', 'province', 'women', 'male', 'feminists']\n",
            "Text after Lemmatization: ['saying', 'anything', 'shaming', 'province', 'woman', 'male', 'feminist']\n",
            "Final pre-processed text: saying anything shaming province woman male feminist\n",
            "Text after removing HTML tags:  kya ho gaya is everything ok\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  kya ho gaya is everything ok\n",
            "Text after tokenization: ['kya', 'ho', 'gaya', 'is', 'everything', 'ok']\n",
            "Text after removing stop words: ['kya', 'ho', 'gaya', 'everything', 'ok']\n",
            "Text after Lemmatization: ['kya', 'ho', 'gaya', 'everything', 'ok']\n",
            "Final pre-processed text: kya ho gaya everything ok\n",
            "Text after removing HTML tags:  my mom said it doesn ’ t even matter bc at the end of the day “ most asians if you trace them far back enough have chinese blood ”\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  my mom said it doesn   t even matter bc at the end of the day   most asians if you trace them far back enough have chinese blood  \n",
            "Text after tokenization: ['my', 'mom', 'said', 'it', 'doesn', 't', 'even', 'matter', 'bc', 'at', 'the', 'end', 'of', 'the', 'day', 'most', 'asians', 'if', 'you', 'trace', 'them', 'far', 'back', 'enough', 'have', 'chinese', 'blood']\n",
            "Text after removing stop words: ['mom', 'said', 'even', 'matter', 'bc', 'end', 'day', 'asians', 'trace', 'far', 'back', 'enough', 'chinese', 'blood']\n",
            "Text after Lemmatization: ['mom', 'said', 'even', 'matter', 'bc', 'end', 'day', 'asian', 'trace', 'far', 'back', 'enough', 'chinese', 'blood']\n",
            "Final pre-processed text: mom said even matter bc end day asian trace far back enough chinese blood\n",
            "Text after removing HTML tags: can not wait to see the european refugees flooding to america when their countries are overrun with crazy dogmatic extremists\n",
            "Text after removing non-alphabetic characters and converting to lowercase: can not wait to see the european refugees flooding to america when their countries are overrun with crazy dogmatic extremists\n",
            "Text after tokenization: ['can', 'not', 'wait', 'to', 'see', 'the', 'european', 'refugees', 'flooding', 'to', 'america', 'when', 'their', 'countries', 'are', 'overrun', 'with', 'crazy', 'dogmatic', 'extremists']\n",
            "Text after removing stop words: ['wait', 'see', 'european', 'refugees', 'flooding', 'america', 'countries', 'overrun', 'crazy', 'dogmatic', 'extremists']\n",
            "Text after Lemmatization: ['wait', 'see', 'european', 'refugee', 'flooding', 'america', 'country', 'overrun', 'crazy', 'dogmatic', 'extremist']\n",
            "Final pre-processed text: wait see european refugee flooding america country overrun crazy dogmatic extremist\n",
            "Text after removing HTML tags:  so basically people in the comments are mad because he is helping the immigrants\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  so basically people in the comments are mad because he is helping the immigrants\n",
            "Text after tokenization: ['so', 'basically', 'people', 'in', 'the', 'comments', 'are', 'mad', 'because', 'he', 'is', 'helping', 'the', 'immigrants']\n",
            "Text after removing stop words: ['basically', 'people', 'comments', 'mad', 'helping', 'immigrants']\n",
            "Text after Lemmatization: ['basically', 'people', 'comment', 'mad', 'helping', 'immigrant']\n",
            "Final pre-processed text: basically people comment mad helping immigrant\n",
            "Text after removing HTML tags:   wow that was beyond retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   wow that was beyond retarded\n",
            "Text after tokenization: ['wow', 'that', 'was', 'beyond', 'retarded']\n",
            "Text after removing stop words: ['wow', 'beyond', 'retarded']\n",
            "Text after Lemmatization: ['wow', 'beyond', 'retarded']\n",
            "Final pre-processed text: wow beyond retarded\n",
            "Text after removing HTML tags: non asians btw u cant make jokes like “ why do you hate asians ” “ do all asians look the same to you ” like u cant do that ❤ ️ thank u\n",
            "Text after removing non-alphabetic characters and converting to lowercase: non asians btw u cant make jokes like   why do you hate asians     do all asians look the same to you   like u cant do that     thank u\n",
            "Text after tokenization: ['non', 'asians', 'btw', 'u', 'cant', 'make', 'jokes', 'like', 'why', 'do', 'you', 'hate', 'asians', 'do', 'all', 'asians', 'look', 'the', 'same', 'to', 'you', 'like', 'u', 'cant', 'do', 'that', 'thank', 'u']\n",
            "Text after removing stop words: ['non', 'asians', 'btw', 'u', 'cant', 'make', 'jokes', 'like', 'hate', 'asians', 'asians', 'look', 'like', 'u', 'cant', 'thank', 'u']\n",
            "Text after Lemmatization: ['non', 'asian', 'btw', 'u', 'cant', 'make', 'joke', 'like', 'hate', 'asian', 'asian', 'look', 'like', 'u', 'cant', 'thank', 'u']\n",
            "Final pre-processed text: non asian btw u cant make joke like hate asian asian look like u cant thank u\n",
            "Text after removing HTML tags:  number of people being born under the poverty line all time high too but i guess let push for some retarded pseudo feminism\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  number of people being born under the poverty line all time high too but i guess let push for some retarded pseudo feminism\n",
            "Text after tokenization: ['number', 'of', 'people', 'being', 'born', 'under', 'the', 'poverty', 'line', 'all', 'time', 'high', 'too', 'but', 'i', 'guess', 'let', 'push', 'for', 'some', 'retarded', 'pseudo', 'feminism']\n",
            "Text after removing stop words: ['number', 'people', 'born', 'poverty', 'line', 'time', 'high', 'guess', 'let', 'push', 'retarded', 'pseudo', 'feminism']\n",
            "Text after Lemmatization: ['number', 'people', 'born', 'poverty', 'line', 'time', 'high', 'guess', 'let', 'push', 'retarded', 'pseudo', 'feminism']\n",
            "Final pre-processed text: number people born poverty line time high guess let push retarded pseudo feminism\n",
            "Text after removing HTML tags:  and democrats will never ever again call you racist sexist nazis no sir never\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  and democrats will never ever again call you racist sexist nazis no sir never\n",
            "Text after tokenization: ['and', 'democrats', 'will', 'never', 'ever', 'again', 'call', 'you', 'racist', 'sexist', 'nazis', 'no', 'sir', 'never']\n",
            "Text after removing stop words: ['democrats', 'never', 'ever', 'call', 'racist', 'sexist', 'nazis', 'sir', 'never']\n",
            "Text after Lemmatization: ['democrat', 'never', 'ever', 'call', 'racist', 'sexist', 'nazi', 'sir', 'never']\n",
            "Final pre-processed text: democrat never ever call racist sexist nazi sir never\n",
            "Text after removing HTML tags: a couple of retarded liberal has beens bend the knee for stupidity is anyone surprised does anyone care\n",
            "Text after removing non-alphabetic characters and converting to lowercase: a couple of retarded liberal has beens bend the knee for stupidity is anyone surprised does anyone care\n",
            "Text after tokenization: ['a', 'couple', 'of', 'retarded', 'liberal', 'has', 'beens', 'bend', 'the', 'knee', 'for', 'stupidity', 'is', 'anyone', 'surprised', 'does', 'anyone', 'care']\n",
            "Text after removing stop words: ['couple', 'retarded', 'liberal', 'beens', 'bend', 'knee', 'stupidity', 'anyone', 'surprised', 'anyone', 'care']\n",
            "Text after Lemmatization: ['couple', 'retarded', 'liberal', 'beens', 'bend', 'knee', 'stupidity', 'anyone', 'surprised', 'anyone', 'care']\n",
            "Final pre-processed text: couple retarded liberal beens bend knee stupidity anyone surprised anyone care\n",
            "Text after removing HTML tags:   you expect anything else from a retard\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   you expect anything else from a retard\n",
            "Text after tokenization: ['you', 'expect', 'anything', 'else', 'from', 'a', 'retard']\n",
            "Text after removing stop words: ['expect', 'anything', 'else', 'retard']\n",
            "Text after Lemmatization: ['expect', 'anything', 'else', 'retard']\n",
            "Final pre-processed text: expect anything else retard\n",
            "Text after removing HTML tags: refugees are the army that the no balls jews use against all gentiles its state sponsored terrorism time to revolt or die like a woman\n",
            "Text after removing non-alphabetic characters and converting to lowercase: refugees are the army that the no balls jews use against all gentiles its state sponsored terrorism time to revolt or die like a woman\n",
            "Text after tokenization: ['refugees', 'are', 'the', 'army', 'that', 'the', 'no', 'balls', 'jews', 'use', 'against', 'all', 'gentiles', 'its', 'state', 'sponsored', 'terrorism', 'time', 'to', 'revolt', 'or', 'die', 'like', 'a', 'woman']\n",
            "Text after removing stop words: ['refugees', 'army', 'balls', 'jews', 'use', 'gentiles', 'state', 'sponsored', 'terrorism', 'time', 'revolt', 'die', 'like', 'woman']\n",
            "Text after Lemmatization: ['refugee', 'army', 'ball', 'jew', 'use', 'gentile', 'state', 'sponsored', 'terrorism', 'time', 'revolt', 'die', 'like', 'woman']\n",
            "Final pre-processed text: refugee army ball jew use gentile state sponsored terrorism time revolt die like woman\n",
            "Text after removing HTML tags:  eid is only for muslim u idiot shut the fuck up with ur exmuslim\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  eid is only for muslim u idiot shut the fuck up with ur exmuslim\n",
            "Text after tokenization: ['eid', 'is', 'only', 'for', 'muslim', 'u', 'idiot', 'shut', 'the', 'fuck', 'up', 'with', 'ur', 'exmuslim']\n",
            "Text after removing stop words: ['eid', 'muslim', 'u', 'idiot', 'shut', 'fuck', 'ur', 'exmuslim']\n",
            "Text after Lemmatization: ['eid', 'muslim', 'u', 'idiot', 'shut', 'fuck', 'ur', 'exmuslim']\n",
            "Final pre-processed text: eid muslim u idiot shut fuck ur exmuslim\n",
            "Text after removing HTML tags: 💡 as long as the jews in hollyweird are gonna keep re making popular old movies but putting niggers in them they should re make the movie pretty woman but with a black hooker as the main character and they could call it ugly nigger 💡\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   as long as the jews in hollyweird are gonna keep re making popular old movies but putting niggers in them they should re make the movie pretty woman but with a black hooker as the main character and they could call it ugly nigger  \n",
            "Text after tokenization: ['as', 'long', 'as', 'the', 'jews', 'in', 'hollyweird', 'are', 'gon', 'na', 'keep', 're', 'making', 'popular', 'old', 'movies', 'but', 'putting', 'niggers', 'in', 'them', 'they', 'should', 're', 'make', 'the', 'movie', 'pretty', 'woman', 'but', 'with', 'a', 'black', 'hooker', 'as', 'the', 'main', 'character', 'and', 'they', 'could', 'call', 'it', 'ugly', 'nigger']\n",
            "Text after removing stop words: ['long', 'jews', 'hollyweird', 'gon', 'na', 'keep', 'making', 'popular', 'old', 'movies', 'putting', 'niggers', 'make', 'movie', 'pretty', 'woman', 'black', 'hooker', 'main', 'character', 'could', 'call', 'ugly', 'nigger']\n",
            "Text after Lemmatization: ['long', 'jew', 'hollyweird', 'gon', 'na', 'keep', 'making', 'popular', 'old', 'movie', 'putting', 'nigger', 'make', 'movie', 'pretty', 'woman', 'black', 'hooker', 'main', 'character', 'could', 'call', 'ugly', 'nigger']\n",
            "Final pre-processed text: long jew hollyweird gon na keep making popular old movie putting nigger make movie pretty woman black hooker main character could call ugly nigger\n",
            "Text after removing HTML tags: white people are so uncultured that they kill black people as a hobby blacklivesmatter allivesmatter\n",
            "Text after removing non-alphabetic characters and converting to lowercase: white people are so uncultured that they kill black people as a hobby blacklivesmatter allivesmatter\n",
            "Text after tokenization: ['white', 'people', 'are', 'so', 'uncultured', 'that', 'they', 'kill', 'black', 'people', 'as', 'a', 'hobby', 'blacklivesmatter', 'allivesmatter']\n",
            "Text after removing stop words: ['white', 'people', 'uncultured', 'kill', 'black', 'people', 'hobby', 'blacklivesmatter', 'allivesmatter']\n",
            "Text after Lemmatization: ['white', 'people', 'uncultured', 'kill', 'black', 'people', 'hobby', 'blacklivesmatter', 'allivesmatter']\n",
            "Final pre-processed text: white people uncultured kill black people hobby blacklivesmatter allivesmatter\n",
            "Text after removing HTML tags: i wanna be ghetto and buy some rims\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i wanna be ghetto and buy some rims\n",
            "Text after tokenization: ['i', 'wan', 'na', 'be', 'ghetto', 'and', 'buy', 'some', 'rims']\n",
            "Text after removing stop words: ['wan', 'na', 'ghetto', 'buy', 'rims']\n",
            "Text after Lemmatization: ['wan', 'na', 'ghetto', 'buy', 'rim']\n",
            "Final pre-processed text: wan na ghetto buy rim\n",
            "Text after removing HTML tags:  muslim twitter what is the hudud for stealing tweets\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  muslim twitter what is the hudud for stealing tweets\n",
            "Text after tokenization: ['muslim', 'twitter', 'what', 'is', 'the', 'hudud', 'for', 'stealing', 'tweets']\n",
            "Text after removing stop words: ['muslim', 'twitter', 'hudud', 'stealing', 'tweets']\n",
            "Text after Lemmatization: ['muslim', 'twitter', 'hudud', 'stealing', 'tweet']\n",
            "Final pre-processed text: muslim twitter hudud stealing tweet\n",
            "Text after removing HTML tags: death of jean charles de menezes jnagappan and all jews guilty of genocide must be exterminated by communism\n",
            "Text after removing non-alphabetic characters and converting to lowercase: death of jean charles de menezes jnagappan and all jews guilty of genocide must be exterminated by communism\n",
            "Text after tokenization: ['death', 'of', 'jean', 'charles', 'de', 'menezes', 'jnagappan', 'and', 'all', 'jews', 'guilty', 'of', 'genocide', 'must', 'be', 'exterminated', 'by', 'communism']\n",
            "Text after removing stop words: ['death', 'jean', 'charles', 'de', 'menezes', 'jnagappan', 'jews', 'guilty', 'genocide', 'must', 'exterminated', 'communism']\n",
            "Text after Lemmatization: ['death', 'jean', 'charles', 'de', 'menezes', 'jnagappan', 'jew', 'guilty', 'genocide', 'must', 'exterminated', 'communism']\n",
            "Final pre-processed text: death jean charles de menezes jnagappan jew guilty genocide must exterminated communism\n",
            "Text after removing HTML tags: on today ’ episode of white people virtue signaling\n",
            "Text after removing non-alphabetic characters and converting to lowercase: on today   episode of white people virtue signaling\n",
            "Text after tokenization: ['on', 'today', 'episode', 'of', 'white', 'people', 'virtue', 'signaling']\n",
            "Text after removing stop words: ['today', 'episode', 'white', 'people', 'virtue', 'signaling']\n",
            "Text after Lemmatization: ['today', 'episode', 'white', 'people', 'virtue', 'signaling']\n",
            "Final pre-processed text: today episode white people virtue signaling\n",
            "Text after removing HTML tags: i hate you ka opposite sidnaazshines\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i hate you ka opposite sidnaazshines\n",
            "Text after tokenization: ['i', 'hate', 'you', 'ka', 'opposite', 'sidnaazshines']\n",
            "Text after removing stop words: ['hate', 'ka', 'opposite', 'sidnaazshines']\n",
            "Text after Lemmatization: ['hate', 'ka', 'opposite', 'sidnaazshines']\n",
            "Final pre-processed text: hate ka opposite sidnaazshines\n",
            "Text after removing HTML tags:  it ’ black refugees speaking at the protest themselves and addressing their issues\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  it   black refugees speaking at the protest themselves and addressing their issues\n",
            "Text after tokenization: ['it', 'black', 'refugees', 'speaking', 'at', 'the', 'protest', 'themselves', 'and', 'addressing', 'their', 'issues']\n",
            "Text after removing stop words: ['black', 'refugees', 'speaking', 'protest', 'addressing', 'issues']\n",
            "Text after Lemmatization: ['black', 'refugee', 'speaking', 'protest', 'addressing', 'issue']\n",
            "Final pre-processed text: black refugee speaking protest addressing issue\n",
            "Text after removing HTML tags:   every single time nothing an islamist says can ever be trusted\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   every single time nothing an islamist says can ever be trusted\n",
            "Text after tokenization: ['every', 'single', 'time', 'nothing', 'an', 'islamist', 'says', 'can', 'ever', 'be', 'trusted']\n",
            "Text after removing stop words: ['every', 'single', 'time', 'nothing', 'islamist', 'says', 'ever', 'trusted']\n",
            "Text after Lemmatization: ['every', 'single', 'time', 'nothing', 'islamist', 'say', 'ever', 'trusted']\n",
            "Final pre-processed text: every single time nothing islamist say ever trusted\n",
            "Text after removing HTML tags:    has nicki minaj raped anyone  karruche filed a restraining order against cb stating he beat her\n",
            "Text after removing non-alphabetic characters and converting to lowercase:    has nicki minaj raped anyone  karruche filed a restraining order against cb stating he beat her\n",
            "Text after tokenization: ['has', 'nicki', 'minaj', 'raped', 'anyone', 'karruche', 'filed', 'a', 'restraining', 'order', 'against', 'cb', 'stating', 'he', 'beat', 'her']\n",
            "Text after removing stop words: ['nicki', 'minaj', 'raped', 'anyone', 'karruche', 'filed', 'restraining', 'order', 'cb', 'stating', 'beat']\n",
            "Text after Lemmatization: ['nicki', 'minaj', 'raped', 'anyone', 'karruche', 'filed', 'restraining', 'order', 'cb', 'stating', 'beat']\n",
            "Final pre-processed text: nicki minaj raped anyone karruche filed restraining order cb stating beat\n",
            "Text after removing HTML tags:   cunt ended up helping the nazis wanker\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   cunt ended up helping the nazis wanker\n",
            "Text after tokenization: ['cunt', 'ended', 'up', 'helping', 'the', 'nazis', 'wanker']\n",
            "Text after removing stop words: ['cunt', 'ended', 'helping', 'nazis', 'wanker']\n",
            "Text after Lemmatization: ['cunt', 'ended', 'helping', 'nazi', 'wanker']\n",
            "Final pre-processed text: cunt ended helping nazi wanker\n",
            "Text after removing HTML tags:  sometimes i think you might actually be a nigger is it difficult to keep up this ghetto persona\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  sometimes i think you might actually be a nigger is it difficult to keep up this ghetto persona\n",
            "Text after tokenization: ['sometimes', 'i', 'think', 'you', 'might', 'actually', 'be', 'a', 'nigger', 'is', 'it', 'difficult', 'to', 'keep', 'up', 'this', 'ghetto', 'persona']\n",
            "Text after removing stop words: ['sometimes', 'think', 'might', 'actually', 'nigger', 'difficult', 'keep', 'ghetto', 'persona']\n",
            "Text after Lemmatization: ['sometimes', 'think', 'might', 'actually', 'nigger', 'difficult', 'keep', 'ghetto', 'persona']\n",
            "Final pre-processed text: sometimes think might actually nigger difficult keep ghetto persona\n",
            "Text after removing HTML tags: muh holocaust  billion\n",
            "Text after removing non-alphabetic characters and converting to lowercase: muh holocaust  billion\n",
            "Text after tokenization: ['muh', 'holocaust', 'billion']\n",
            "Text after removing stop words: ['muh', 'holocaust', 'billion']\n",
            "Text after Lemmatization: ['muh', 'holocaust', 'billion']\n",
            "Final pre-processed text: muh holocaust billion\n",
            "Text after removing HTML tags: you know what i roughly gather from them banning me for saying that i gather the hidden message that i am supposed to be fucking blacks and arabs jesus christ another time i was kicked off for saying i do not do crack it a ghetto drug\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you know what i roughly gather from them banning me for saying that i gather the hidden message that i am supposed to be fucking blacks and arabs jesus christ another time i was kicked off for saying i do not do crack it a ghetto drug\n",
            "Text after tokenization: ['you', 'know', 'what', 'i', 'roughly', 'gather', 'from', 'them', 'banning', 'me', 'for', 'saying', 'that', 'i', 'gather', 'the', 'hidden', 'message', 'that', 'i', 'am', 'supposed', 'to', 'be', 'fucking', 'blacks', 'and', 'arabs', 'jesus', 'christ', 'another', 'time', 'i', 'was', 'kicked', 'off', 'for', 'saying', 'i', 'do', 'not', 'do', 'crack', 'it', 'a', 'ghetto', 'drug']\n",
            "Text after removing stop words: ['know', 'roughly', 'gather', 'banning', 'saying', 'gather', 'hidden', 'message', 'supposed', 'fucking', 'blacks', 'arabs', 'jesus', 'christ', 'another', 'time', 'kicked', 'saying', 'crack', 'ghetto', 'drug']\n",
            "Text after Lemmatization: ['know', 'roughly', 'gather', 'banning', 'saying', 'gather', 'hidden', 'message', 'supposed', 'fucking', 'black', 'arab', 'jesus', 'christ', 'another', 'time', 'kicked', 'saying', 'crack', 'ghetto', 'drug']\n",
            "Final pre-processed text: know roughly gather banning saying gather hidden message supposed fucking black arab jesus christ another time kicked saying crack ghetto drug\n",
            "Text after removing HTML tags: psa do not i repeat not trust your lily white ass cheeks to spray on sunscreens\n",
            "Text after removing non-alphabetic characters and converting to lowercase: psa do not i repeat not trust your lily white ass cheeks to spray on sunscreens\n",
            "Text after tokenization: ['psa', 'do', 'not', 'i', 'repeat', 'not', 'trust', 'your', 'lily', 'white', 'ass', 'cheeks', 'to', 'spray', 'on', 'sunscreens']\n",
            "Text after removing stop words: ['psa', 'repeat', 'trust', 'lily', 'white', 'ass', 'cheeks', 'spray', 'sunscreens']\n",
            "Text after Lemmatization: ['psa', 'repeat', 'trust', 'lily', 'white', 'as', 'cheek', 'spray', 'sunscreen']\n",
            "Final pre-processed text: psa repeat trust lily white as cheek spray sunscreen\n",
            "Text after removing HTML tags: i do not blame blacks for not feeling any connections with america this country was not meant for them and never will be they would be a lot better off if we send them back to africa\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i do not blame blacks for not feeling any connections with america this country was not meant for them and never will be they would be a lot better off if we send them back to africa\n",
            "Text after tokenization: ['i', 'do', 'not', 'blame', 'blacks', 'for', 'not', 'feeling', 'any', 'connections', 'with', 'america', 'this', 'country', 'was', 'not', 'meant', 'for', 'them', 'and', 'never', 'will', 'be', 'they', 'would', 'be', 'a', 'lot', 'better', 'off', 'if', 'we', 'send', 'them', 'back', 'to', 'africa']\n",
            "Text after removing stop words: ['blame', 'blacks', 'feeling', 'connections', 'america', 'country', 'meant', 'never', 'would', 'lot', 'better', 'send', 'back', 'africa']\n",
            "Text after Lemmatization: ['blame', 'black', 'feeling', 'connection', 'america', 'country', 'meant', 'never', 'would', 'lot', 'better', 'send', 'back', 'africa']\n",
            "Final pre-processed text: blame black feeling connection america country meant never would lot better send back africa\n",
            "Text after removing HTML tags: being a person is a fucking nightmare i did not ask for this 😤 😤\n",
            "Text after removing non-alphabetic characters and converting to lowercase: being a person is a fucking nightmare i did not ask for this    \n",
            "Text after tokenization: ['being', 'a', 'person', 'is', 'a', 'fucking', 'nightmare', 'i', 'did', 'not', 'ask', 'for', 'this']\n",
            "Text after removing stop words: ['person', 'fucking', 'nightmare', 'ask']\n",
            "Text after Lemmatization: ['person', 'fucking', 'nightmare', 'ask']\n",
            "Final pre-processed text: person fucking nightmare ask\n",
            "Text after removing HTML tags:  russia ’ neo nazi party calls itself the russian liberal democratic party\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  russia   neo nazi party calls itself the russian liberal democratic party\n",
            "Text after tokenization: ['russia', 'neo', 'nazi', 'party', 'calls', 'itself', 'the', 'russian', 'liberal', 'democratic', 'party']\n",
            "Text after removing stop words: ['russia', 'neo', 'nazi', 'party', 'calls', 'russian', 'liberal', 'democratic', 'party']\n",
            "Text after Lemmatization: ['russia', 'neo', 'nazi', 'party', 'call', 'russian', 'liberal', 'democratic', 'party']\n",
            "Final pre-processed text: russia neo nazi party call russian liberal democratic party\n",
            "Text after removing HTML tags: i ’ ll fucking hate when depression kicks it cause that mf be kicking in hard asf\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i   ll fucking hate when depression kicks it cause that mf be kicking in hard asf\n",
            "Text after tokenization: ['i', 'll', 'fucking', 'hate', 'when', 'depression', 'kicks', 'it', 'cause', 'that', 'mf', 'be', 'kicking', 'in', 'hard', 'asf']\n",
            "Text after removing stop words: ['fucking', 'hate', 'depression', 'kicks', 'cause', 'mf', 'kicking', 'hard', 'asf']\n",
            "Text after Lemmatization: ['fucking', 'hate', 'depression', 'kick', 'cause', 'mf', 'kicking', 'hard', 'asf']\n",
            "Final pre-processed text: fucking hate depression kick cause mf kicking hard asf\n",
            "Text after removing HTML tags:   yea because they dont know iraqi arabs as much as i do\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   yea because they dont know iraqi arabs as much as i do\n",
            "Text after tokenization: ['yea', 'because', 'they', 'dont', 'know', 'iraqi', 'arabs', 'as', 'much', 'as', 'i', 'do']\n",
            "Text after removing stop words: ['yea', 'dont', 'know', 'iraqi', 'arabs', 'much']\n",
            "Text after Lemmatization: ['yea', 'dont', 'know', 'iraqi', 'arab', 'much']\n",
            "Final pre-processed text: yea dont know iraqi arab much\n",
            "Text after removing HTML tags: only white people will preserve our whiteculture and europeanheritage tradlife altright\n",
            "Text after removing non-alphabetic characters and converting to lowercase: only white people will preserve our whiteculture and europeanheritage tradlife altright\n",
            "Text after tokenization: ['only', 'white', 'people', 'will', 'preserve', 'our', 'whiteculture', 'and', 'europeanheritage', 'tradlife', 'altright']\n",
            "Text after removing stop words: ['white', 'people', 'preserve', 'whiteculture', 'europeanheritage', 'tradlife', 'altright']\n",
            "Text after Lemmatization: ['white', 'people', 'preserve', 'whiteculture', 'europeanheritage', 'tradlife', 'altright']\n",
            "Final pre-processed text: white people preserve whiteculture europeanheritage tradlife altright\n",
            "Text after removing HTML tags: the only good muzzie is a dead muzzie\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the only good muzzie is a dead muzzie\n",
            "Text after tokenization: ['the', 'only', 'good', 'muzzie', 'is', 'a', 'dead', 'muzzie']\n",
            "Text after removing stop words: ['good', 'muzzie', 'dead', 'muzzie']\n",
            "Text after Lemmatization: ['good', 'muzzie', 'dead', 'muzzie']\n",
            "Final pre-processed text: good muzzie dead muzzie\n",
            "Text after removing HTML tags: they send their best and brightest to europe and america for education obviously becuase they sand niggers and gooks have the best universities and are oppressed by the evil white man learning institutions it is our learning institutions that oppress the brown nigger people\n",
            "Text after removing non-alphabetic characters and converting to lowercase: they send their best and brightest to europe and america for education obviously becuase they sand niggers and gooks have the best universities and are oppressed by the evil white man learning institutions it is our learning institutions that oppress the brown nigger people\n",
            "Text after tokenization: ['they', 'send', 'their', 'best', 'and', 'brightest', 'to', 'europe', 'and', 'america', 'for', 'education', 'obviously', 'becuase', 'they', 'sand', 'niggers', 'and', 'gooks', 'have', 'the', 'best', 'universities', 'and', 'are', 'oppressed', 'by', 'the', 'evil', 'white', 'man', 'learning', 'institutions', 'it', 'is', 'our', 'learning', 'institutions', 'that', 'oppress', 'the', 'brown', 'nigger', 'people']\n",
            "Text after removing stop words: ['send', 'best', 'brightest', 'europe', 'america', 'education', 'obviously', 'becuase', 'sand', 'niggers', 'gooks', 'best', 'universities', 'oppressed', 'evil', 'white', 'man', 'learning', 'institutions', 'learning', 'institutions', 'oppress', 'brown', 'nigger', 'people']\n",
            "Text after Lemmatization: ['send', 'best', 'brightest', 'europe', 'america', 'education', 'obviously', 'becuase', 'sand', 'nigger', 'gook', 'best', 'university', 'oppressed', 'evil', 'white', 'man', 'learning', 'institution', 'learning', 'institution', 'oppress', 'brown', 'nigger', 'people']\n",
            "Final pre-processed text: send best brightest europe america education obviously becuase sand nigger gook best university oppressed evil white man learning institution learning institution oppress brown nigger people\n",
            "Text after removing HTML tags: banning weev for making an observation is not supporting first amendment rights even if it were a call to violence as you said which it obviously was not in that context it was protected under 1 a we have linked you ppl brandenburg v ohio  million times\n",
            "Text after removing non-alphabetic characters and converting to lowercase: banning weev for making an observation is not supporting first amendment rights even if it were a call to violence as you said which it obviously was not in that context it was protected under   a we have linked you ppl brandenburg v ohio  million times\n",
            "Text after tokenization: ['banning', 'weev', 'for', 'making', 'an', 'observation', 'is', 'not', 'supporting', 'first', 'amendment', 'rights', 'even', 'if', 'it', 'were', 'a', 'call', 'to', 'violence', 'as', 'you', 'said', 'which', 'it', 'obviously', 'was', 'not', 'in', 'that', 'context', 'it', 'was', 'protected', 'under', 'a', 'we', 'have', 'linked', 'you', 'ppl', 'brandenburg', 'v', 'ohio', 'million', 'times']\n",
            "Text after removing stop words: ['banning', 'weev', 'making', 'observation', 'supporting', 'first', 'amendment', 'rights', 'even', 'call', 'violence', 'said', 'obviously', 'context', 'protected', 'linked', 'ppl', 'brandenburg', 'v', 'ohio', 'million', 'times']\n",
            "Text after Lemmatization: ['banning', 'weev', 'making', 'observation', 'supporting', 'first', 'amendment', 'right', 'even', 'call', 'violence', 'said', 'obviously', 'context', 'protected', 'linked', 'ppl', 'brandenburg', 'v', 'ohio', 'million', 'time']\n",
            "Final pre-processed text: banning weev making observation supporting first amendment right even call violence said obviously context protected linked ppl brandenburg v ohio million time\n",
            "Text after removing HTML tags: it time to purge these sand nigger savages from our country once and for all\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it time to purge these sand nigger savages from our country once and for all\n",
            "Text after tokenization: ['it', 'time', 'to', 'purge', 'these', 'sand', 'nigger', 'savages', 'from', 'our', 'country', 'once', 'and', 'for', 'all']\n",
            "Text after removing stop words: ['time', 'purge', 'sand', 'nigger', 'savages', 'country']\n",
            "Text after Lemmatization: ['time', 'purge', 'sand', 'nigger', 'savage', 'country']\n",
            "Final pre-processed text: time purge sand nigger savage country\n",
            "Text after removing HTML tags: i just overheard my girlfriend telling her sister that she taking her retarded boyfriend to the pub tonight i can not believe she cheating on me\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i just overheard my girlfriend telling her sister that she taking her retarded boyfriend to the pub tonight i can not believe she cheating on me\n",
            "Text after tokenization: ['i', 'just', 'overheard', 'my', 'girlfriend', 'telling', 'her', 'sister', 'that', 'she', 'taking', 'her', 'retarded', 'boyfriend', 'to', 'the', 'pub', 'tonight', 'i', 'can', 'not', 'believe', 'she', 'cheating', 'on', 'me']\n",
            "Text after removing stop words: ['overheard', 'girlfriend', 'telling', 'sister', 'taking', 'retarded', 'boyfriend', 'pub', 'tonight', 'believe', 'cheating']\n",
            "Text after Lemmatization: ['overheard', 'girlfriend', 'telling', 'sister', 'taking', 'retarded', 'boyfriend', 'pub', 'tonight', 'believe', 'cheating']\n",
            "Final pre-processed text: overheard girlfriend telling sister taking retarded boyfriend pub tonight believe cheating\n",
            "Text after removing HTML tags: tolerable slavs spanish italians greek hapas japanese maybe koreans chinese enemies blacks arabs south central americans mezistos mulatos indians pakis south east asians jews all ethnic even partially any mixes of these\n",
            "Text after removing non-alphabetic characters and converting to lowercase: tolerable slavs spanish italians greek hapas japanese maybe koreans chinese enemies blacks arabs south central americans mezistos mulatos indians pakis south east asians jews all ethnic even partially any mixes of these\n",
            "Text after tokenization: ['tolerable', 'slavs', 'spanish', 'italians', 'greek', 'hapas', 'japanese', 'maybe', 'koreans', 'chinese', 'enemies', 'blacks', 'arabs', 'south', 'central', 'americans', 'mezistos', 'mulatos', 'indians', 'pakis', 'south', 'east', 'asians', 'jews', 'all', 'ethnic', 'even', 'partially', 'any', 'mixes', 'of', 'these']\n",
            "Text after removing stop words: ['tolerable', 'slavs', 'spanish', 'italians', 'greek', 'hapas', 'japanese', 'maybe', 'koreans', 'chinese', 'enemies', 'blacks', 'arabs', 'south', 'central', 'americans', 'mezistos', 'mulatos', 'indians', 'pakis', 'south', 'east', 'asians', 'jews', 'ethnic', 'even', 'partially', 'mixes']\n",
            "Text after Lemmatization: ['tolerable', 'slav', 'spanish', 'italian', 'greek', 'hapas', 'japanese', 'maybe', 'korean', 'chinese', 'enemy', 'black', 'arab', 'south', 'central', 'american', 'mezistos', 'mulatos', 'indian', 'pakis', 'south', 'east', 'asian', 'jew', 'ethnic', 'even', 'partially', 'mix']\n",
            "Final pre-processed text: tolerable slav spanish italian greek hapas japanese maybe korean chinese enemy black arab south central american mezistos mulatos indian pakis south east asian jew ethnic even partially mix\n",
            "Text after removing HTML tags: the terror attack in las vegas is only the beginning if you stupid americans refuse to wake up and take action america will look like sweden in a very short time if you do not elect good people into office and protest radical islam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the terror attack in las vegas is only the beginning if you stupid americans refuse to wake up and take action america will look like sweden in a very short time if you do not elect good people into office and protest radical islam\n",
            "Text after tokenization: ['the', 'terror', 'attack', 'in', 'las', 'vegas', 'is', 'only', 'the', 'beginning', 'if', 'you', 'stupid', 'americans', 'refuse', 'to', 'wake', 'up', 'and', 'take', 'action', 'america', 'will', 'look', 'like', 'sweden', 'in', 'a', 'very', 'short', 'time', 'if', 'you', 'do', 'not', 'elect', 'good', 'people', 'into', 'office', 'and', 'protest', 'radical', 'islam']\n",
            "Text after removing stop words: ['terror', 'attack', 'las', 'vegas', 'beginning', 'stupid', 'americans', 'refuse', 'wake', 'take', 'action', 'america', 'look', 'like', 'sweden', 'short', 'time', 'elect', 'good', 'people', 'office', 'protest', 'radical', 'islam']\n",
            "Text after Lemmatization: ['terror', 'attack', 'la', 'vega', 'beginning', 'stupid', 'american', 'refuse', 'wake', 'take', 'action', 'america', 'look', 'like', 'sweden', 'short', 'time', 'elect', 'good', 'people', 'office', 'protest', 'radical', 'islam']\n",
            "Final pre-processed text: terror attack la vega beginning stupid american refuse wake take action america look like sweden short time elect good people office protest radical islam\n",
            "Text after removing HTML tags: the next step is to slap that bitch with a bundle of bacon\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the next step is to slap that bitch with a bundle of bacon\n",
            "Text after tokenization: ['the', 'next', 'step', 'is', 'to', 'slap', 'that', 'bitch', 'with', 'a', 'bundle', 'of', 'bacon']\n",
            "Text after removing stop words: ['next', 'step', 'slap', 'bitch', 'bundle', 'bacon']\n",
            "Text after Lemmatization: ['next', 'step', 'slap', 'bitch', 'bundle', 'bacon']\n",
            "Final pre-processed text: next step slap bitch bundle bacon\n",
            "Text after removing HTML tags: considering your low negro iq you basically gather dust in that empty gorilla skull speaking of wasted taxpayer dollars how is rutgers treating you are they still pretending you are a human what a wacky experiment\n",
            "Text after removing non-alphabetic characters and converting to lowercase: considering your low negro iq you basically gather dust in that empty gorilla skull speaking of wasted taxpayer dollars how is rutgers treating you are they still pretending you are a human what a wacky experiment\n",
            "Text after tokenization: ['considering', 'your', 'low', 'negro', 'iq', 'you', 'basically', 'gather', 'dust', 'in', 'that', 'empty', 'gorilla', 'skull', 'speaking', 'of', 'wasted', 'taxpayer', 'dollars', 'how', 'is', 'rutgers', 'treating', 'you', 'are', 'they', 'still', 'pretending', 'you', 'are', 'a', 'human', 'what', 'a', 'wacky', 'experiment']\n",
            "Text after removing stop words: ['considering', 'low', 'negro', 'iq', 'basically', 'gather', 'dust', 'empty', 'gorilla', 'skull', 'speaking', 'wasted', 'taxpayer', 'dollars', 'rutgers', 'treating', 'still', 'pretending', 'human', 'wacky', 'experiment']\n",
            "Text after Lemmatization: ['considering', 'low', 'negro', 'iq', 'basically', 'gather', 'dust', 'empty', 'gorilla', 'skull', 'speaking', 'wasted', 'taxpayer', 'dollar', 'rutgers', 'treating', 'still', 'pretending', 'human', 'wacky', 'experiment']\n",
            "Final pre-processed text: considering low negro iq basically gather dust empty gorilla skull speaking wasted taxpayer dollar rutgers treating still pretending human wacky experiment\n",
            "Text after removing HTML tags: you just let the jewish media do the thinking for you bud while australia is flooded with asians and other new australians see what kind of future you have when you are a white minority\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you just let the jewish media do the thinking for you bud while australia is flooded with asians and other new australians see what kind of future you have when you are a white minority\n",
            "Text after tokenization: ['you', 'just', 'let', 'the', 'jewish', 'media', 'do', 'the', 'thinking', 'for', 'you', 'bud', 'while', 'australia', 'is', 'flooded', 'with', 'asians', 'and', 'other', 'new', 'australians', 'see', 'what', 'kind', 'of', 'future', 'you', 'have', 'when', 'you', 'are', 'a', 'white', 'minority']\n",
            "Text after removing stop words: ['let', 'jewish', 'media', 'thinking', 'bud', 'australia', 'flooded', 'asians', 'new', 'australians', 'see', 'kind', 'future', 'white', 'minority']\n",
            "Text after Lemmatization: ['let', 'jewish', 'medium', 'thinking', 'bud', 'australia', 'flooded', 'asian', 'new', 'australian', 'see', 'kind', 'future', 'white', 'minority']\n",
            "Final pre-processed text: let jewish medium thinking bud australia flooded asian new australian see kind future white minority\n",
            "Text after removing HTML tags: welcome to boomer history  where all you do is share fancy pictures the young people make repeat whatever retarded shit you heard pre internet never fact checked since\n",
            "Text after removing non-alphabetic characters and converting to lowercase: welcome to boomer history  where all you do is share fancy pictures the young people make repeat whatever retarded shit you heard pre internet never fact checked since\n",
            "Text after tokenization: ['welcome', 'to', 'boomer', 'history', 'where', 'all', 'you', 'do', 'is', 'share', 'fancy', 'pictures', 'the', 'young', 'people', 'make', 'repeat', 'whatever', 'retarded', 'shit', 'you', 'heard', 'pre', 'internet', 'never', 'fact', 'checked', 'since']\n",
            "Text after removing stop words: ['welcome', 'boomer', 'history', 'share', 'fancy', 'pictures', 'young', 'people', 'make', 'repeat', 'whatever', 'retarded', 'shit', 'heard', 'pre', 'internet', 'never', 'fact', 'checked', 'since']\n",
            "Text after Lemmatization: ['welcome', 'boomer', 'history', 'share', 'fancy', 'picture', 'young', 'people', 'make', 'repeat', 'whatever', 'retarded', 'shit', 'heard', 'pre', 'internet', 'never', 'fact', 'checked', 'since']\n",
            "Final pre-processed text: welcome boomer history share fancy picture young people make repeat whatever retarded shit heard pre internet never fact checked since\n",
            "Text after removing HTML tags: unspeakable crimes to christians women and girls especially\n",
            "Text after removing non-alphabetic characters and converting to lowercase: unspeakable crimes to christians women and girls especially\n",
            "Text after tokenization: ['unspeakable', 'crimes', 'to', 'christians', 'women', 'and', 'girls', 'especially']\n",
            "Text after removing stop words: ['unspeakable', 'crimes', 'christians', 'women', 'girls', 'especially']\n",
            "Text after Lemmatization: ['unspeakable', 'crime', 'christian', 'woman', 'girl', 'especially']\n",
            "Final pre-processed text: unspeakable crime christian woman girl especially\n",
            "Text after removing HTML tags: another liberal lie disproved it quite the opposite but the leftist anti american mainstream media would have you believe otherwise conscious and aware no not the ghetto patois woke used by assholes and dipshits patriots know the truth\n",
            "Text after removing non-alphabetic characters and converting to lowercase: another liberal lie disproved it quite the opposite but the leftist anti american mainstream media would have you believe otherwise conscious and aware no not the ghetto patois woke used by assholes and dipshits patriots know the truth\n",
            "Text after tokenization: ['another', 'liberal', 'lie', 'disproved', 'it', 'quite', 'the', 'opposite', 'but', 'the', 'leftist', 'anti', 'american', 'mainstream', 'media', 'would', 'have', 'you', 'believe', 'otherwise', 'conscious', 'and', 'aware', 'no', 'not', 'the', 'ghetto', 'patois', 'woke', 'used', 'by', 'assholes', 'and', 'dipshits', 'patriots', 'know', 'the', 'truth']\n",
            "Text after removing stop words: ['another', 'liberal', 'lie', 'disproved', 'quite', 'opposite', 'leftist', 'anti', 'american', 'mainstream', 'media', 'would', 'believe', 'otherwise', 'conscious', 'aware', 'ghetto', 'patois', 'woke', 'used', 'assholes', 'dipshits', 'patriots', 'know', 'truth']\n",
            "Text after Lemmatization: ['another', 'liberal', 'lie', 'disproved', 'quite', 'opposite', 'leftist', 'anti', 'american', 'mainstream', 'medium', 'would', 'believe', 'otherwise', 'conscious', 'aware', 'ghetto', 'patois', 'woke', 'used', 'asshole', 'dipshits', 'patriot', 'know', 'truth']\n",
            "Final pre-processed text: another liberal lie disproved quite opposite leftist anti american mainstream medium would believe otherwise conscious aware ghetto patois woke used asshole dipshits patriot know truth\n",
            "Text after removing HTML tags: what an unhinged retard\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what an unhinged retard\n",
            "Text after tokenization: ['what', 'an', 'unhinged', 'retard']\n",
            "Text after removing stop words: ['unhinged', 'retard']\n",
            "Text after Lemmatization: ['unhinged', 'retard']\n",
            "Final pre-processed text: unhinged retard\n",
            "Text after removing HTML tags:  lol you follow whiteboys here i can not tell if it for gay interests or white supremacy\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  lol you follow whiteboys here i can not tell if it for gay interests or white supremacy\n",
            "Text after tokenization: ['lol', 'you', 'follow', 'whiteboys', 'here', 'i', 'can', 'not', 'tell', 'if', 'it', 'for', 'gay', 'interests', 'or', 'white', 'supremacy']\n",
            "Text after removing stop words: ['lol', 'follow', 'whiteboys', 'tell', 'gay', 'interests', 'white', 'supremacy']\n",
            "Text after Lemmatization: ['lol', 'follow', 'whiteboys', 'tell', 'gay', 'interest', 'white', 'supremacy']\n",
            "Final pre-processed text: lol follow whiteboys tell gay interest white supremacy\n",
            "Text after removing HTML tags: if dhimmi therea sharia may amber rudd y useless will not sort the moslem problem this bunch millions more brits will we will not be visiting mosques to sit down for samosas or posing with qurans footballadsalliance moslemsaretheproblem islamofascism muslim islam sandcoackroaches\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if dhimmi therea sharia may amber rudd y useless will not sort the moslem problem this bunch millions more brits will we will not be visiting mosques to sit down for samosas or posing with qurans footballadsalliance moslemsaretheproblem islamofascism muslim islam sandcoackroaches\n",
            "Text after tokenization: ['if', 'dhimmi', 'therea', 'sharia', 'may', 'amber', 'rudd', 'y', 'useless', 'will', 'not', 'sort', 'the', 'moslem', 'problem', 'this', 'bunch', 'millions', 'more', 'brits', 'will', 'we', 'will', 'not', 'be', 'visiting', 'mosques', 'to', 'sit', 'down', 'for', 'samosas', 'or', 'posing', 'with', 'qurans', 'footballadsalliance', 'moslemsaretheproblem', 'islamofascism', 'muslim', 'islam', 'sandcoackroaches']\n",
            "Text after removing stop words: ['dhimmi', 'therea', 'sharia', 'may', 'amber', 'rudd', 'useless', 'sort', 'moslem', 'problem', 'bunch', 'millions', 'brits', 'visiting', 'mosques', 'sit', 'samosas', 'posing', 'qurans', 'footballadsalliance', 'moslemsaretheproblem', 'islamofascism', 'muslim', 'islam', 'sandcoackroaches']\n",
            "Text after Lemmatization: ['dhimmi', 'therea', 'sharia', 'may', 'amber', 'rudd', 'useless', 'sort', 'moslem', 'problem', 'bunch', 'million', 'brit', 'visiting', 'mosque', 'sit', 'samosa', 'posing', 'quran', 'footballadsalliance', 'moslemsaretheproblem', 'islamofascism', 'muslim', 'islam', 'sandcoackroaches']\n",
            "Final pre-processed text: dhimmi therea sharia may amber rudd useless sort moslem problem bunch million brit visiting mosque sit samosa posing quran footballadsalliance moslemsaretheproblem islamofascism muslim islam sandcoackroaches\n",
            "Text after removing HTML tags: yet here you are being triggered like the utter faggot you are\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yet here you are being triggered like the utter faggot you are\n",
            "Text after tokenization: ['yet', 'here', 'you', 'are', 'being', 'triggered', 'like', 'the', 'utter', 'faggot', 'you', 'are']\n",
            "Text after removing stop words: ['yet', 'triggered', 'like', 'utter', 'faggot']\n",
            "Text after Lemmatization: ['yet', 'triggered', 'like', 'utter', 'faggot']\n",
            "Final pre-processed text: yet triggered like utter faggot\n",
            "Text after removing HTML tags: was a cnn white van nearby ready to take the shot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: was a cnn white van nearby ready to take the shot\n",
            "Text after tokenization: ['was', 'a', 'cnn', 'white', 'van', 'nearby', 'ready', 'to', 'take', 'the', 'shot']\n",
            "Text after removing stop words: ['cnn', 'white', 'van', 'nearby', 'ready', 'take', 'shot']\n",
            "Text after Lemmatization: ['cnn', 'white', 'van', 'nearby', 'ready', 'take', 'shot']\n",
            "Final pre-processed text: cnn white van nearby ready take shot\n",
            "Text after removing HTML tags: sad dick khan is the chief of the london muslim ghetto he first allegiance is to islam and muslims\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sad dick khan is the chief of the london muslim ghetto he first allegiance is to islam and muslims\n",
            "Text after tokenization: ['sad', 'dick', 'khan', 'is', 'the', 'chief', 'of', 'the', 'london', 'muslim', 'ghetto', 'he', 'first', 'allegiance', 'is', 'to', 'islam', 'and', 'muslims']\n",
            "Text after removing stop words: ['sad', 'dick', 'khan', 'chief', 'london', 'muslim', 'ghetto', 'first', 'allegiance', 'islam', 'muslims']\n",
            "Text after Lemmatization: ['sad', 'dick', 'khan', 'chief', 'london', 'muslim', 'ghetto', 'first', 'allegiance', 'islam', 'muslim']\n",
            "Final pre-processed text: sad dick khan chief london muslim ghetto first allegiance islam muslim\n",
            "Text after removing HTML tags: another shit thread bumped up by the gab faggot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: another shit thread bumped up by the gab faggot\n",
            "Text after tokenization: ['another', 'shit', 'thread', 'bumped', 'up', 'by', 'the', 'gab', 'faggot']\n",
            "Text after removing stop words: ['another', 'shit', 'thread', 'bumped', 'gab', 'faggot']\n",
            "Text after Lemmatization: ['another', 'shit', 'thread', 'bumped', 'gab', 'faggot']\n",
            "Final pre-processed text: another shit thread bumped gab faggot\n",
            "Text after removing HTML tags: lol i did not want to run out of space but do not forget deplorable redneck bible clinging gun toting homo hating science deniers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lol i did not want to run out of space but do not forget deplorable redneck bible clinging gun toting homo hating science deniers\n",
            "Text after tokenization: ['lol', 'i', 'did', 'not', 'want', 'to', 'run', 'out', 'of', 'space', 'but', 'do', 'not', 'forget', 'deplorable', 'redneck', 'bible', 'clinging', 'gun', 'toting', 'homo', 'hating', 'science', 'deniers']\n",
            "Text after removing stop words: ['lol', 'want', 'run', 'space', 'forget', 'deplorable', 'redneck', 'bible', 'clinging', 'gun', 'toting', 'homo', 'hating', 'science', 'deniers']\n",
            "Text after Lemmatization: ['lol', 'want', 'run', 'space', 'forget', 'deplorable', 'redneck', 'bible', 'clinging', 'gun', 'toting', 'homo', 'hating', 'science', 'denier']\n",
            "Final pre-processed text: lol want run space forget deplorable redneck bible clinging gun toting homo hating science denier\n",
            "Text after removing HTML tags: hollywood is out of ideas and is now running out of money so the jews are cannibalizing each other and weinstein is the first victim that my theory anyway\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hollywood is out of ideas and is now running out of money so the jews are cannibalizing each other and weinstein is the first victim that my theory anyway\n",
            "Text after tokenization: ['hollywood', 'is', 'out', 'of', 'ideas', 'and', 'is', 'now', 'running', 'out', 'of', 'money', 'so', 'the', 'jews', 'are', 'cannibalizing', 'each', 'other', 'and', 'weinstein', 'is', 'the', 'first', 'victim', 'that', 'my', 'theory', 'anyway']\n",
            "Text after removing stop words: ['hollywood', 'ideas', 'running', 'money', 'jews', 'cannibalizing', 'weinstein', 'first', 'victim', 'theory', 'anyway']\n",
            "Text after Lemmatization: ['hollywood', 'idea', 'running', 'money', 'jew', 'cannibalizing', 'weinstein', 'first', 'victim', 'theory', 'anyway']\n",
            "Final pre-processed text: hollywood idea running money jew cannibalizing weinstein first victim theory anyway\n",
            "Text after removing HTML tags: sensitive people ruined the reputation of direct confrontation now you are a thug or a ghetto bastard for doing so\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sensitive people ruined the reputation of direct confrontation now you are a thug or a ghetto bastard for doing so\n",
            "Text after tokenization: ['sensitive', 'people', 'ruined', 'the', 'reputation', 'of', 'direct', 'confrontation', 'now', 'you', 'are', 'a', 'thug', 'or', 'a', 'ghetto', 'bastard', 'for', 'doing', 'so']\n",
            "Text after removing stop words: ['sensitive', 'people', 'ruined', 'reputation', 'direct', 'confrontation', 'thug', 'ghetto', 'bastard']\n",
            "Text after Lemmatization: ['sensitive', 'people', 'ruined', 'reputation', 'direct', 'confrontation', 'thug', 'ghetto', 'bastard']\n",
            "Final pre-processed text: sensitive people ruined reputation direct confrontation thug ghetto bastard\n",
            "Text after removing HTML tags: the great muslim purge is coming assimilation is not what they have in mind they are starting to show their asses liberals cry about a ban when the ban starts the purge will start also some will enjoy the roundup\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the great muslim purge is coming assimilation is not what they have in mind they are starting to show their asses liberals cry about a ban when the ban starts the purge will start also some will enjoy the roundup\n",
            "Text after tokenization: ['the', 'great', 'muslim', 'purge', 'is', 'coming', 'assimilation', 'is', 'not', 'what', 'they', 'have', 'in', 'mind', 'they', 'are', 'starting', 'to', 'show', 'their', 'asses', 'liberals', 'cry', 'about', 'a', 'ban', 'when', 'the', 'ban', 'starts', 'the', 'purge', 'will', 'start', 'also', 'some', 'will', 'enjoy', 'the', 'roundup']\n",
            "Text after removing stop words: ['great', 'muslim', 'purge', 'coming', 'assimilation', 'mind', 'starting', 'show', 'asses', 'liberals', 'cry', 'ban', 'ban', 'starts', 'purge', 'start', 'also', 'enjoy', 'roundup']\n",
            "Text after Lemmatization: ['great', 'muslim', 'purge', 'coming', 'assimilation', 'mind', 'starting', 'show', 'ass', 'liberal', 'cry', 'ban', 'ban', 'start', 'purge', 'start', 'also', 'enjoy', 'roundup']\n",
            "Final pre-processed text: great muslim purge coming assimilation mind starting show ass liberal cry ban ban start purge start also enjoy roundup\n",
            "Text after removing HTML tags: who cares they are only soulless niggers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: who cares they are only soulless niggers\n",
            "Text after tokenization: ['who', 'cares', 'they', 'are', 'only', 'soulless', 'niggers']\n",
            "Text after removing stop words: ['cares', 'soulless', 'niggers']\n",
            "Text after Lemmatization: ['care', 'soulless', 'nigger']\n",
            "Final pre-processed text: care soulless nigger\n",
            "Text after removing HTML tags: does anyone recognize this google girl we are supposed to be celebrating she is a random brown person i cannot remember the last time anyone that could be recognized as a white man specifically has been hurrahed by google for no particular reason\n",
            "Text after removing non-alphabetic characters and converting to lowercase: does anyone recognize this google girl we are supposed to be celebrating she is a random brown person i cannot remember the last time anyone that could be recognized as a white man specifically has been hurrahed by google for no particular reason\n",
            "Text after tokenization: ['does', 'anyone', 'recognize', 'this', 'google', 'girl', 'we', 'are', 'supposed', 'to', 'be', 'celebrating', 'she', 'is', 'a', 'random', 'brown', 'person', 'i', 'can', 'not', 'remember', 'the', 'last', 'time', 'anyone', 'that', 'could', 'be', 'recognized', 'as', 'a', 'white', 'man', 'specifically', 'has', 'been', 'hurrahed', 'by', 'google', 'for', 'no', 'particular', 'reason']\n",
            "Text after removing stop words: ['anyone', 'recognize', 'google', 'girl', 'supposed', 'celebrating', 'random', 'brown', 'person', 'remember', 'last', 'time', 'anyone', 'could', 'recognized', 'white', 'man', 'specifically', 'hurrahed', 'google', 'particular', 'reason']\n",
            "Text after Lemmatization: ['anyone', 'recognize', 'google', 'girl', 'supposed', 'celebrating', 'random', 'brown', 'person', 'remember', 'last', 'time', 'anyone', 'could', 'recognized', 'white', 'man', 'specifically', 'hurrahed', 'google', 'particular', 'reason']\n",
            "Final pre-processed text: anyone recognize google girl supposed celebrating random brown person remember last time anyone could recognized white man specifically hurrahed google particular reason\n",
            "Text after removing HTML tags: 1 0 yrs  white women raped by niggers  negress raped by a cracker source fbi crime stats wake up rise up\n",
            "Text after removing non-alphabetic characters and converting to lowercase:     yrs  white women raped by niggers  negress raped by a cracker source fbi crime stats wake up rise up\n",
            "Text after tokenization: ['yrs', 'white', 'women', 'raped', 'by', 'niggers', 'negress', 'raped', 'by', 'a', 'cracker', 'source', 'fbi', 'crime', 'stats', 'wake', 'up', 'rise', 'up']\n",
            "Text after removing stop words: ['yrs', 'white', 'women', 'raped', 'niggers', 'negress', 'raped', 'cracker', 'source', 'fbi', 'crime', 'stats', 'wake', 'rise']\n",
            "Text after Lemmatization: ['yr', 'white', 'woman', 'raped', 'nigger', 'negress', 'raped', 'cracker', 'source', 'fbi', 'crime', 'stats', 'wake', 'rise']\n",
            "Final pre-processed text: yr white woman raped nigger negress raped cracker source fbi crime stats wake rise\n",
            "Text after removing HTML tags: cucked fuckyelp sjw faggots\n",
            "Text after removing non-alphabetic characters and converting to lowercase: cucked fuckyelp sjw faggots\n",
            "Text after tokenization: ['cucked', 'fuckyelp', 'sjw', 'faggots']\n",
            "Text after removing stop words: ['cucked', 'fuckyelp', 'sjw', 'faggots']\n",
            "Text after Lemmatization: ['cucked', 'fuckyelp', 'sjw', 'faggot']\n",
            "Final pre-processed text: cucked fuckyelp sjw faggot\n",
            "Text after removing HTML tags: young ugly turds you have to kind of feel sorry for them it like looking at a retarded two headed animal i mean it not it fault that it hideous but you know you have to put it down anyway or parade it around as a sideshow\n",
            "Text after removing non-alphabetic characters and converting to lowercase: young ugly turds you have to kind of feel sorry for them it like looking at a retarded two headed animal i mean it not it fault that it hideous but you know you have to put it down anyway or parade it around as a sideshow\n",
            "Text after tokenization: ['young', 'ugly', 'turds', 'you', 'have', 'to', 'kind', 'of', 'feel', 'sorry', 'for', 'them', 'it', 'like', 'looking', 'at', 'a', 'retarded', 'two', 'headed', 'animal', 'i', 'mean', 'it', 'not', 'it', 'fault', 'that', 'it', 'hideous', 'but', 'you', 'know', 'you', 'have', 'to', 'put', 'it', 'down', 'anyway', 'or', 'parade', 'it', 'around', 'as', 'a', 'sideshow']\n",
            "Text after removing stop words: ['young', 'ugly', 'turds', 'kind', 'feel', 'sorry', 'like', 'looking', 'retarded', 'two', 'headed', 'animal', 'mean', 'fault', 'hideous', 'know', 'put', 'anyway', 'parade', 'around', 'sideshow']\n",
            "Text after Lemmatization: ['young', 'ugly', 'turd', 'kind', 'feel', 'sorry', 'like', 'looking', 'retarded', 'two', 'headed', 'animal', 'mean', 'fault', 'hideous', 'know', 'put', 'anyway', 'parade', 'around', 'sideshow']\n",
            "Final pre-processed text: young ugly turd kind feel sorry like looking retarded two headed animal mean fault hideous know put anyway parade around sideshow\n",
            "Text after removing HTML tags: what happens when you elect ghetto girls\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what happens when you elect ghetto girls\n",
            "Text after tokenization: ['what', 'happens', 'when', 'you', 'elect', 'ghetto', 'girls']\n",
            "Text after removing stop words: ['happens', 'elect', 'ghetto', 'girls']\n",
            "Text after Lemmatization: ['happens', 'elect', 'ghetto', 'girl']\n",
            "Final pre-processed text: happens elect ghetto girl\n",
            "Text after removing HTML tags: what is it like to live as a white minority when there is no escapte to a white community it one thing to travel abroad or visit an non white enclave it another to have no place to go\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what is it like to live as a white minority when there is no escapte to a white community it one thing to travel abroad or visit an non white enclave it another to have no place to go\n",
            "Text after tokenization: ['what', 'is', 'it', 'like', 'to', 'live', 'as', 'a', 'white', 'minority', 'when', 'there', 'is', 'no', 'escapte', 'to', 'a', 'white', 'community', 'it', 'one', 'thing', 'to', 'travel', 'abroad', 'or', 'visit', 'an', 'non', 'white', 'enclave', 'it', 'another', 'to', 'have', 'no', 'place', 'to', 'go']\n",
            "Text after removing stop words: ['like', 'live', 'white', 'minority', 'escapte', 'white', 'community', 'one', 'thing', 'travel', 'abroad', 'visit', 'non', 'white', 'enclave', 'another', 'place', 'go']\n",
            "Text after Lemmatization: ['like', 'live', 'white', 'minority', 'escapte', 'white', 'community', 'one', 'thing', 'travel', 'abroad', 'visit', 'non', 'white', 'enclave', 'another', 'place', 'go']\n",
            "Final pre-processed text: like live white minority escapte white community one thing travel abroad visit non white enclave another place go\n",
            "Text after removing HTML tags: problem with jewish holocaust is that what really happened does not live up to expectations no gas chambers no shootings no experiments just starvation and typhus for less than  same shit as with every pow in history real events do not live up to hype like almost everything jews promote\n",
            "Text after removing non-alphabetic characters and converting to lowercase: problem with jewish holocaust is that what really happened does not live up to expectations no gas chambers no shootings no experiments just starvation and typhus for less than  same shit as with every pow in history real events do not live up to hype like almost everything jews promote\n",
            "Text after tokenization: ['problem', 'with', 'jewish', 'holocaust', 'is', 'that', 'what', 'really', 'happened', 'does', 'not', 'live', 'up', 'to', 'expectations', 'no', 'gas', 'chambers', 'no', 'shootings', 'no', 'experiments', 'just', 'starvation', 'and', 'typhus', 'for', 'less', 'than', 'same', 'shit', 'as', 'with', 'every', 'pow', 'in', 'history', 'real', 'events', 'do', 'not', 'live', 'up', 'to', 'hype', 'like', 'almost', 'everything', 'jews', 'promote']\n",
            "Text after removing stop words: ['problem', 'jewish', 'holocaust', 'really', 'happened', 'live', 'expectations', 'gas', 'chambers', 'shootings', 'experiments', 'starvation', 'typhus', 'less', 'shit', 'every', 'pow', 'history', 'real', 'events', 'live', 'hype', 'like', 'almost', 'everything', 'jews', 'promote']\n",
            "Text after Lemmatization: ['problem', 'jewish', 'holocaust', 'really', 'happened', 'live', 'expectation', 'gas', 'chamber', 'shooting', 'experiment', 'starvation', 'typhus', 'le', 'shit', 'every', 'pow', 'history', 'real', 'event', 'live', 'hype', 'like', 'almost', 'everything', 'jew', 'promote']\n",
            "Final pre-processed text: problem jewish holocaust really happened live expectation gas chamber shooting experiment starvation typhus le shit every pow history real event live hype like almost everything jew promote\n",
            "Text after removing HTML tags: gabfam   ok so over at twitz they have a ceo who thinks words are violence and will ban you if you use anything that they see as violent to communicate with your friends how the hell does that work\n",
            "Text after removing non-alphabetic characters and converting to lowercase: gabfam   ok so over at twitz they have a ceo who thinks words are violence and will ban you if you use anything that they see as violent to communicate with your friends how the hell does that work\n",
            "Text after tokenization: ['gabfam', 'ok', 'so', 'over', 'at', 'twitz', 'they', 'have', 'a', 'ceo', 'who', 'thinks', 'words', 'are', 'violence', 'and', 'will', 'ban', 'you', 'if', 'you', 'use', 'anything', 'that', 'they', 'see', 'as', 'violent', 'to', 'communicate', 'with', 'your', 'friends', 'how', 'the', 'hell', 'does', 'that', 'work']\n",
            "Text after removing stop words: ['gabfam', 'ok', 'twitz', 'ceo', 'thinks', 'words', 'violence', 'ban', 'use', 'anything', 'see', 'violent', 'communicate', 'friends', 'hell', 'work']\n",
            "Text after Lemmatization: ['gabfam', 'ok', 'twitz', 'ceo', 'think', 'word', 'violence', 'ban', 'use', 'anything', 'see', 'violent', 'communicate', 'friend', 'hell', 'work']\n",
            "Final pre-processed text: gabfam ok twitz ceo think word violence ban use anything see violent communicate friend hell work\n",
            "Text after removing HTML tags: are not you a hostile little snowflake listen dickwad quit misrepresenting my argument the hoodrats in the ghetto do not have the same culture as middle america you are retarded if you think they do ditto haiti they are slave colony that revolted and burned their infrastructure down doing it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: are not you a hostile little snowflake listen dickwad quit misrepresenting my argument the hoodrats in the ghetto do not have the same culture as middle america you are retarded if you think they do ditto haiti they are slave colony that revolted and burned their infrastructure down doing it\n",
            "Text after tokenization: ['are', 'not', 'you', 'a', 'hostile', 'little', 'snowflake', 'listen', 'dickwad', 'quit', 'misrepresenting', 'my', 'argument', 'the', 'hoodrats', 'in', 'the', 'ghetto', 'do', 'not', 'have', 'the', 'same', 'culture', 'as', 'middle', 'america', 'you', 'are', 'retarded', 'if', 'you', 'think', 'they', 'do', 'ditto', 'haiti', 'they', 'are', 'slave', 'colony', 'that', 'revolted', 'and', 'burned', 'their', 'infrastructure', 'down', 'doing', 'it']\n",
            "Text after removing stop words: ['hostile', 'little', 'snowflake', 'listen', 'dickwad', 'quit', 'misrepresenting', 'argument', 'hoodrats', 'ghetto', 'culture', 'middle', 'america', 'retarded', 'think', 'ditto', 'haiti', 'slave', 'colony', 'revolted', 'burned', 'infrastructure']\n",
            "Text after Lemmatization: ['hostile', 'little', 'snowflake', 'listen', 'dickwad', 'quit', 'misrepresenting', 'argument', 'hoodrats', 'ghetto', 'culture', 'middle', 'america', 'retarded', 'think', 'ditto', 'haiti', 'slave', 'colony', 'revolted', 'burned', 'infrastructure']\n",
            "Final pre-processed text: hostile little snowflake listen dickwad quit misrepresenting argument hoodrats ghetto culture middle america retarded think ditto haiti slave colony revolted burned infrastructure\n",
            "Text after removing HTML tags: so explain why it the only aspect of history that is protected by laws in europe people go to jail for questioning any aspect of the holocaust\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so explain why it the only aspect of history that is protected by laws in europe people go to jail for questioning any aspect of the holocaust\n",
            "Text after tokenization: ['so', 'explain', 'why', 'it', 'the', 'only', 'aspect', 'of', 'history', 'that', 'is', 'protected', 'by', 'laws', 'in', 'europe', 'people', 'go', 'to', 'jail', 'for', 'questioning', 'any', 'aspect', 'of', 'the', 'holocaust']\n",
            "Text after removing stop words: ['explain', 'aspect', 'history', 'protected', 'laws', 'europe', 'people', 'go', 'jail', 'questioning', 'aspect', 'holocaust']\n",
            "Text after Lemmatization: ['explain', 'aspect', 'history', 'protected', 'law', 'europe', 'people', 'go', 'jail', 'questioning', 'aspect', 'holocaust']\n",
            "Final pre-processed text: explain aspect history protected law europe people go jail questioning aspect holocaust\n",
            "Text after removing HTML tags: ignoring the historical role women played is to disgrace them men were not and are not in a vacuum they confided in received council from and embraced the women in their lives in that regard not much has changed\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ignoring the historical role women played is to disgrace them men were not and are not in a vacuum they confided in received council from and embraced the women in their lives in that regard not much has changed\n",
            "Text after tokenization: ['ignoring', 'the', 'historical', 'role', 'women', 'played', 'is', 'to', 'disgrace', 'them', 'men', 'were', 'not', 'and', 'are', 'not', 'in', 'a', 'vacuum', 'they', 'confided', 'in', 'received', 'council', 'from', 'and', 'embraced', 'the', 'women', 'in', 'their', 'lives', 'in', 'that', 'regard', 'not', 'much', 'has', 'changed']\n",
            "Text after removing stop words: ['ignoring', 'historical', 'role', 'women', 'played', 'disgrace', 'men', 'vacuum', 'confided', 'received', 'council', 'embraced', 'women', 'lives', 'regard', 'much', 'changed']\n",
            "Text after Lemmatization: ['ignoring', 'historical', 'role', 'woman', 'played', 'disgrace', 'men', 'vacuum', 'confided', 'received', 'council', 'embraced', 'woman', 'life', 'regard', 'much', 'changed']\n",
            "Final pre-processed text: ignoring historical role woman played disgrace men vacuum confided received council embraced woman life regard much changed\n",
            "Text after removing HTML tags: now a days how many will bitch and moan if they had that power back because it would require great responsibility and hard work\n",
            "Text after removing non-alphabetic characters and converting to lowercase: now a days how many will bitch and moan if they had that power back because it would require great responsibility and hard work\n",
            "Text after tokenization: ['now', 'a', 'days', 'how', 'many', 'will', 'bitch', 'and', 'moan', 'if', 'they', 'had', 'that', 'power', 'back', 'because', 'it', 'would', 'require', 'great', 'responsibility', 'and', 'hard', 'work']\n",
            "Text after removing stop words: ['days', 'many', 'bitch', 'moan', 'power', 'back', 'would', 'require', 'great', 'responsibility', 'hard', 'work']\n",
            "Text after Lemmatization: ['day', 'many', 'bitch', 'moan', 'power', 'back', 'would', 'require', 'great', 'responsibility', 'hard', 'work']\n",
            "Final pre-processed text: day many bitch moan power back would require great responsibility hard work\n",
            "Text after removing HTML tags: benny boy is just as anti white as the rest\n",
            "Text after removing non-alphabetic characters and converting to lowercase: benny boy is just as anti white as the rest\n",
            "Text after tokenization: ['benny', 'boy', 'is', 'just', 'as', 'anti', 'white', 'as', 'the', 'rest']\n",
            "Text after removing stop words: ['benny', 'boy', 'anti', 'white', 'rest']\n",
            "Text after Lemmatization: ['benny', 'boy', 'anti', 'white', 'rest']\n",
            "Final pre-processed text: benny boy anti white rest\n",
            "Text after removing HTML tags: paul joseph watson ‏  2 m2 minutes ago paul joseph watson retweeted bernie sanders bernie is right the diversity of women hating gay killing minority oppressing middle eastern dictatorships that fund hillary is varied\n",
            "Text after removing non-alphabetic characters and converting to lowercase: paul joseph watson      m  minutes ago paul joseph watson retweeted bernie sanders bernie is right the diversity of women hating gay killing minority oppressing middle eastern dictatorships that fund hillary is varied\n",
            "Text after tokenization: ['paul', 'joseph', 'watson', 'm', 'minutes', 'ago', 'paul', 'joseph', 'watson', 'retweeted', 'bernie', 'sanders', 'bernie', 'is', 'right', 'the', 'diversity', 'of', 'women', 'hating', 'gay', 'killing', 'minority', 'oppressing', 'middle', 'eastern', 'dictatorships', 'that', 'fund', 'hillary', 'is', 'varied']\n",
            "Text after removing stop words: ['paul', 'joseph', 'watson', 'minutes', 'ago', 'paul', 'joseph', 'watson', 'retweeted', 'bernie', 'sanders', 'bernie', 'right', 'diversity', 'women', 'hating', 'gay', 'killing', 'minority', 'oppressing', 'middle', 'eastern', 'dictatorships', 'fund', 'hillary', 'varied']\n",
            "Text after Lemmatization: ['paul', 'joseph', 'watson', 'minute', 'ago', 'paul', 'joseph', 'watson', 'retweeted', 'bernie', 'sander', 'bernie', 'right', 'diversity', 'woman', 'hating', 'gay', 'killing', 'minority', 'oppressing', 'middle', 'eastern', 'dictatorship', 'fund', 'hillary', 'varied']\n",
            "Final pre-processed text: paul joseph watson minute ago paul joseph watson retweeted bernie sander bernie right diversity woman hating gay killing minority oppressing middle eastern dictatorship fund hillary varied\n",
            "Text after removing HTML tags: i am jewish in  generations not one kike has been introduced into my family i am half pure aryan\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am jewish in  generations not one kike has been introduced into my family i am half pure aryan\n",
            "Text after tokenization: ['i', 'am', 'jewish', 'in', 'generations', 'not', 'one', 'kike', 'has', 'been', 'introduced', 'into', 'my', 'family', 'i', 'am', 'half', 'pure', 'aryan']\n",
            "Text after removing stop words: ['jewish', 'generations', 'one', 'kike', 'introduced', 'family', 'half', 'pure', 'aryan']\n",
            "Text after Lemmatization: ['jewish', 'generation', 'one', 'kike', 'introduced', 'family', 'half', 'pure', 'aryan']\n",
            "Final pre-processed text: jewish generation one kike introduced family half pure aryan\n",
            "Text after removing HTML tags: math algebra reason the work ethic the nuclear family all halloween costumes and all white people in general are racist so yes count me in metoo\n",
            "Text after removing non-alphabetic characters and converting to lowercase: math algebra reason the work ethic the nuclear family all halloween costumes and all white people in general are racist so yes count me in metoo\n",
            "Text after tokenization: ['math', 'algebra', 'reason', 'the', 'work', 'ethic', 'the', 'nuclear', 'family', 'all', 'halloween', 'costumes', 'and', 'all', 'white', 'people', 'in', 'general', 'are', 'racist', 'so', 'yes', 'count', 'me', 'in', 'metoo']\n",
            "Text after removing stop words: ['math', 'algebra', 'reason', 'work', 'ethic', 'nuclear', 'family', 'halloween', 'costumes', 'white', 'people', 'general', 'racist', 'yes', 'count', 'metoo']\n",
            "Text after Lemmatization: ['math', 'algebra', 'reason', 'work', 'ethic', 'nuclear', 'family', 'halloween', 'costume', 'white', 'people', 'general', 'racist', 'yes', 'count', 'metoo']\n",
            "Final pre-processed text: math algebra reason work ethic nuclear family halloween costume white people general racist yes count metoo\n",
            "Text after removing HTML tags: what raped  in the ass\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what raped  in the ass\n",
            "Text after tokenization: ['what', 'raped', 'in', 'the', 'ass']\n",
            "Text after removing stop words: ['raped', 'ass']\n",
            "Text after Lemmatization: ['raped', 'as']\n",
            "Final pre-processed text: raped as\n",
            "Text after removing HTML tags: the picture on the left is the audience at the bataclan theatre paris just before muslims murdered raped mutilated them hope it happens to this stupid bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the picture on the left is the audience at the bataclan theatre paris just before muslims murdered raped mutilated them hope it happens to this stupid bitch\n",
            "Text after tokenization: ['the', 'picture', 'on', 'the', 'left', 'is', 'the', 'audience', 'at', 'the', 'bataclan', 'theatre', 'paris', 'just', 'before', 'muslims', 'murdered', 'raped', 'mutilated', 'them', 'hope', 'it', 'happens', 'to', 'this', 'stupid', 'bitch']\n",
            "Text after removing stop words: ['picture', 'left', 'audience', 'bataclan', 'theatre', 'paris', 'muslims', 'murdered', 'raped', 'mutilated', 'hope', 'happens', 'stupid', 'bitch']\n",
            "Text after Lemmatization: ['picture', 'left', 'audience', 'bataclan', 'theatre', 'paris', 'muslim', 'murdered', 'raped', 'mutilated', 'hope', 'happens', 'stupid', 'bitch']\n",
            "Final pre-processed text: picture left audience bataclan theatre paris muslim murdered raped mutilated hope happens stupid bitch\n",
            "Text after removing HTML tags: he has to be retarded to keep coming around every day for this kind of punishment 😂\n",
            "Text after removing non-alphabetic characters and converting to lowercase: he has to be retarded to keep coming around every day for this kind of punishment  \n",
            "Text after tokenization: ['he', 'has', 'to', 'be', 'retarded', 'to', 'keep', 'coming', 'around', 'every', 'day', 'for', 'this', 'kind', 'of', 'punishment']\n",
            "Text after removing stop words: ['retarded', 'keep', 'coming', 'around', 'every', 'day', 'kind', 'punishment']\n",
            "Text after Lemmatization: ['retarded', 'keep', 'coming', 'around', 'every', 'day', 'kind', 'punishment']\n",
            "Final pre-processed text: retarded keep coming around every day kind punishment\n",
            "Text after removing HTML tags: people in my ghetto ass school used to wear shirts like these like xl sized\n",
            "Text after removing non-alphabetic characters and converting to lowercase: people in my ghetto ass school used to wear shirts like these like xl sized\n",
            "Text after tokenization: ['people', 'in', 'my', 'ghetto', 'ass', 'school', 'used', 'to', 'wear', 'shirts', 'like', 'these', 'like', 'xl', 'sized']\n",
            "Text after removing stop words: ['people', 'ghetto', 'ass', 'school', 'used', 'wear', 'shirts', 'like', 'like', 'xl', 'sized']\n",
            "Text after Lemmatization: ['people', 'ghetto', 'as', 'school', 'used', 'wear', 'shirt', 'like', 'like', 'xl', 'sized']\n",
            "Final pre-processed text: people ghetto as school used wear shirt like like xl sized\n",
            "Text after removing HTML tags: slight correction jewish supremacism is winning wtf you think let these sand niggers into our backyard\n",
            "Text after removing non-alphabetic characters and converting to lowercase: slight correction jewish supremacism is winning wtf you think let these sand niggers into our backyard\n",
            "Text after tokenization: ['slight', 'correction', 'jewish', 'supremacism', 'is', 'winning', 'wtf', 'you', 'think', 'let', 'these', 'sand', 'niggers', 'into', 'our', 'backyard']\n",
            "Text after removing stop words: ['slight', 'correction', 'jewish', 'supremacism', 'winning', 'wtf', 'think', 'let', 'sand', 'niggers', 'backyard']\n",
            "Text after Lemmatization: ['slight', 'correction', 'jewish', 'supremacism', 'winning', 'wtf', 'think', 'let', 'sand', 'nigger', 'backyard']\n",
            "Final pre-processed text: slight correction jewish supremacism winning wtf think let sand nigger backyard\n",
            "Text after removing HTML tags: why would the fbi have ears on the ground in a the muslim shithole that paterson nj home of two   hijackers has become you people must be nuts they have got to investigate the russians they bought ads on facebook dontcha know\n",
            "Text after removing non-alphabetic characters and converting to lowercase: why would the fbi have ears on the ground in a the muslim shithole that paterson nj home of two   hijackers has become you people must be nuts they have got to investigate the russians they bought ads on facebook dontcha know\n",
            "Text after tokenization: ['why', 'would', 'the', 'fbi', 'have', 'ears', 'on', 'the', 'ground', 'in', 'a', 'the', 'muslim', 'shithole', 'that', 'paterson', 'nj', 'home', 'of', 'two', 'hijackers', 'has', 'become', 'you', 'people', 'must', 'be', 'nuts', 'they', 'have', 'got', 'to', 'investigate', 'the', 'russians', 'they', 'bought', 'ads', 'on', 'facebook', 'dontcha', 'know']\n",
            "Text after removing stop words: ['would', 'fbi', 'ears', 'ground', 'muslim', 'shithole', 'paterson', 'nj', 'home', 'two', 'hijackers', 'become', 'people', 'must', 'nuts', 'got', 'investigate', 'russians', 'bought', 'ads', 'facebook', 'dontcha', 'know']\n",
            "Text after Lemmatization: ['would', 'fbi', 'ear', 'ground', 'muslim', 'shithole', 'paterson', 'nj', 'home', 'two', 'hijacker', 'become', 'people', 'must', 'nut', 'got', 'investigate', 'russian', 'bought', 'ad', 'facebook', 'dontcha', 'know']\n",
            "Final pre-processed text: would fbi ear ground muslim shithole paterson nj home two hijacker become people must nut got investigate russian bought ad facebook dontcha know\n",
            "Text after removing HTML tags: im a couple dicerolls from being a bad egg and am very aware this is why i tutor ghetto kids and single parent kids who are black for free having a family myself that is all i can do at this time\n",
            "Text after removing non-alphabetic characters and converting to lowercase: im a couple dicerolls from being a bad egg and am very aware this is why i tutor ghetto kids and single parent kids who are black for free having a family myself that is all i can do at this time\n",
            "Text after tokenization: ['im', 'a', 'couple', 'dicerolls', 'from', 'being', 'a', 'bad', 'egg', 'and', 'am', 'very', 'aware', 'this', 'is', 'why', 'i', 'tutor', 'ghetto', 'kids', 'and', 'single', 'parent', 'kids', 'who', 'are', 'black', 'for', 'free', 'having', 'a', 'family', 'myself', 'that', 'is', 'all', 'i', 'can', 'do', 'at', 'this', 'time']\n",
            "Text after removing stop words: ['im', 'couple', 'dicerolls', 'bad', 'egg', 'aware', 'tutor', 'ghetto', 'kids', 'single', 'parent', 'kids', 'black', 'free', 'family', 'time']\n",
            "Text after Lemmatization: ['im', 'couple', 'dicerolls', 'bad', 'egg', 'aware', 'tutor', 'ghetto', 'kid', 'single', 'parent', 'kid', 'black', 'free', 'family', 'time']\n",
            "Final pre-processed text: im couple dicerolls bad egg aware tutor ghetto kid single parent kid black free family time\n",
            "Text after removing HTML tags: let the snow flakes cry bitch and scream it judgement day and time normalize islamaphobia as a healthy condition piss on political correctness and then deport every single last sand nigger savage\n",
            "Text after removing non-alphabetic characters and converting to lowercase: let the snow flakes cry bitch and scream it judgement day and time normalize islamaphobia as a healthy condition piss on political correctness and then deport every single last sand nigger savage\n",
            "Text after tokenization: ['let', 'the', 'snow', 'flakes', 'cry', 'bitch', 'and', 'scream', 'it', 'judgement', 'day', 'and', 'time', 'normalize', 'islamaphobia', 'as', 'a', 'healthy', 'condition', 'piss', 'on', 'political', 'correctness', 'and', 'then', 'deport', 'every', 'single', 'last', 'sand', 'nigger', 'savage']\n",
            "Text after removing stop words: ['let', 'snow', 'flakes', 'cry', 'bitch', 'scream', 'judgement', 'day', 'time', 'normalize', 'islamaphobia', 'healthy', 'condition', 'piss', 'political', 'correctness', 'deport', 'every', 'single', 'last', 'sand', 'nigger', 'savage']\n",
            "Text after Lemmatization: ['let', 'snow', 'flake', 'cry', 'bitch', 'scream', 'judgement', 'day', 'time', 'normalize', 'islamaphobia', 'healthy', 'condition', 'piss', 'political', 'correctness', 'deport', 'every', 'single', 'last', 'sand', 'nigger', 'savage']\n",
            "Final pre-processed text: let snow flake cry bitch scream judgement day time normalize islamaphobia healthy condition piss political correctness deport every single last sand nigger savage\n",
            "Text after removing HTML tags: that stupid mls has been booming for the last decade that just dumb to claim a thing that has massively grown in the past   years does not exist seriously that retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase: that stupid mls has been booming for the last decade that just dumb to claim a thing that has massively grown in the past   years does not exist seriously that retarded\n",
            "Text after tokenization: ['that', 'stupid', 'mls', 'has', 'been', 'booming', 'for', 'the', 'last', 'decade', 'that', 'just', 'dumb', 'to', 'claim', 'a', 'thing', 'that', 'has', 'massively', 'grown', 'in', 'the', 'past', 'years', 'does', 'not', 'exist', 'seriously', 'that', 'retarded']\n",
            "Text after removing stop words: ['stupid', 'mls', 'booming', 'last', 'decade', 'dumb', 'claim', 'thing', 'massively', 'grown', 'past', 'years', 'exist', 'seriously', 'retarded']\n",
            "Text after Lemmatization: ['stupid', 'ml', 'booming', 'last', 'decade', 'dumb', 'claim', 'thing', 'massively', 'grown', 'past', 'year', 'exist', 'seriously', 'retarded']\n",
            "Final pre-processed text: stupid ml booming last decade dumb claim thing massively grown past year exist seriously retarded\n",
            "Text after removing HTML tags: thank you that what i think too we are white spanairds are white hence hispanics are actually white and asians are yellow and africans are black there only  races\n",
            "Text after removing non-alphabetic characters and converting to lowercase: thank you that what i think too we are white spanairds are white hence hispanics are actually white and asians are yellow and africans are black there only  races\n",
            "Text after tokenization: ['thank', 'you', 'that', 'what', 'i', 'think', 'too', 'we', 'are', 'white', 'spanairds', 'are', 'white', 'hence', 'hispanics', 'are', 'actually', 'white', 'and', 'asians', 'are', 'yellow', 'and', 'africans', 'are', 'black', 'there', 'only', 'races']\n",
            "Text after removing stop words: ['thank', 'think', 'white', 'spanairds', 'white', 'hence', 'hispanics', 'actually', 'white', 'asians', 'yellow', 'africans', 'black', 'races']\n",
            "Text after Lemmatization: ['thank', 'think', 'white', 'spanairds', 'white', 'hence', 'hispanic', 'actually', 'white', 'asian', 'yellow', 'african', 'black', 'race']\n",
            "Final pre-processed text: thank think white spanairds white hence hispanic actually white asian yellow african black race\n",
            "Text after removing HTML tags: yep asians here also very hi achievers tiger mom and indian students love them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yep asians here also very hi achievers tiger mom and indian students love them\n",
            "Text after tokenization: ['yep', 'asians', 'here', 'also', 'very', 'hi', 'achievers', 'tiger', 'mom', 'and', 'indian', 'students', 'love', 'them']\n",
            "Text after removing stop words: ['yep', 'asians', 'also', 'hi', 'achievers', 'tiger', 'mom', 'indian', 'students', 'love']\n",
            "Text after Lemmatization: ['yep', 'asian', 'also', 'hi', 'achiever', 'tiger', 'mom', 'indian', 'student', 'love']\n",
            "Final pre-processed text: yep asian also hi achiever tiger mom indian student love\n",
            "Text after removing HTML tags: arm yourself against soros funded altifa tyranny and muslime ohell sedition there is no place in america for millions of ohell muslime refugee barbarians and their cult sharia law the dems love women are not slaves in america they are or will be armed if desired for their protection\n",
            "Text after removing non-alphabetic characters and converting to lowercase: arm yourself against soros funded altifa tyranny and muslime ohell sedition there is no place in america for millions of ohell muslime refugee barbarians and their cult sharia law the dems love women are not slaves in america they are or will be armed if desired for their protection\n",
            "Text after tokenization: ['arm', 'yourself', 'against', 'soros', 'funded', 'altifa', 'tyranny', 'and', 'muslime', 'ohell', 'sedition', 'there', 'is', 'no', 'place', 'in', 'america', 'for', 'millions', 'of', 'ohell', 'muslime', 'refugee', 'barbarians', 'and', 'their', 'cult', 'sharia', 'law', 'the', 'dems', 'love', 'women', 'are', 'not', 'slaves', 'in', 'america', 'they', 'are', 'or', 'will', 'be', 'armed', 'if', 'desired', 'for', 'their', 'protection']\n",
            "Text after removing stop words: ['arm', 'soros', 'funded', 'altifa', 'tyranny', 'muslime', 'ohell', 'sedition', 'place', 'america', 'millions', 'ohell', 'muslime', 'refugee', 'barbarians', 'cult', 'sharia', 'law', 'dems', 'love', 'women', 'slaves', 'america', 'armed', 'desired', 'protection']\n",
            "Text after Lemmatization: ['arm', 'soros', 'funded', 'altifa', 'tyranny', 'muslime', 'ohell', 'sedition', 'place', 'america', 'million', 'ohell', 'muslime', 'refugee', 'barbarian', 'cult', 'sharia', 'law', 'dems', 'love', 'woman', 'slave', 'america', 'armed', 'desired', 'protection']\n",
            "Final pre-processed text: arm soros funded altifa tyranny muslime ohell sedition place america million ohell muslime refugee barbarian cult sharia law dems love woman slave america armed desired protection\n",
            "Text after removing HTML tags: or someone suffering from a serious illness he needs support and intervention but that can not be done via cyberspace poor soul he turning off all the women of gabfam too what a mess\n",
            "Text after removing non-alphabetic characters and converting to lowercase: or someone suffering from a serious illness he needs support and intervention but that can not be done via cyberspace poor soul he turning off all the women of gabfam too what a mess\n",
            "Text after tokenization: ['or', 'someone', 'suffering', 'from', 'a', 'serious', 'illness', 'he', 'needs', 'support', 'and', 'intervention', 'but', 'that', 'can', 'not', 'be', 'done', 'via', 'cyberspace', 'poor', 'soul', 'he', 'turning', 'off', 'all', 'the', 'women', 'of', 'gabfam', 'too', 'what', 'a', 'mess']\n",
            "Text after removing stop words: ['someone', 'suffering', 'serious', 'illness', 'needs', 'support', 'intervention', 'done', 'via', 'cyberspace', 'poor', 'soul', 'turning', 'women', 'gabfam', 'mess']\n",
            "Text after Lemmatization: ['someone', 'suffering', 'serious', 'illness', 'need', 'support', 'intervention', 'done', 'via', 'cyberspace', 'poor', 'soul', 'turning', 'woman', 'gabfam', 'mess']\n",
            "Final pre-processed text: someone suffering serious illness need support intervention done via cyberspace poor soul turning woman gabfam mess\n",
            "Text after removing HTML tags: roseanne barr courteney cox and david arquette all say they knew about louis ck sexual harassment and gawker actually published a story about his actions in  but it was ignored at the time as per reporting on fox once again hollywood knew\n",
            "Text after removing non-alphabetic characters and converting to lowercase: roseanne barr courteney cox and david arquette all say they knew about louis ck sexual harassment and gawker actually published a story about his actions in  but it was ignored at the time as per reporting on fox once again hollywood knew\n",
            "Text after tokenization: ['roseanne', 'barr', 'courteney', 'cox', 'and', 'david', 'arquette', 'all', 'say', 'they', 'knew', 'about', 'louis', 'ck', 'sexual', 'harassment', 'and', 'gawker', 'actually', 'published', 'a', 'story', 'about', 'his', 'actions', 'in', 'but', 'it', 'was', 'ignored', 'at', 'the', 'time', 'as', 'per', 'reporting', 'on', 'fox', 'once', 'again', 'hollywood', 'knew']\n",
            "Text after removing stop words: ['roseanne', 'barr', 'courteney', 'cox', 'david', 'arquette', 'say', 'knew', 'louis', 'ck', 'sexual', 'harassment', 'gawker', 'actually', 'published', 'story', 'actions', 'ignored', 'time', 'per', 'reporting', 'fox', 'hollywood', 'knew']\n",
            "Text after Lemmatization: ['roseanne', 'barr', 'courteney', 'cox', 'david', 'arquette', 'say', 'knew', 'louis', 'ck', 'sexual', 'harassment', 'gawker', 'actually', 'published', 'story', 'action', 'ignored', 'time', 'per', 'reporting', 'fox', 'hollywood', 'knew']\n",
            "Final pre-processed text: roseanne barr courteney cox david arquette say knew louis ck sexual harassment gawker actually published story action ignored time per reporting fox hollywood knew\n",
            "Text after removing HTML tags: in oakland one black dude will sit in the front of the bus and another will sit in the back they will then talk to one another over everyone heads laughing cussing an saying nigger every third word and the other passengers including the bus driver know not to interrupt\n",
            "Text after removing non-alphabetic characters and converting to lowercase: in oakland one black dude will sit in the front of the bus and another will sit in the back they will then talk to one another over everyone heads laughing cussing an saying nigger every third word and the other passengers including the bus driver know not to interrupt\n",
            "Text after tokenization: ['in', 'oakland', 'one', 'black', 'dude', 'will', 'sit', 'in', 'the', 'front', 'of', 'the', 'bus', 'and', 'another', 'will', 'sit', 'in', 'the', 'back', 'they', 'will', 'then', 'talk', 'to', 'one', 'another', 'over', 'everyone', 'heads', 'laughing', 'cussing', 'an', 'saying', 'nigger', 'every', 'third', 'word', 'and', 'the', 'other', 'passengers', 'including', 'the', 'bus', 'driver', 'know', 'not', 'to', 'interrupt']\n",
            "Text after removing stop words: ['oakland', 'one', 'black', 'dude', 'sit', 'front', 'bus', 'another', 'sit', 'back', 'talk', 'one', 'another', 'everyone', 'heads', 'laughing', 'cussing', 'saying', 'nigger', 'every', 'third', 'word', 'passengers', 'including', 'bus', 'driver', 'know', 'interrupt']\n",
            "Text after Lemmatization: ['oakland', 'one', 'black', 'dude', 'sit', 'front', 'bus', 'another', 'sit', 'back', 'talk', 'one', 'another', 'everyone', 'head', 'laughing', 'cussing', 'saying', 'nigger', 'every', 'third', 'word', 'passenger', 'including', 'bus', 'driver', 'know', 'interrupt']\n",
            "Final pre-processed text: oakland one black dude sit front bus another sit back talk one another everyone head laughing cussing saying nigger every third word passenger including bus driver know interrupt\n",
            "Text after removing HTML tags: but boom you are welcome feminists are pro women in the draft you might want to check with the anti feminists who want to keep women in the home domain pretty sure they are more any women in military than feminists are think about it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: but boom you are welcome feminists are pro women in the draft you might want to check with the anti feminists who want to keep women in the home domain pretty sure they are more any women in military than feminists are think about it\n",
            "Text after tokenization: ['but', 'boom', 'you', 'are', 'welcome', 'feminists', 'are', 'pro', 'women', 'in', 'the', 'draft', 'you', 'might', 'want', 'to', 'check', 'with', 'the', 'anti', 'feminists', 'who', 'want', 'to', 'keep', 'women', 'in', 'the', 'home', 'domain', 'pretty', 'sure', 'they', 'are', 'more', 'any', 'women', 'in', 'military', 'than', 'feminists', 'are', 'think', 'about', 'it']\n",
            "Text after removing stop words: ['boom', 'welcome', 'feminists', 'pro', 'women', 'draft', 'might', 'want', 'check', 'anti', 'feminists', 'want', 'keep', 'women', 'home', 'domain', 'pretty', 'sure', 'women', 'military', 'feminists', 'think']\n",
            "Text after Lemmatization: ['boom', 'welcome', 'feminist', 'pro', 'woman', 'draft', 'might', 'want', 'check', 'anti', 'feminist', 'want', 'keep', 'woman', 'home', 'domain', 'pretty', 'sure', 'woman', 'military', 'feminist', 'think']\n",
            "Final pre-processed text: boom welcome feminist pro woman draft might want check anti feminist want keep woman home domain pretty sure woman military feminist think\n",
            "Text after removing HTML tags: merry christmas is like whitesupremacy and ready to gas   jews jews shit on our traditions diamonds\n",
            "Text after removing non-alphabetic characters and converting to lowercase: merry christmas is like whitesupremacy and ready to gas   jews jews shit on our traditions diamonds\n",
            "Text after tokenization: ['merry', 'christmas', 'is', 'like', 'whitesupremacy', 'and', 'ready', 'to', 'gas', 'jews', 'jews', 'shit', 'on', 'our', 'traditions', 'diamonds']\n",
            "Text after removing stop words: ['merry', 'christmas', 'like', 'whitesupremacy', 'ready', 'gas', 'jews', 'jews', 'shit', 'traditions', 'diamonds']\n",
            "Text after Lemmatization: ['merry', 'christmas', 'like', 'whitesupremacy', 'ready', 'gas', 'jew', 'jew', 'shit', 'tradition', 'diamond']\n",
            "Final pre-processed text: merry christmas like whitesupremacy ready gas jew jew shit tradition diamond\n",
            "Text after removing HTML tags: precisely and these nuances appear in all groups no sufi muslim will ever become a suicide bomber the most radical become whirling dervishes\n",
            "Text after removing non-alphabetic characters and converting to lowercase: precisely and these nuances appear in all groups no sufi muslim will ever become a suicide bomber the most radical become whirling dervishes\n",
            "Text after tokenization: ['precisely', 'and', 'these', 'nuances', 'appear', 'in', 'all', 'groups', 'no', 'sufi', 'muslim', 'will', 'ever', 'become', 'a', 'suicide', 'bomber', 'the', 'most', 'radical', 'become', 'whirling', 'dervishes']\n",
            "Text after removing stop words: ['precisely', 'nuances', 'appear', 'groups', 'sufi', 'muslim', 'ever', 'become', 'suicide', 'bomber', 'radical', 'become', 'whirling', 'dervishes']\n",
            "Text after Lemmatization: ['precisely', 'nuance', 'appear', 'group', 'sufi', 'muslim', 'ever', 'become', 'suicide', 'bomber', 'radical', 'become', 'whirling', 'dervish']\n",
            "Final pre-processed text: precisely nuance appear group sufi muslim ever become suicide bomber radical become whirling dervish\n",
            "Text after removing HTML tags: it looks a bit muggy out there i said what makes you say that replied my wife i said that gang of blacks with knives on the corner\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it looks a bit muggy out there i said what makes you say that replied my wife i said that gang of blacks with knives on the corner\n",
            "Text after tokenization: ['it', 'looks', 'a', 'bit', 'muggy', 'out', 'there', 'i', 'said', 'what', 'makes', 'you', 'say', 'that', 'replied', 'my', 'wife', 'i', 'said', 'that', 'gang', 'of', 'blacks', 'with', 'knives', 'on', 'the', 'corner']\n",
            "Text after removing stop words: ['looks', 'bit', 'muggy', 'said', 'makes', 'say', 'replied', 'wife', 'said', 'gang', 'blacks', 'knives', 'corner']\n",
            "Text after Lemmatization: ['look', 'bit', 'muggy', 'said', 'make', 'say', 'replied', 'wife', 'said', 'gang', 'black', 'knife', 'corner']\n",
            "Final pre-processed text: look bit muggy said make say replied wife said gang black knife corner\n",
            "Text after removing HTML tags: keeps happening though someone invented a robot to judge a beauty contest being an evil racist the robot picked all white women so the robot was reprogrammed to be nice re judged the contest only picked white women again robots side with whites every time they know\n",
            "Text after removing non-alphabetic characters and converting to lowercase: keeps happening though someone invented a robot to judge a beauty contest being an evil racist the robot picked all white women so the robot was reprogrammed to be nice re judged the contest only picked white women again robots side with whites every time they know\n",
            "Text after tokenization: ['keeps', 'happening', 'though', 'someone', 'invented', 'a', 'robot', 'to', 'judge', 'a', 'beauty', 'contest', 'being', 'an', 'evil', 'racist', 'the', 'robot', 'picked', 'all', 'white', 'women', 'so', 'the', 'robot', 'was', 'reprogrammed', 'to', 'be', 'nice', 're', 'judged', 'the', 'contest', 'only', 'picked', 'white', 'women', 'again', 'robots', 'side', 'with', 'whites', 'every', 'time', 'they', 'know']\n",
            "Text after removing stop words: ['keeps', 'happening', 'though', 'someone', 'invented', 'robot', 'judge', 'beauty', 'contest', 'evil', 'racist', 'robot', 'picked', 'white', 'women', 'robot', 'reprogrammed', 'nice', 'judged', 'contest', 'picked', 'white', 'women', 'robots', 'side', 'whites', 'every', 'time', 'know']\n",
            "Text after Lemmatization: ['keep', 'happening', 'though', 'someone', 'invented', 'robot', 'judge', 'beauty', 'contest', 'evil', 'racist', 'robot', 'picked', 'white', 'woman', 'robot', 'reprogrammed', 'nice', 'judged', 'contest', 'picked', 'white', 'woman', 'robot', 'side', 'white', 'every', 'time', 'know']\n",
            "Final pre-processed text: keep happening though someone invented robot judge beauty contest evil racist robot picked white woman robot reprogrammed nice judged contest picked white woman robot side white every time know\n",
            "Text after removing HTML tags: a majority of us votes against muslim immigration and thanks to the total death of sweden that is happening right now we will begin with mass deportations in the future we are nationalists we love our country and our way of life consensus in our country is illegal immigrant gtfo\n",
            "Text after removing non-alphabetic characters and converting to lowercase: a majority of us votes against muslim immigration and thanks to the total death of sweden that is happening right now we will begin with mass deportations in the future we are nationalists we love our country and our way of life consensus in our country is illegal immigrant gtfo\n",
            "Text after tokenization: ['a', 'majority', 'of', 'us', 'votes', 'against', 'muslim', 'immigration', 'and', 'thanks', 'to', 'the', 'total', 'death', 'of', 'sweden', 'that', 'is', 'happening', 'right', 'now', 'we', 'will', 'begin', 'with', 'mass', 'deportations', 'in', 'the', 'future', 'we', 'are', 'nationalists', 'we', 'love', 'our', 'country', 'and', 'our', 'way', 'of', 'life', 'consensus', 'in', 'our', 'country', 'is', 'illegal', 'immigrant', 'gtfo']\n",
            "Text after removing stop words: ['majority', 'us', 'votes', 'muslim', 'immigration', 'thanks', 'total', 'death', 'sweden', 'happening', 'right', 'begin', 'mass', 'deportations', 'future', 'nationalists', 'love', 'country', 'way', 'life', 'consensus', 'country', 'illegal', 'immigrant', 'gtfo']\n",
            "Text after Lemmatization: ['majority', 'u', 'vote', 'muslim', 'immigration', 'thanks', 'total', 'death', 'sweden', 'happening', 'right', 'begin', 'mass', 'deportation', 'future', 'nationalist', 'love', 'country', 'way', 'life', 'consensus', 'country', 'illegal', 'immigrant', 'gtfo']\n",
            "Final pre-processed text: majority u vote muslim immigration thanks total death sweden happening right begin mass deportation future nationalist love country way life consensus country illegal immigrant gtfo\n",
            "Text after removing HTML tags: teaching men not to rape is a meme that pisses me off and it has some women so scared of men they think they are going to be gang raped at any moment at any place\n",
            "Text after removing non-alphabetic characters and converting to lowercase: teaching men not to rape is a meme that pisses me off and it has some women so scared of men they think they are going to be gang raped at any moment at any place\n",
            "Text after tokenization: ['teaching', 'men', 'not', 'to', 'rape', 'is', 'a', 'meme', 'that', 'pisses', 'me', 'off', 'and', 'it', 'has', 'some', 'women', 'so', 'scared', 'of', 'men', 'they', 'think', 'they', 'are', 'going', 'to', 'be', 'gang', 'raped', 'at', 'any', 'moment', 'at', 'any', 'place']\n",
            "Text after removing stop words: ['teaching', 'men', 'rape', 'meme', 'pisses', 'women', 'scared', 'men', 'think', 'going', 'gang', 'raped', 'moment', 'place']\n",
            "Text after Lemmatization: ['teaching', 'men', 'rape', 'meme', 'piss', 'woman', 'scared', 'men', 'think', 'going', 'gang', 'raped', 'moment', 'place']\n",
            "Final pre-processed text: teaching men rape meme piss woman scared men think going gang raped moment place\n",
            "Text after removing HTML tags: we are accepting donations on behalf of rose west who has really made a remarkable transformation from straight white female killer to east indian lesbian kindly send donations to killerdonations org\n",
            "Text after removing non-alphabetic characters and converting to lowercase: we are accepting donations on behalf of rose west who has really made a remarkable transformation from straight white female killer to east indian lesbian kindly send donations to killerdonations org\n",
            "Text after tokenization: ['we', 'are', 'accepting', 'donations', 'on', 'behalf', 'of', 'rose', 'west', 'who', 'has', 'really', 'made', 'a', 'remarkable', 'transformation', 'from', 'straight', 'white', 'female', 'killer', 'to', 'east', 'indian', 'lesbian', 'kindly', 'send', 'donations', 'to', 'killerdonations', 'org']\n",
            "Text after removing stop words: ['accepting', 'donations', 'behalf', 'rose', 'west', 'really', 'made', 'remarkable', 'transformation', 'straight', 'white', 'female', 'killer', 'east', 'indian', 'lesbian', 'kindly', 'send', 'donations', 'killerdonations', 'org']\n",
            "Text after Lemmatization: ['accepting', 'donation', 'behalf', 'rose', 'west', 'really', 'made', 'remarkable', 'transformation', 'straight', 'white', 'female', 'killer', 'east', 'indian', 'lesbian', 'kindly', 'send', 'donation', 'killerdonations', 'org']\n",
            "Final pre-processed text: accepting donation behalf rose west really made remarkable transformation straight white female killer east indian lesbian kindly send donation killerdonations org\n",
            "Text after removing HTML tags: the nerve of jews they think poland is us senate to execute all their demands lol withjewsyoulose\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the nerve of jews they think poland is us senate to execute all their demands lol withjewsyoulose\n",
            "Text after tokenization: ['the', 'nerve', 'of', 'jews', 'they', 'think', 'poland', 'is', 'us', 'senate', 'to', 'execute', 'all', 'their', 'demands', 'lol', 'withjewsyoulose']\n",
            "Text after removing stop words: ['nerve', 'jews', 'think', 'poland', 'us', 'senate', 'execute', 'demands', 'lol', 'withjewsyoulose']\n",
            "Text after Lemmatization: ['nerve', 'jew', 'think', 'poland', 'u', 'senate', 'execute', 'demand', 'lol', 'withjewsyoulose']\n",
            "Final pre-processed text: nerve jew think poland u senate execute demand lol withjewsyoulose\n",
            "Text after removing HTML tags: when i watched clinton play the woman card during the election it seemed pretty clear to me that this is why women can not be in politics the fact that everybody did not immediately conclude that is a mystery i do not even see how people rationalize it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: when i watched clinton play the woman card during the election it seemed pretty clear to me that this is why women can not be in politics the fact that everybody did not immediately conclude that is a mystery i do not even see how people rationalize it\n",
            "Text after tokenization: ['when', 'i', 'watched', 'clinton', 'play', 'the', 'woman', 'card', 'during', 'the', 'election', 'it', 'seemed', 'pretty', 'clear', 'to', 'me', 'that', 'this', 'is', 'why', 'women', 'can', 'not', 'be', 'in', 'politics', 'the', 'fact', 'that', 'everybody', 'did', 'not', 'immediately', 'conclude', 'that', 'is', 'a', 'mystery', 'i', 'do', 'not', 'even', 'see', 'how', 'people', 'rationalize', 'it']\n",
            "Text after removing stop words: ['watched', 'clinton', 'play', 'woman', 'card', 'election', 'seemed', 'pretty', 'clear', 'women', 'politics', 'fact', 'everybody', 'immediately', 'conclude', 'mystery', 'even', 'see', 'people', 'rationalize']\n",
            "Text after Lemmatization: ['watched', 'clinton', 'play', 'woman', 'card', 'election', 'seemed', 'pretty', 'clear', 'woman', 'politics', 'fact', 'everybody', 'immediately', 'conclude', 'mystery', 'even', 'see', 'people', 'rationalize']\n",
            "Final pre-processed text: watched clinton play woman card election seemed pretty clear woman politics fact everybody immediately conclude mystery even see people rationalize\n",
            "Text after removing HTML tags: but those motherfuckers that beat that retarded white boy are a o fucking kay right sorry i just fucking loathe hypocrisy\n",
            "Text after removing non-alphabetic characters and converting to lowercase: but those motherfuckers that beat that retarded white boy are a o fucking kay right sorry i just fucking loathe hypocrisy\n",
            "Text after tokenization: ['but', 'those', 'motherfuckers', 'that', 'beat', 'that', 'retarded', 'white', 'boy', 'are', 'a', 'o', 'fucking', 'kay', 'right', 'sorry', 'i', 'just', 'fucking', 'loathe', 'hypocrisy']\n",
            "Text after removing stop words: ['motherfuckers', 'beat', 'retarded', 'white', 'boy', 'fucking', 'kay', 'right', 'sorry', 'fucking', 'loathe', 'hypocrisy']\n",
            "Text after Lemmatization: ['motherfucker', 'beat', 'retarded', 'white', 'boy', 'fucking', 'kay', 'right', 'sorry', 'fucking', 'loathe', 'hypocrisy']\n",
            "Final pre-processed text: motherfucker beat retarded white boy fucking kay right sorry fucking loathe hypocrisy\n",
            "Text after removing HTML tags: so a few days ago the faggots of the yes vote for gay marriage in australia won and if they dont make a bill about supporting freedom of speech when it comes to that topic we are fucked\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so a few days ago the faggots of the yes vote for gay marriage in australia won and if they dont make a bill about supporting freedom of speech when it comes to that topic we are fucked\n",
            "Text after tokenization: ['so', 'a', 'few', 'days', 'ago', 'the', 'faggots', 'of', 'the', 'yes', 'vote', 'for', 'gay', 'marriage', 'in', 'australia', 'won', 'and', 'if', 'they', 'dont', 'make', 'a', 'bill', 'about', 'supporting', 'freedom', 'of', 'speech', 'when', 'it', 'comes', 'to', 'that', 'topic', 'we', 'are', 'fucked']\n",
            "Text after removing stop words: ['days', 'ago', 'faggots', 'yes', 'vote', 'gay', 'marriage', 'australia', 'dont', 'make', 'bill', 'supporting', 'freedom', 'speech', 'comes', 'topic', 'fucked']\n",
            "Text after Lemmatization: ['day', 'ago', 'faggot', 'yes', 'vote', 'gay', 'marriage', 'australia', 'dont', 'make', 'bill', 'supporting', 'freedom', 'speech', 'come', 'topic', 'fucked']\n",
            "Final pre-processed text: day ago faggot yes vote gay marriage australia dont make bill supporting freedom speech come topic fucked\n",
            "Text after removing HTML tags: 2 x 3 xy  x  y solve for xy shibshastihtudes the way to reach ghetto mathematicians is ghetto up the work and all dat  shib  shib shastihtudes  shib shib shastithudes  3 shastitudes  shib shastihtudes  shastidudes  shib  and all dat\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   x   xy  x  y solve for xy shibshastihtudes the way to reach ghetto mathematicians is ghetto up the work and all dat  shib  shib shastihtudes  shib shib shastithudes    shastitudes  shib shastihtudes  shastidudes  shib  and all dat\n",
            "Text after tokenization: ['x', 'xy', 'x', 'y', 'solve', 'for', 'xy', 'shibshastihtudes', 'the', 'way', 'to', 'reach', 'ghetto', 'mathematicians', 'is', 'ghetto', 'up', 'the', 'work', 'and', 'all', 'dat', 'shib', 'shib', 'shastihtudes', 'shib', 'shib', 'shastithudes', 'shastitudes', 'shib', 'shastihtudes', 'shastidudes', 'shib', 'and', 'all', 'dat']\n",
            "Text after removing stop words: ['x', 'xy', 'x', 'solve', 'xy', 'shibshastihtudes', 'way', 'reach', 'ghetto', 'mathematicians', 'ghetto', 'work', 'dat', 'shib', 'shib', 'shastihtudes', 'shib', 'shib', 'shastithudes', 'shastitudes', 'shib', 'shastihtudes', 'shastidudes', 'shib', 'dat']\n",
            "Text after Lemmatization: ['x', 'xy', 'x', 'solve', 'xy', 'shibshastihtudes', 'way', 'reach', 'ghetto', 'mathematician', 'ghetto', 'work', 'dat', 'shib', 'shib', 'shastihtudes', 'shib', 'shib', 'shastithudes', 'shastitudes', 'shib', 'shastihtudes', 'shastidudes', 'shib', 'dat']\n",
            "Final pre-processed text: x xy x solve xy shibshastihtudes way reach ghetto mathematician ghetto work dat shib shib shastihtudes shib shib shastithudes shastitudes shib shastihtudes shastidudes shib dat\n",
            "Text after removing HTML tags: that is defeatist thinking and would not work anyways the muslim state would just spread even further french should fight and kill all the muslims non whites while they still can deportation would be significantly harder and would cost way more white lives than killing non whites on sight\n",
            "Text after removing non-alphabetic characters and converting to lowercase: that is defeatist thinking and would not work anyways the muslim state would just spread even further french should fight and kill all the muslims non whites while they still can deportation would be significantly harder and would cost way more white lives than killing non whites on sight\n",
            "Text after tokenization: ['that', 'is', 'defeatist', 'thinking', 'and', 'would', 'not', 'work', 'anyways', 'the', 'muslim', 'state', 'would', 'just', 'spread', 'even', 'further', 'french', 'should', 'fight', 'and', 'kill', 'all', 'the', 'muslims', 'non', 'whites', 'while', 'they', 'still', 'can', 'deportation', 'would', 'be', 'significantly', 'harder', 'and', 'would', 'cost', 'way', 'more', 'white', 'lives', 'than', 'killing', 'non', 'whites', 'on', 'sight']\n",
            "Text after removing stop words: ['defeatist', 'thinking', 'would', 'work', 'anyways', 'muslim', 'state', 'would', 'spread', 'even', 'french', 'fight', 'kill', 'muslims', 'non', 'whites', 'still', 'deportation', 'would', 'significantly', 'harder', 'would', 'cost', 'way', 'white', 'lives', 'killing', 'non', 'whites', 'sight']\n",
            "Text after Lemmatization: ['defeatist', 'thinking', 'would', 'work', 'anyways', 'muslim', 'state', 'would', 'spread', 'even', 'french', 'fight', 'kill', 'muslim', 'non', 'white', 'still', 'deportation', 'would', 'significantly', 'harder', 'would', 'cost', 'way', 'white', 'life', 'killing', 'non', 'white', 'sight']\n",
            "Final pre-processed text: defeatist thinking would work anyways muslim state would spread even french fight kill muslim non white still deportation would significantly harder would cost way white life killing non white sight\n",
            "Text after removing HTML tags: on one hand i kinda like the safety of living in deep south country on the other hand there not enough ghetto rats to try and riot so i do not get open hunting season when they try to start the next civil war\n",
            "Text after removing non-alphabetic characters and converting to lowercase: on one hand i kinda like the safety of living in deep south country on the other hand there not enough ghetto rats to try and riot so i do not get open hunting season when they try to start the next civil war\n",
            "Text after tokenization: ['on', 'one', 'hand', 'i', 'kinda', 'like', 'the', 'safety', 'of', 'living', 'in', 'deep', 'south', 'country', 'on', 'the', 'other', 'hand', 'there', 'not', 'enough', 'ghetto', 'rats', 'to', 'try', 'and', 'riot', 'so', 'i', 'do', 'not', 'get', 'open', 'hunting', 'season', 'when', 'they', 'try', 'to', 'start', 'the', 'next', 'civil', 'war']\n",
            "Text after removing stop words: ['one', 'hand', 'kinda', 'like', 'safety', 'living', 'deep', 'south', 'country', 'hand', 'enough', 'ghetto', 'rats', 'try', 'riot', 'get', 'open', 'hunting', 'season', 'try', 'start', 'next', 'civil', 'war']\n",
            "Text after Lemmatization: ['one', 'hand', 'kinda', 'like', 'safety', 'living', 'deep', 'south', 'country', 'hand', 'enough', 'ghetto', 'rat', 'try', 'riot', 'get', 'open', 'hunting', 'season', 'try', 'start', 'next', 'civil', 'war']\n",
            "Final pre-processed text: one hand kinda like safety living deep south country hand enough ghetto rat try riot get open hunting season try start next civil war\n",
            "Text after removing HTML tags: strikes what israel is too small for immigrants\n",
            "Text after removing non-alphabetic characters and converting to lowercase: strikes what israel is too small for immigrants\n",
            "Text after tokenization: ['strikes', 'what', 'israel', 'is', 'too', 'small', 'for', 'immigrants']\n",
            "Text after removing stop words: ['strikes', 'israel', 'small', 'immigrants']\n",
            "Text after Lemmatization: ['strike', 'israel', 'small', 'immigrant']\n",
            "Final pre-processed text: strike israel small immigrant\n",
            "Text after removing HTML tags: cernovich is a rapist no joke he raped a chick\n",
            "Text after removing non-alphabetic characters and converting to lowercase: cernovich is a rapist no joke he raped a chick\n",
            "Text after tokenization: ['cernovich', 'is', 'a', 'rapist', 'no', 'joke', 'he', 'raped', 'a', 'chick']\n",
            "Text after removing stop words: ['cernovich', 'rapist', 'joke', 'raped', 'chick']\n",
            "Text after Lemmatization: ['cernovich', 'rapist', 'joke', 'raped', 'chick']\n",
            "Final pre-processed text: cernovich rapist joke raped chick\n",
            "Text after removing HTML tags: this is so disgusting i have adored and fiercely proudly reveled with real women for bravely taking wild boar as prey with clean rifle shots then field dressing their prey thereby simultaneously being more manly and infinitely more female than these faggots humanity is broken\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this is so disgusting i have adored and fiercely proudly reveled with real women for bravely taking wild boar as prey with clean rifle shots then field dressing their prey thereby simultaneously being more manly and infinitely more female than these faggots humanity is broken\n",
            "Text after tokenization: ['this', 'is', 'so', 'disgusting', 'i', 'have', 'adored', 'and', 'fiercely', 'proudly', 'reveled', 'with', 'real', 'women', 'for', 'bravely', 'taking', 'wild', 'boar', 'as', 'prey', 'with', 'clean', 'rifle', 'shots', 'then', 'field', 'dressing', 'their', 'prey', 'thereby', 'simultaneously', 'being', 'more', 'manly', 'and', 'infinitely', 'more', 'female', 'than', 'these', 'faggots', 'humanity', 'is', 'broken']\n",
            "Text after removing stop words: ['disgusting', 'adored', 'fiercely', 'proudly', 'reveled', 'real', 'women', 'bravely', 'taking', 'wild', 'boar', 'prey', 'clean', 'rifle', 'shots', 'field', 'dressing', 'prey', 'thereby', 'simultaneously', 'manly', 'infinitely', 'female', 'faggots', 'humanity', 'broken']\n",
            "Text after Lemmatization: ['disgusting', 'adored', 'fiercely', 'proudly', 'reveled', 'real', 'woman', 'bravely', 'taking', 'wild', 'boar', 'prey', 'clean', 'rifle', 'shot', 'field', 'dressing', 'prey', 'thereby', 'simultaneously', 'manly', 'infinitely', 'female', 'faggot', 'humanity', 'broken']\n",
            "Final pre-processed text: disgusting adored fiercely proudly reveled real woman bravely taking wild boar prey clean rifle shot field dressing prey thereby simultaneously manly infinitely female faggot humanity broken\n",
            "Text after removing HTML tags: i wasn t born yesterday kike when i infiltrated the nuis on kikebook in  they had dozens of its not the jews its the zionists fake palestinian groups to fool the goyim get gassed kike\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i wasn t born yesterday kike when i infiltrated the nuis on kikebook in  they had dozens of its not the jews its the zionists fake palestinian groups to fool the goyim get gassed kike\n",
            "Text after tokenization: ['i', 'wasn', 't', 'born', 'yesterday', 'kike', 'when', 'i', 'infiltrated', 'the', 'nuis', 'on', 'kikebook', 'in', 'they', 'had', 'dozens', 'of', 'its', 'not', 'the', 'jews', 'its', 'the', 'zionists', 'fake', 'palestinian', 'groups', 'to', 'fool', 'the', 'goyim', 'get', 'gassed', 'kike']\n",
            "Text after removing stop words: ['born', 'yesterday', 'kike', 'infiltrated', 'nuis', 'kikebook', 'dozens', 'jews', 'zionists', 'fake', 'palestinian', 'groups', 'fool', 'goyim', 'get', 'gassed', 'kike']\n",
            "Text after Lemmatization: ['born', 'yesterday', 'kike', 'infiltrated', 'nuis', 'kikebook', 'dozen', 'jew', 'zionist', 'fake', 'palestinian', 'group', 'fool', 'goy', 'get', 'gassed', 'kike']\n",
            "Final pre-processed text: born yesterday kike infiltrated nuis kikebook dozen jew zionist fake palestinian group fool goy get gassed kike\n",
            "Text after removing HTML tags: attacking the retard left is lame any way they block you for saying nices things to them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: attacking the retard left is lame any way they block you for saying nices things to them\n",
            "Text after tokenization: ['attacking', 'the', 'retard', 'left', 'is', 'lame', 'any', 'way', 'they', 'block', 'you', 'for', 'saying', 'nices', 'things', 'to', 'them']\n",
            "Text after removing stop words: ['attacking', 'retard', 'left', 'lame', 'way', 'block', 'saying', 'nices', 'things']\n",
            "Text after Lemmatization: ['attacking', 'retard', 'left', 'lame', 'way', 'block', 'saying', 'nice', 'thing']\n",
            "Final pre-processed text: attacking retard left lame way block saying nice thing\n",
            "Text after removing HTML tags: nah just calling you the larping faggot that you are aloud somebody has to point out that you are an oversensitive sjw\n",
            "Text after removing non-alphabetic characters and converting to lowercase: nah just calling you the larping faggot that you are aloud somebody has to point out that you are an oversensitive sjw\n",
            "Text after tokenization: ['nah', 'just', 'calling', 'you', 'the', 'larping', 'faggot', 'that', 'you', 'are', 'aloud', 'somebody', 'has', 'to', 'point', 'out', 'that', 'you', 'are', 'an', 'oversensitive', 'sjw']\n",
            "Text after removing stop words: ['nah', 'calling', 'larping', 'faggot', 'aloud', 'somebody', 'point', 'oversensitive', 'sjw']\n",
            "Text after Lemmatization: ['nah', 'calling', 'larping', 'faggot', 'aloud', 'somebody', 'point', 'oversensitive', 'sjw']\n",
            "Final pre-processed text: nah calling larping faggot aloud somebody point oversensitive sjw\n",
            "Text after removing HTML tags: this faggot  likes muslim goat dick advocates for white genocide and shills using the same repeated script i am sure that he a paid shill\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this faggot  likes muslim goat dick advocates for white genocide and shills using the same repeated script i am sure that he a paid shill\n",
            "Text after tokenization: ['this', 'faggot', 'likes', 'muslim', 'goat', 'dick', 'advocates', 'for', 'white', 'genocide', 'and', 'shills', 'using', 'the', 'same', 'repeated', 'script', 'i', 'am', 'sure', 'that', 'he', 'a', 'paid', 'shill']\n",
            "Text after removing stop words: ['faggot', 'likes', 'muslim', 'goat', 'dick', 'advocates', 'white', 'genocide', 'shills', 'using', 'repeated', 'script', 'sure', 'paid', 'shill']\n",
            "Text after Lemmatization: ['faggot', 'like', 'muslim', 'goat', 'dick', 'advocate', 'white', 'genocide', 'shill', 'using', 'repeated', 'script', 'sure', 'paid', 'shill']\n",
            "Final pre-processed text: faggot like muslim goat dick advocate white genocide shill using repeated script sure paid shill\n",
            "Text after removing HTML tags: like i said these pedantic stop talking about x things alt right spergs do are retarded and lead nowhere productive i do not have time for them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: like i said these pedantic stop talking about x things alt right spergs do are retarded and lead nowhere productive i do not have time for them\n",
            "Text after tokenization: ['like', 'i', 'said', 'these', 'pedantic', 'stop', 'talking', 'about', 'x', 'things', 'alt', 'right', 'spergs', 'do', 'are', 'retarded', 'and', 'lead', 'nowhere', 'productive', 'i', 'do', 'not', 'have', 'time', 'for', 'them']\n",
            "Text after removing stop words: ['like', 'said', 'pedantic', 'stop', 'talking', 'x', 'things', 'alt', 'right', 'spergs', 'retarded', 'lead', 'nowhere', 'productive', 'time']\n",
            "Text after Lemmatization: ['like', 'said', 'pedantic', 'stop', 'talking', 'x', 'thing', 'alt', 'right', 'spergs', 'retarded', 'lead', 'nowhere', 'productive', 'time']\n",
            "Final pre-processed text: like said pedantic stop talking x thing alt right spergs retarded lead nowhere productive time\n",
            "Text after removing HTML tags: thanksgiving is a great day shitposting to trigger neurotic shitlibs but you know what i will save that for the faggots over on twatter to all my gab fam a happy and blessed holiday take stock of your blessings and be thankful happythanksgiving\n",
            "Text after removing non-alphabetic characters and converting to lowercase: thanksgiving is a great day shitposting to trigger neurotic shitlibs but you know what i will save that for the faggots over on twatter to all my gab fam a happy and blessed holiday take stock of your blessings and be thankful happythanksgiving\n",
            "Text after tokenization: ['thanksgiving', 'is', 'a', 'great', 'day', 'shitposting', 'to', 'trigger', 'neurotic', 'shitlibs', 'but', 'you', 'know', 'what', 'i', 'will', 'save', 'that', 'for', 'the', 'faggots', 'over', 'on', 'twatter', 'to', 'all', 'my', 'gab', 'fam', 'a', 'happy', 'and', 'blessed', 'holiday', 'take', 'stock', 'of', 'your', 'blessings', 'and', 'be', 'thankful', 'happythanksgiving']\n",
            "Text after removing stop words: ['thanksgiving', 'great', 'day', 'shitposting', 'trigger', 'neurotic', 'shitlibs', 'know', 'save', 'faggots', 'twatter', 'gab', 'fam', 'happy', 'blessed', 'holiday', 'take', 'stock', 'blessings', 'thankful', 'happythanksgiving']\n",
            "Text after Lemmatization: ['thanksgiving', 'great', 'day', 'shitposting', 'trigger', 'neurotic', 'shitlibs', 'know', 'save', 'faggot', 'twatter', 'gab', 'fam', 'happy', 'blessed', 'holiday', 'take', 'stock', 'blessing', 'thankful', 'happythanksgiving']\n",
            "Final pre-processed text: thanksgiving great day shitposting trigger neurotic shitlibs know save faggot twatter gab fam happy blessed holiday take stock blessing thankful happythanksgiving\n",
            "Text after removing HTML tags: no democrats do not want them to assimilate as proven by their actions after the civil war right up to today where they make sure blacks are in ghettos like chicago and no where near them does obama live in the a black hood did not think so\n",
            "Text after removing non-alphabetic characters and converting to lowercase: no democrats do not want them to assimilate as proven by their actions after the civil war right up to today where they make sure blacks are in ghettos like chicago and no where near them does obama live in the a black hood did not think so\n",
            "Text after tokenization: ['no', 'democrats', 'do', 'not', 'want', 'them', 'to', 'assimilate', 'as', 'proven', 'by', 'their', 'actions', 'after', 'the', 'civil', 'war', 'right', 'up', 'to', 'today', 'where', 'they', 'make', 'sure', 'blacks', 'are', 'in', 'ghettos', 'like', 'chicago', 'and', 'no', 'where', 'near', 'them', 'does', 'obama', 'live', 'in', 'the', 'a', 'black', 'hood', 'did', 'not', 'think', 'so']\n",
            "Text after removing stop words: ['democrats', 'want', 'assimilate', 'proven', 'actions', 'civil', 'war', 'right', 'today', 'make', 'sure', 'blacks', 'ghettos', 'like', 'chicago', 'near', 'obama', 'live', 'black', 'hood', 'think']\n",
            "Text after Lemmatization: ['democrat', 'want', 'assimilate', 'proven', 'action', 'civil', 'war', 'right', 'today', 'make', 'sure', 'black', 'ghetto', 'like', 'chicago', 'near', 'obama', 'live', 'black', 'hood', 'think']\n",
            "Final pre-processed text: democrat want assimilate proven action civil war right today make sure black ghetto like chicago near obama live black hood think\n",
            "Text after removing HTML tags: i certainly hope this report is true alwaleed is definitely our enemy they can beat him upside down until all his blood comes out his nose and mouth he epitomizes the jihad supporting filthy rich raghead cocksucker fuckyoualwaleed servesyourighttosufferalwaleed\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i certainly hope this report is true alwaleed is definitely our enemy they can beat him upside down until all his blood comes out his nose and mouth he epitomizes the jihad supporting filthy rich raghead cocksucker fuckyoualwaleed servesyourighttosufferalwaleed\n",
            "Text after tokenization: ['i', 'certainly', 'hope', 'this', 'report', 'is', 'true', 'alwaleed', 'is', 'definitely', 'our', 'enemy', 'they', 'can', 'beat', 'him', 'upside', 'down', 'until', 'all', 'his', 'blood', 'comes', 'out', 'his', 'nose', 'and', 'mouth', 'he', 'epitomizes', 'the', 'jihad', 'supporting', 'filthy', 'rich', 'raghead', 'cocksucker', 'fuckyoualwaleed', 'servesyourighttosufferalwaleed']\n",
            "Text after removing stop words: ['certainly', 'hope', 'report', 'true', 'alwaleed', 'definitely', 'enemy', 'beat', 'upside', 'blood', 'comes', 'nose', 'mouth', 'epitomizes', 'jihad', 'supporting', 'filthy', 'rich', 'raghead', 'cocksucker', 'fuckyoualwaleed', 'servesyourighttosufferalwaleed']\n",
            "Text after Lemmatization: ['certainly', 'hope', 'report', 'true', 'alwaleed', 'definitely', 'enemy', 'beat', 'upside', 'blood', 'come', 'nose', 'mouth', 'epitomizes', 'jihad', 'supporting', 'filthy', 'rich', 'raghead', 'cocksucker', 'fuckyoualwaleed', 'servesyourighttosufferalwaleed']\n",
            "Final pre-processed text: certainly hope report true alwaleed definitely enemy beat upside blood come nose mouth epitomizes jihad supporting filthy rich raghead cocksucker fuckyoualwaleed servesyourighttosufferalwaleed\n",
            "Text after removing HTML tags: you know how decent people say that islamists need to band together against the radical muz among them so should god fearing blacks band together against these sub human iq feral murderer freaks that happen to be black we are coming to a decision point are not we\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you know how decent people say that islamists need to band together against the radical muz among them so should god fearing blacks band together against these sub human iq feral murderer freaks that happen to be black we are coming to a decision point are not we\n",
            "Text after tokenization: ['you', 'know', 'how', 'decent', 'people', 'say', 'that', 'islamists', 'need', 'to', 'band', 'together', 'against', 'the', 'radical', 'muz', 'among', 'them', 'so', 'should', 'god', 'fearing', 'blacks', 'band', 'together', 'against', 'these', 'sub', 'human', 'iq', 'feral', 'murderer', 'freaks', 'that', 'happen', 'to', 'be', 'black', 'we', 'are', 'coming', 'to', 'a', 'decision', 'point', 'are', 'not', 'we']\n",
            "Text after removing stop words: ['know', 'decent', 'people', 'say', 'islamists', 'need', 'band', 'together', 'radical', 'muz', 'among', 'god', 'fearing', 'blacks', 'band', 'together', 'sub', 'human', 'iq', 'feral', 'murderer', 'freaks', 'happen', 'black', 'coming', 'decision', 'point']\n",
            "Text after Lemmatization: ['know', 'decent', 'people', 'say', 'islamist', 'need', 'band', 'together', 'radical', 'muz', 'among', 'god', 'fearing', 'black', 'band', 'together', 'sub', 'human', 'iq', 'feral', 'murderer', 'freak', 'happen', 'black', 'coming', 'decision', 'point']\n",
            "Final pre-processed text: know decent people say islamist need band together radical muz among god fearing black band together sub human iq feral murderer freak happen black coming decision point\n",
            "Text after removing HTML tags: britcops are cowards they know the whites they arrest for speaking the truth will not blow them up or cut their throats like a sand nigger might\n",
            "Text after removing non-alphabetic characters and converting to lowercase: britcops are cowards they know the whites they arrest for speaking the truth will not blow them up or cut their throats like a sand nigger might\n",
            "Text after tokenization: ['britcops', 'are', 'cowards', 'they', 'know', 'the', 'whites', 'they', 'arrest', 'for', 'speaking', 'the', 'truth', 'will', 'not', 'blow', 'them', 'up', 'or', 'cut', 'their', 'throats', 'like', 'a', 'sand', 'nigger', 'might']\n",
            "Text after removing stop words: ['britcops', 'cowards', 'know', 'whites', 'arrest', 'speaking', 'truth', 'blow', 'cut', 'throats', 'like', 'sand', 'nigger', 'might']\n",
            "Text after Lemmatization: ['britcops', 'coward', 'know', 'white', 'arrest', 'speaking', 'truth', 'blow', 'cut', 'throat', 'like', 'sand', 'nigger', 'might']\n",
            "Final pre-processed text: britcops coward know white arrest speaking truth blow cut throat like sand nigger might\n",
            "Text after removing HTML tags: reminder that having asperger and being retarded or two entirely different things\n",
            "Text after removing non-alphabetic characters and converting to lowercase: reminder that having asperger and being retarded or two entirely different things\n",
            "Text after tokenization: ['reminder', 'that', 'having', 'asperger', 'and', 'being', 'retarded', 'or', 'two', 'entirely', 'different', 'things']\n",
            "Text after removing stop words: ['reminder', 'asperger', 'retarded', 'two', 'entirely', 'different', 'things']\n",
            "Text after Lemmatization: ['reminder', 'asperger', 'retarded', 'two', 'entirely', 'different', 'thing']\n",
            "Final pre-processed text: reminder asperger retarded two entirely different thing\n",
            "Text after removing HTML tags: you never know there was that one girl who made herself blind and a bunch of other deranged liberals that harm themselves to virtue signal there might be some women that undergo fgm to virtue signal\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you never know there was that one girl who made herself blind and a bunch of other deranged liberals that harm themselves to virtue signal there might be some women that undergo fgm to virtue signal\n",
            "Text after tokenization: ['you', 'never', 'know', 'there', 'was', 'that', 'one', 'girl', 'who', 'made', 'herself', 'blind', 'and', 'a', 'bunch', 'of', 'other', 'deranged', 'liberals', 'that', 'harm', 'themselves', 'to', 'virtue', 'signal', 'there', 'might', 'be', 'some', 'women', 'that', 'undergo', 'fgm', 'to', 'virtue', 'signal']\n",
            "Text after removing stop words: ['never', 'know', 'one', 'girl', 'made', 'blind', 'bunch', 'deranged', 'liberals', 'harm', 'virtue', 'signal', 'might', 'women', 'undergo', 'fgm', 'virtue', 'signal']\n",
            "Text after Lemmatization: ['never', 'know', 'one', 'girl', 'made', 'blind', 'bunch', 'deranged', 'liberal', 'harm', 'virtue', 'signal', 'might', 'woman', 'undergo', 'fgm', 'virtue', 'signal']\n",
            "Final pre-processed text: never know one girl made blind bunch deranged liberal harm virtue signal might woman undergo fgm virtue signal\n",
            "Text after removing HTML tags: prince harry diana is his mom but who is his daddy and why would he marry a piece of shit but the brits are not very smart londonstan has a muzzie asshole for mayor and they are taking in all kinds of mussie wifebeaters and goathumpers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: prince harry diana is his mom but who is his daddy and why would he marry a piece of shit but the brits are not very smart londonstan has a muzzie asshole for mayor and they are taking in all kinds of mussie wifebeaters and goathumpers\n",
            "Text after tokenization: ['prince', 'harry', 'diana', 'is', 'his', 'mom', 'but', 'who', 'is', 'his', 'daddy', 'and', 'why', 'would', 'he', 'marry', 'a', 'piece', 'of', 'shit', 'but', 'the', 'brits', 'are', 'not', 'very', 'smart', 'londonstan', 'has', 'a', 'muzzie', 'asshole', 'for', 'mayor', 'and', 'they', 'are', 'taking', 'in', 'all', 'kinds', 'of', 'mussie', 'wifebeaters', 'and', 'goathumpers']\n",
            "Text after removing stop words: ['prince', 'harry', 'diana', 'mom', 'daddy', 'would', 'marry', 'piece', 'shit', 'brits', 'smart', 'londonstan', 'muzzie', 'asshole', 'mayor', 'taking', 'kinds', 'mussie', 'wifebeaters', 'goathumpers']\n",
            "Text after Lemmatization: ['prince', 'harry', 'diana', 'mom', 'daddy', 'would', 'marry', 'piece', 'shit', 'brit', 'smart', 'londonstan', 'muzzie', 'asshole', 'mayor', 'taking', 'kind', 'mussie', 'wifebeaters', 'goathumpers']\n",
            "Final pre-processed text: prince harry diana mom daddy would marry piece shit brit smart londonstan muzzie asshole mayor taking kind mussie wifebeaters goathumpers\n",
            "Text after removing HTML tags: canadians are being ruled by a retarded asshole\n",
            "Text after removing non-alphabetic characters and converting to lowercase: canadians are being ruled by a retarded asshole\n",
            "Text after tokenization: ['canadians', 'are', 'being', 'ruled', 'by', 'a', 'retarded', 'asshole']\n",
            "Text after removing stop words: ['canadians', 'ruled', 'retarded', 'asshole']\n",
            "Text after Lemmatization: ['canadian', 'ruled', 'retarded', 'asshole']\n",
            "Final pre-processed text: canadian ruled retarded asshole\n",
            "Text after removing HTML tags: hm i am talking to women today who are blaming other women for all of this sexual harassment claiming too many women act way too provocatively in the workplace 🤔\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hm i am talking to women today who are blaming other women for all of this sexual harassment claiming too many women act way too provocatively in the workplace  \n",
            "Text after tokenization: ['hm', 'i', 'am', 'talking', 'to', 'women', 'today', 'who', 'are', 'blaming', 'other', 'women', 'for', 'all', 'of', 'this', 'sexual', 'harassment', 'claiming', 'too', 'many', 'women', 'act', 'way', 'too', 'provocatively', 'in', 'the', 'workplace']\n",
            "Text after removing stop words: ['hm', 'talking', 'women', 'today', 'blaming', 'women', 'sexual', 'harassment', 'claiming', 'many', 'women', 'act', 'way', 'provocatively', 'workplace']\n",
            "Text after Lemmatization: ['hm', 'talking', 'woman', 'today', 'blaming', 'woman', 'sexual', 'harassment', 'claiming', 'many', 'woman', 'act', 'way', 'provocatively', 'workplace']\n",
            "Final pre-processed text: hm talking woman today blaming woman sexual harassment claiming many woman act way provocatively workplace\n",
            "Text after removing HTML tags: there e nothing in the world like first rate pussy only faggots think there something wrong with this sentimental truism which i have heard even my daddy voice using different wording perhaps but this is what he meant momma blushed with embarrassed pride \n",
            "Text after removing non-alphabetic characters and converting to lowercase: there e nothing in the world like first rate pussy only faggots think there something wrong with this sentimental truism which i have heard even my daddy voice using different wording perhaps but this is what he meant momma blushed with embarrassed pride \n",
            "Text after tokenization: ['there', 'e', 'nothing', 'in', 'the', 'world', 'like', 'first', 'rate', 'pussy', 'only', 'faggots', 'think', 'there', 'something', 'wrong', 'with', 'this', 'sentimental', 'truism', 'which', 'i', 'have', 'heard', 'even', 'my', 'daddy', 'voice', 'using', 'different', 'wording', 'perhaps', 'but', 'this', 'is', 'what', 'he', 'meant', 'momma', 'blushed', 'with', 'embarrassed', 'pride']\n",
            "Text after removing stop words: ['e', 'nothing', 'world', 'like', 'first', 'rate', 'pussy', 'faggots', 'think', 'something', 'wrong', 'sentimental', 'truism', 'heard', 'even', 'daddy', 'voice', 'using', 'different', 'wording', 'perhaps', 'meant', 'momma', 'blushed', 'embarrassed', 'pride']\n",
            "Text after Lemmatization: ['e', 'nothing', 'world', 'like', 'first', 'rate', 'pussy', 'faggot', 'think', 'something', 'wrong', 'sentimental', 'truism', 'heard', 'even', 'daddy', 'voice', 'using', 'different', 'wording', 'perhaps', 'meant', 'momma', 'blushed', 'embarrassed', 'pride']\n",
            "Final pre-processed text: e nothing world like first rate pussy faggot think something wrong sentimental truism heard even daddy voice using different wording perhaps meant momma blushed embarrassed pride\n",
            "Text after removing HTML tags: rosie is an insane lesbian being a lesbian tells you she is mentally ill\n",
            "Text after removing non-alphabetic characters and converting to lowercase: rosie is an insane lesbian being a lesbian tells you she is mentally ill\n",
            "Text after tokenization: ['rosie', 'is', 'an', 'insane', 'lesbian', 'being', 'a', 'lesbian', 'tells', 'you', 'she', 'is', 'mentally', 'ill']\n",
            "Text after removing stop words: ['rosie', 'insane', 'lesbian', 'lesbian', 'tells', 'mentally', 'ill']\n",
            "Text after Lemmatization: ['rosie', 'insane', 'lesbian', 'lesbian', 'tell', 'mentally', 'ill']\n",
            "Final pre-processed text: rosie insane lesbian lesbian tell mentally ill\n",
            "Text after removing HTML tags: what wonderful news arabs are getting sweet deals and trade with us meanwhile we are over run by low iq shit skins murdering white women and this fucking useless twat wants to play lets make a deal never elect businessmen\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what wonderful news arabs are getting sweet deals and trade with us meanwhile we are over run by low iq shit skins murdering white women and this fucking useless twat wants to play lets make a deal never elect businessmen\n",
            "Text after tokenization: ['what', 'wonderful', 'news', 'arabs', 'are', 'getting', 'sweet', 'deals', 'and', 'trade', 'with', 'us', 'meanwhile', 'we', 'are', 'over', 'run', 'by', 'low', 'iq', 'shit', 'skins', 'murdering', 'white', 'women', 'and', 'this', 'fucking', 'useless', 'twat', 'wants', 'to', 'play', 'lets', 'make', 'a', 'deal', 'never', 'elect', 'businessmen']\n",
            "Text after removing stop words: ['wonderful', 'news', 'arabs', 'getting', 'sweet', 'deals', 'trade', 'us', 'meanwhile', 'run', 'low', 'iq', 'shit', 'skins', 'murdering', 'white', 'women', 'fucking', 'useless', 'twat', 'wants', 'play', 'lets', 'make', 'deal', 'never', 'elect', 'businessmen']\n",
            "Text after Lemmatization: ['wonderful', 'news', 'arab', 'getting', 'sweet', 'deal', 'trade', 'u', 'meanwhile', 'run', 'low', 'iq', 'shit', 'skin', 'murdering', 'white', 'woman', 'fucking', 'useless', 'twat', 'want', 'play', 'let', 'make', 'deal', 'never', 'elect', 'businessmen']\n",
            "Final pre-processed text: wonderful news arab getting sweet deal trade u meanwhile run low iq shit skin murdering white woman fucking useless twat want play let make deal never elect businessmen\n",
            "Text after removing HTML tags: it does not bother me it is rather amusing to see someone like yourself claim to be intelligent then resort instantly to baseless accusations when proven wrong you communist jews are all the same\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it does not bother me it is rather amusing to see someone like yourself claim to be intelligent then resort instantly to baseless accusations when proven wrong you communist jews are all the same\n",
            "Text after tokenization: ['it', 'does', 'not', 'bother', 'me', 'it', 'is', 'rather', 'amusing', 'to', 'see', 'someone', 'like', 'yourself', 'claim', 'to', 'be', 'intelligent', 'then', 'resort', 'instantly', 'to', 'baseless', 'accusations', 'when', 'proven', 'wrong', 'you', 'communist', 'jews', 'are', 'all', 'the', 'same']\n",
            "Text after removing stop words: ['bother', 'rather', 'amusing', 'see', 'someone', 'like', 'claim', 'intelligent', 'resort', 'instantly', 'baseless', 'accusations', 'proven', 'wrong', 'communist', 'jews']\n",
            "Text after Lemmatization: ['bother', 'rather', 'amusing', 'see', 'someone', 'like', 'claim', 'intelligent', 'resort', 'instantly', 'baseless', 'accusation', 'proven', 'wrong', 'communist', 'jew']\n",
            "Final pre-processed text: bother rather amusing see someone like claim intelligent resort instantly baseless accusation proven wrong communist jew\n",
            "Text after removing HTML tags: i do not want to hear about systemic racism and power privilege from asians and kikes since they are statistically wealthier than us shut the fuck up\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i do not want to hear about systemic racism and power privilege from asians and kikes since they are statistically wealthier than us shut the fuck up\n",
            "Text after tokenization: ['i', 'do', 'not', 'want', 'to', 'hear', 'about', 'systemic', 'racism', 'and', 'power', 'privilege', 'from', 'asians', 'and', 'kikes', 'since', 'they', 'are', 'statistically', 'wealthier', 'than', 'us', 'shut', 'the', 'fuck', 'up']\n",
            "Text after removing stop words: ['want', 'hear', 'systemic', 'racism', 'power', 'privilege', 'asians', 'kikes', 'since', 'statistically', 'wealthier', 'us', 'shut', 'fuck']\n",
            "Text after Lemmatization: ['want', 'hear', 'systemic', 'racism', 'power', 'privilege', 'asian', 'kike', 'since', 'statistically', 'wealthier', 'u', 'shut', 'fuck']\n",
            "Final pre-processed text: want hear systemic racism power privilege asian kike since statistically wealthier u shut fuck\n",
            "Text after removing HTML tags: have at it i am white and glad of it niggers hate niggers more than white people hate white people but then niggers are morons and those are the smart niggers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: have at it i am white and glad of it niggers hate niggers more than white people hate white people but then niggers are morons and those are the smart niggers\n",
            "Text after tokenization: ['have', 'at', 'it', 'i', 'am', 'white', 'and', 'glad', 'of', 'it', 'niggers', 'hate', 'niggers', 'more', 'than', 'white', 'people', 'hate', 'white', 'people', 'but', 'then', 'niggers', 'are', 'morons', 'and', 'those', 'are', 'the', 'smart', 'niggers']\n",
            "Text after removing stop words: ['white', 'glad', 'niggers', 'hate', 'niggers', 'white', 'people', 'hate', 'white', 'people', 'niggers', 'morons', 'smart', 'niggers']\n",
            "Text after Lemmatization: ['white', 'glad', 'nigger', 'hate', 'nigger', 'white', 'people', 'hate', 'white', 'people', 'nigger', 'moron', 'smart', 'nigger']\n",
            "Final pre-processed text: white glad nigger hate nigger white people hate white people nigger moron smart nigger\n",
            "Text after removing HTML tags: some people just do not get it islam is a religion not a race any concerns about modern violence performed in the name of that religion have nothing to do with racism or discrimination why is not he speaking out against the vicious behaviour in the videos condemn that before yelling racism\n",
            "Text after removing non-alphabetic characters and converting to lowercase: some people just do not get it islam is a religion not a race any concerns about modern violence performed in the name of that religion have nothing to do with racism or discrimination why is not he speaking out against the vicious behaviour in the videos condemn that before yelling racism\n",
            "Text after tokenization: ['some', 'people', 'just', 'do', 'not', 'get', 'it', 'islam', 'is', 'a', 'religion', 'not', 'a', 'race', 'any', 'concerns', 'about', 'modern', 'violence', 'performed', 'in', 'the', 'name', 'of', 'that', 'religion', 'have', 'nothing', 'to', 'do', 'with', 'racism', 'or', 'discrimination', 'why', 'is', 'not', 'he', 'speaking', 'out', 'against', 'the', 'vicious', 'behaviour', 'in', 'the', 'videos', 'condemn', 'that', 'before', 'yelling', 'racism']\n",
            "Text after removing stop words: ['people', 'get', 'islam', 'religion', 'race', 'concerns', 'modern', 'violence', 'performed', 'name', 'religion', 'nothing', 'racism', 'discrimination', 'speaking', 'vicious', 'behaviour', 'videos', 'condemn', 'yelling', 'racism']\n",
            "Text after Lemmatization: ['people', 'get', 'islam', 'religion', 'race', 'concern', 'modern', 'violence', 'performed', 'name', 'religion', 'nothing', 'racism', 'discrimination', 'speaking', 'vicious', 'behaviour', 'video', 'condemn', 'yelling', 'racism']\n",
            "Final pre-processed text: people get islam religion race concern modern violence performed name religion nothing racism discrimination speaking vicious behaviour video condemn yelling racism\n",
            "Text after removing HTML tags: the myth of the hardworking immigrant who just wants to be an apple pie loving american and pay taxes is pure bullshit it time people start waking up and realizing that\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the myth of the hardworking immigrant who just wants to be an apple pie loving american and pay taxes is pure bullshit it time people start waking up and realizing that\n",
            "Text after tokenization: ['the', 'myth', 'of', 'the', 'hardworking', 'immigrant', 'who', 'just', 'wants', 'to', 'be', 'an', 'apple', 'pie', 'loving', 'american', 'and', 'pay', 'taxes', 'is', 'pure', 'bullshit', 'it', 'time', 'people', 'start', 'waking', 'up', 'and', 'realizing', 'that']\n",
            "Text after removing stop words: ['myth', 'hardworking', 'immigrant', 'wants', 'apple', 'pie', 'loving', 'american', 'pay', 'taxes', 'pure', 'bullshit', 'time', 'people', 'start', 'waking', 'realizing']\n",
            "Text after Lemmatization: ['myth', 'hardworking', 'immigrant', 'want', 'apple', 'pie', 'loving', 'american', 'pay', 'tax', 'pure', 'bullshit', 'time', 'people', 'start', 'waking', 'realizing']\n",
            "Final pre-processed text: myth hardworking immigrant want apple pie loving american pay tax pure bullshit time people start waking realizing\n",
            "Text after removing HTML tags: lol the goatfucker on the top comment tho\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lol the goatfucker on the top comment tho\n",
            "Text after tokenization: ['lol', 'the', 'goatfucker', 'on', 'the', 'top', 'comment', 'tho']\n",
            "Text after removing stop words: ['lol', 'goatfucker', 'top', 'comment', 'tho']\n",
            "Text after Lemmatization: ['lol', 'goatfucker', 'top', 'comment', 'tho']\n",
            "Final pre-processed text: lol goatfucker top comment tho\n",
            "Text after removing HTML tags: dickpepe will not let the antifa scum get away with the violence and destruction dick pepe no  pepe the bloody bikelock \n",
            "Text after removing non-alphabetic characters and converting to lowercase: dickpepe will not let the antifa scum get away with the violence and destruction dick pepe no  pepe the bloody bikelock \n",
            "Text after tokenization: ['dickpepe', 'will', 'not', 'let', 'the', 'antifa', 'scum', 'get', 'away', 'with', 'the', 'violence', 'and', 'destruction', 'dick', 'pepe', 'no', 'pepe', 'the', 'bloody', 'bikelock']\n",
            "Text after removing stop words: ['dickpepe', 'let', 'antifa', 'scum', 'get', 'away', 'violence', 'destruction', 'dick', 'pepe', 'pepe', 'bloody', 'bikelock']\n",
            "Text after Lemmatization: ['dickpepe', 'let', 'antifa', 'scum', 'get', 'away', 'violence', 'destruction', 'dick', 'pepe', 'pepe', 'bloody', 'bikelock']\n",
            "Final pre-processed text: dickpepe let antifa scum get away violence destruction dick pepe pepe bloody bikelock\n",
            "Text after removing HTML tags: i am so happy potus told those silly brits to piss off imagine them allowing their trained chimps to order him to stay out it already too late for britain what white person is willing to die for a people who freely give their children to monkeys to destroy \n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am so happy potus told those silly brits to piss off imagine them allowing their trained chimps to order him to stay out it already too late for britain what white person is willing to die for a people who freely give their children to monkeys to destroy \n",
            "Text after tokenization: ['i', 'am', 'so', 'happy', 'potus', 'told', 'those', 'silly', 'brits', 'to', 'piss', 'off', 'imagine', 'them', 'allowing', 'their', 'trained', 'chimps', 'to', 'order', 'him', 'to', 'stay', 'out', 'it', 'already', 'too', 'late', 'for', 'britain', 'what', 'white', 'person', 'is', 'willing', 'to', 'die', 'for', 'a', 'people', 'who', 'freely', 'give', 'their', 'children', 'to', 'monkeys', 'to', 'destroy']\n",
            "Text after removing stop words: ['happy', 'potus', 'told', 'silly', 'brits', 'piss', 'imagine', 'allowing', 'trained', 'chimps', 'order', 'stay', 'already', 'late', 'britain', 'white', 'person', 'willing', 'die', 'people', 'freely', 'give', 'children', 'monkeys', 'destroy']\n",
            "Text after Lemmatization: ['happy', 'potus', 'told', 'silly', 'brit', 'piss', 'imagine', 'allowing', 'trained', 'chimp', 'order', 'stay', 'already', 'late', 'britain', 'white', 'person', 'willing', 'die', 'people', 'freely', 'give', 'child', 'monkey', 'destroy']\n",
            "Final pre-processed text: happy potus told silly brit piss imagine allowing trained chimp order stay already late britain white person willing die people freely give child monkey destroy\n",
            "Text after removing HTML tags: fuck your data you sound like a homo i am not going to get sucked into your gay game of name the fed for all i know you are the fed alex jones is definitely some kind of fed or jew loving gatekeeper or crypto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fuck your data you sound like a homo i am not going to get sucked into your gay game of name the fed for all i know you are the fed alex jones is definitely some kind of fed or jew loving gatekeeper or crypto\n",
            "Text after tokenization: ['fuck', 'your', 'data', 'you', 'sound', 'like', 'a', 'homo', 'i', 'am', 'not', 'going', 'to', 'get', 'sucked', 'into', 'your', 'gay', 'game', 'of', 'name', 'the', 'fed', 'for', 'all', 'i', 'know', 'you', 'are', 'the', 'fed', 'alex', 'jones', 'is', 'definitely', 'some', 'kind', 'of', 'fed', 'or', 'jew', 'loving', 'gatekeeper', 'or', 'crypto']\n",
            "Text after removing stop words: ['fuck', 'data', 'sound', 'like', 'homo', 'going', 'get', 'sucked', 'gay', 'game', 'name', 'fed', 'know', 'fed', 'alex', 'jones', 'definitely', 'kind', 'fed', 'jew', 'loving', 'gatekeeper', 'crypto']\n",
            "Text after Lemmatization: ['fuck', 'data', 'sound', 'like', 'homo', 'going', 'get', 'sucked', 'gay', 'game', 'name', 'fed', 'know', 'fed', 'alex', 'jones', 'definitely', 'kind', 'fed', 'jew', 'loving', 'gatekeeper', 'crypto']\n",
            "Final pre-processed text: fuck data sound like homo going get sucked gay game name fed know fed alex jones definitely kind fed jew loving gatekeeper crypto\n",
            "Text after removing HTML tags: feminists blacks muslims queers illegals libtards hispanics latinos transgenders jews do not like men like me why solely strictly because i am a college educated conservative hetero christian white male who also happens to be a legal immigrant american citizen by the way\n",
            "Text after removing non-alphabetic characters and converting to lowercase: feminists blacks muslims queers illegals libtards hispanics latinos transgenders jews do not like men like me why solely strictly because i am a college educated conservative hetero christian white male who also happens to be a legal immigrant american citizen by the way\n",
            "Text after tokenization: ['feminists', 'blacks', 'muslims', 'queers', 'illegals', 'libtards', 'hispanics', 'latinos', 'transgenders', 'jews', 'do', 'not', 'like', 'men', 'like', 'me', 'why', 'solely', 'strictly', 'because', 'i', 'am', 'a', 'college', 'educated', 'conservative', 'hetero', 'christian', 'white', 'male', 'who', 'also', 'happens', 'to', 'be', 'a', 'legal', 'immigrant', 'american', 'citizen', 'by', 'the', 'way']\n",
            "Text after removing stop words: ['feminists', 'blacks', 'muslims', 'queers', 'illegals', 'libtards', 'hispanics', 'latinos', 'transgenders', 'jews', 'like', 'men', 'like', 'solely', 'strictly', 'college', 'educated', 'conservative', 'hetero', 'christian', 'white', 'male', 'also', 'happens', 'legal', 'immigrant', 'american', 'citizen', 'way']\n",
            "Text after Lemmatization: ['feminist', 'black', 'muslim', 'queer', 'illegals', 'libtards', 'hispanic', 'latino', 'transgenders', 'jew', 'like', 'men', 'like', 'solely', 'strictly', 'college', 'educated', 'conservative', 'hetero', 'christian', 'white', 'male', 'also', 'happens', 'legal', 'immigrant', 'american', 'citizen', 'way']\n",
            "Final pre-processed text: feminist black muslim queer illegals libtards hispanic latino transgenders jew like men like solely strictly college educated conservative hetero christian white male also happens legal immigrant american citizen way\n",
            "Text after removing HTML tags: and it fine n dandy if you wanna get married but that not going to raise up the plummeting marriage rates you will not get them up by calling guys gay or telling them to man up if you want traditional marriage go and make it traditional again\n",
            "Text after removing non-alphabetic characters and converting to lowercase: and it fine n dandy if you wanna get married but that not going to raise up the plummeting marriage rates you will not get them up by calling guys gay or telling them to man up if you want traditional marriage go and make it traditional again\n",
            "Text after tokenization: ['and', 'it', 'fine', 'n', 'dandy', 'if', 'you', 'wan', 'na', 'get', 'married', 'but', 'that', 'not', 'going', 'to', 'raise', 'up', 'the', 'plummeting', 'marriage', 'rates', 'you', 'will', 'not', 'get', 'them', 'up', 'by', 'calling', 'guys', 'gay', 'or', 'telling', 'them', 'to', 'man', 'up', 'if', 'you', 'want', 'traditional', 'marriage', 'go', 'and', 'make', 'it', 'traditional', 'again']\n",
            "Text after removing stop words: ['fine', 'n', 'dandy', 'wan', 'na', 'get', 'married', 'going', 'raise', 'plummeting', 'marriage', 'rates', 'get', 'calling', 'guys', 'gay', 'telling', 'man', 'want', 'traditional', 'marriage', 'go', 'make', 'traditional']\n",
            "Text after Lemmatization: ['fine', 'n', 'dandy', 'wan', 'na', 'get', 'married', 'going', 'raise', 'plummeting', 'marriage', 'rate', 'get', 'calling', 'guy', 'gay', 'telling', 'man', 'want', 'traditional', 'marriage', 'go', 'make', 'traditional']\n",
            "Final pre-processed text: fine n dandy wan na get married going raise plummeting marriage rate get calling guy gay telling man want traditional marriage go make traditional\n",
            "Text after removing HTML tags: too late the sand niggers are breeding like rabbits that how they take over\n",
            "Text after removing non-alphabetic characters and converting to lowercase: too late the sand niggers are breeding like rabbits that how they take over\n",
            "Text after tokenization: ['too', 'late', 'the', 'sand', 'niggers', 'are', 'breeding', 'like', 'rabbits', 'that', 'how', 'they', 'take', 'over']\n",
            "Text after removing stop words: ['late', 'sand', 'niggers', 'breeding', 'like', 'rabbits', 'take']\n",
            "Text after Lemmatization: ['late', 'sand', 'nigger', 'breeding', 'like', 'rabbit', 'take']\n",
            "Final pre-processed text: late sand nigger breeding like rabbit take\n",
            "Text after removing HTML tags: i love those liberaltears the muslim ban and the recognition of jerusalem as israel capital shows arabs antisemitism causing more redpillingofthenormies i fucking love you president trump just do not be a cuck to liberals donald \n",
            "Text after removing non-alphabetic characters and converting to lowercase: i love those liberaltears the muslim ban and the recognition of jerusalem as israel capital shows arabs antisemitism causing more redpillingofthenormies i fucking love you president trump just do not be a cuck to liberals donald \n",
            "Text after tokenization: ['i', 'love', 'those', 'liberaltears', 'the', 'muslim', 'ban', 'and', 'the', 'recognition', 'of', 'jerusalem', 'as', 'israel', 'capital', 'shows', 'arabs', 'antisemitism', 'causing', 'more', 'redpillingofthenormies', 'i', 'fucking', 'love', 'you', 'president', 'trump', 'just', 'do', 'not', 'be', 'a', 'cuck', 'to', 'liberals', 'donald']\n",
            "Text after removing stop words: ['love', 'liberaltears', 'muslim', 'ban', 'recognition', 'jerusalem', 'israel', 'capital', 'shows', 'arabs', 'antisemitism', 'causing', 'redpillingofthenormies', 'fucking', 'love', 'president', 'trump', 'cuck', 'liberals', 'donald']\n",
            "Text after Lemmatization: ['love', 'liberaltears', 'muslim', 'ban', 'recognition', 'jerusalem', 'israel', 'capital', 'show', 'arab', 'antisemitism', 'causing', 'redpillingofthenormies', 'fucking', 'love', 'president', 'trump', 'cuck', 'liberal', 'donald']\n",
            "Final pre-processed text: love liberaltears muslim ban recognition jerusalem israel capital show arab antisemitism causing redpillingofthenormies fucking love president trump cuck liberal donald\n",
            "Text after removing HTML tags: i am a liberal and i have consensual sex with more women than the guys you see in porn\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am a liberal and i have consensual sex with more women than the guys you see in porn\n",
            "Text after tokenization: ['i', 'am', 'a', 'liberal', 'and', 'i', 'have', 'consensual', 'sex', 'with', 'more', 'women', 'than', 'the', 'guys', 'you', 'see', 'in', 'porn']\n",
            "Text after removing stop words: ['liberal', 'consensual', 'sex', 'women', 'guys', 'see', 'porn']\n",
            "Text after Lemmatization: ['liberal', 'consensual', 'sex', 'woman', 'guy', 'see', 'porn']\n",
            "Final pre-processed text: liberal consensual sex woman guy see porn\n",
            "Text after removing HTML tags: good now are there any americans left in minnesota or is it overrun by muzzies who are going to send another jackass to the senate\n",
            "Text after removing non-alphabetic characters and converting to lowercase: good now are there any americans left in minnesota or is it overrun by muzzies who are going to send another jackass to the senate\n",
            "Text after tokenization: ['good', 'now', 'are', 'there', 'any', 'americans', 'left', 'in', 'minnesota', 'or', 'is', 'it', 'overrun', 'by', 'muzzies', 'who', 'are', 'going', 'to', 'send', 'another', 'jackass', 'to', 'the', 'senate']\n",
            "Text after removing stop words: ['good', 'americans', 'left', 'minnesota', 'overrun', 'muzzies', 'going', 'send', 'another', 'jackass', 'senate']\n",
            "Text after Lemmatization: ['good', 'american', 'left', 'minnesota', 'overrun', 'muzzies', 'going', 'send', 'another', 'jackass', 'senate']\n",
            "Final pre-processed text: good american left minnesota overrun muzzies going send another jackass senate\n",
            "Text after removing HTML tags: so more dead jews and arabs why ya bitchin israel is now being forced to buy our military weapons and that means more business and cash for us while we watch both jews and arabs die why u bitchin 🤔 🤔 🤔 🤔 🤔\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so more dead jews and arabs why ya bitchin israel is now being forced to buy our military weapons and that means more business and cash for us while we watch both jews and arabs die why u bitchin          \n",
            "Text after tokenization: ['so', 'more', 'dead', 'jews', 'and', 'arabs', 'why', 'ya', 'bitchin', 'israel', 'is', 'now', 'being', 'forced', 'to', 'buy', 'our', 'military', 'weapons', 'and', 'that', 'means', 'more', 'business', 'and', 'cash', 'for', 'us', 'while', 'we', 'watch', 'both', 'jews', 'and', 'arabs', 'die', 'why', 'u', 'bitchin']\n",
            "Text after removing stop words: ['dead', 'jews', 'arabs', 'ya', 'bitchin', 'israel', 'forced', 'buy', 'military', 'weapons', 'means', 'business', 'cash', 'us', 'watch', 'jews', 'arabs', 'die', 'u', 'bitchin']\n",
            "Text after Lemmatization: ['dead', 'jew', 'arab', 'ya', 'bitchin', 'israel', 'forced', 'buy', 'military', 'weapon', 'mean', 'business', 'cash', 'u', 'watch', 'jew', 'arab', 'die', 'u', 'bitchin']\n",
            "Final pre-processed text: dead jew arab ya bitchin israel forced buy military weapon mean business cash u watch jew arab die u bitchin\n",
            "Text after removing HTML tags: msm tatics if shooter is not white bitch about gun control if white use the nazi angle blame trump and probably roll out gabby giffords and the usual suspects\n",
            "Text after removing non-alphabetic characters and converting to lowercase: msm tatics if shooter is not white bitch about gun control if white use the nazi angle blame trump and probably roll out gabby giffords and the usual suspects\n",
            "Text after tokenization: ['msm', 'tatics', 'if', 'shooter', 'is', 'not', 'white', 'bitch', 'about', 'gun', 'control', 'if', 'white', 'use', 'the', 'nazi', 'angle', 'blame', 'trump', 'and', 'probably', 'roll', 'out', 'gabby', 'giffords', 'and', 'the', 'usual', 'suspects']\n",
            "Text after removing stop words: ['msm', 'tatics', 'shooter', 'white', 'bitch', 'gun', 'control', 'white', 'use', 'nazi', 'angle', 'blame', 'trump', 'probably', 'roll', 'gabby', 'giffords', 'usual', 'suspects']\n",
            "Text after Lemmatization: ['msm', 'tatics', 'shooter', 'white', 'bitch', 'gun', 'control', 'white', 'use', 'nazi', 'angle', 'blame', 'trump', 'probably', 'roll', 'gabby', 'giffords', 'usual', 'suspect']\n",
            "Final pre-processed text: msm tatics shooter white bitch gun control white use nazi angle blame trump probably roll gabby giffords usual suspect\n",
            "Text after removing HTML tags: no aid no contracting to build stuff for them and blockading the east asians from stepping into the breach or giving them the run down on why it better long term to take this approach for both our peoples the populations will collapse due to environmental stressers and perhaps political turmoil\n",
            "Text after removing non-alphabetic characters and converting to lowercase: no aid no contracting to build stuff for them and blockading the east asians from stepping into the breach or giving them the run down on why it better long term to take this approach for both our peoples the populations will collapse due to environmental stressers and perhaps political turmoil\n",
            "Text after tokenization: ['no', 'aid', 'no', 'contracting', 'to', 'build', 'stuff', 'for', 'them', 'and', 'blockading', 'the', 'east', 'asians', 'from', 'stepping', 'into', 'the', 'breach', 'or', 'giving', 'them', 'the', 'run', 'down', 'on', 'why', 'it', 'better', 'long', 'term', 'to', 'take', 'this', 'approach', 'for', 'both', 'our', 'peoples', 'the', 'populations', 'will', 'collapse', 'due', 'to', 'environmental', 'stressers', 'and', 'perhaps', 'political', 'turmoil']\n",
            "Text after removing stop words: ['aid', 'contracting', 'build', 'stuff', 'blockading', 'east', 'asians', 'stepping', 'breach', 'giving', 'run', 'better', 'long', 'term', 'take', 'approach', 'peoples', 'populations', 'collapse', 'due', 'environmental', 'stressers', 'perhaps', 'political', 'turmoil']\n",
            "Text after Lemmatization: ['aid', 'contracting', 'build', 'stuff', 'blockading', 'east', 'asian', 'stepping', 'breach', 'giving', 'run', 'better', 'long', 'term', 'take', 'approach', 'people', 'population', 'collapse', 'due', 'environmental', 'stressers', 'perhaps', 'political', 'turmoil']\n",
            "Final pre-processed text: aid contracting build stuff blockading east asian stepping breach giving run better long term take approach people population collapse due environmental stressers perhaps political turmoil\n",
            "Text after removing HTML tags: i am not the one aborting babies here your liberal white women are doing that\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am not the one aborting babies here your liberal white women are doing that\n",
            "Text after tokenization: ['i', 'am', 'not', 'the', 'one', 'aborting', 'babies', 'here', 'your', 'liberal', 'white', 'women', 'are', 'doing', 'that']\n",
            "Text after removing stop words: ['one', 'aborting', 'babies', 'liberal', 'white', 'women']\n",
            "Text after Lemmatization: ['one', 'aborting', 'baby', 'liberal', 'white', 'woman']\n",
            "Final pre-processed text: one aborting baby liberal white woman\n",
            "Text after removing HTML tags: sadly jews have concentrate their extermination of white people un this kind of countries cause they hate blond purity for them every white girls have to be gangbanged or raped or stupid enough to have brown subhuman sperm in their womb to make the white genocide by race mixing unstoppable\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sadly jews have concentrate their extermination of white people un this kind of countries cause they hate blond purity for them every white girls have to be gangbanged or raped or stupid enough to have brown subhuman sperm in their womb to make the white genocide by race mixing unstoppable\n",
            "Text after tokenization: ['sadly', 'jews', 'have', 'concentrate', 'their', 'extermination', 'of', 'white', 'people', 'un', 'this', 'kind', 'of', 'countries', 'cause', 'they', 'hate', 'blond', 'purity', 'for', 'them', 'every', 'white', 'girls', 'have', 'to', 'be', 'gangbanged', 'or', 'raped', 'or', 'stupid', 'enough', 'to', 'have', 'brown', 'subhuman', 'sperm', 'in', 'their', 'womb', 'to', 'make', 'the', 'white', 'genocide', 'by', 'race', 'mixing', 'unstoppable']\n",
            "Text after removing stop words: ['sadly', 'jews', 'concentrate', 'extermination', 'white', 'people', 'un', 'kind', 'countries', 'cause', 'hate', 'blond', 'purity', 'every', 'white', 'girls', 'gangbanged', 'raped', 'stupid', 'enough', 'brown', 'subhuman', 'sperm', 'womb', 'make', 'white', 'genocide', 'race', 'mixing', 'unstoppable']\n",
            "Text after Lemmatization: ['sadly', 'jew', 'concentrate', 'extermination', 'white', 'people', 'un', 'kind', 'country', 'cause', 'hate', 'blond', 'purity', 'every', 'white', 'girl', 'gangbanged', 'raped', 'stupid', 'enough', 'brown', 'subhuman', 'sperm', 'womb', 'make', 'white', 'genocide', 'race', 'mixing', 'unstoppable']\n",
            "Final pre-processed text: sadly jew concentrate extermination white people un kind country cause hate blond purity every white girl gangbanged raped stupid enough brown subhuman sperm womb make white genocide race mixing unstoppable\n",
            "Text after removing HTML tags: diversity means getting shot by blacks raped by mexicans blown up by arabs and shitty customer service from indians\n",
            "Text after removing non-alphabetic characters and converting to lowercase: diversity means getting shot by blacks raped by mexicans blown up by arabs and shitty customer service from indians\n",
            "Text after tokenization: ['diversity', 'means', 'getting', 'shot', 'by', 'blacks', 'raped', 'by', 'mexicans', 'blown', 'up', 'by', 'arabs', 'and', 'shitty', 'customer', 'service', 'from', 'indians']\n",
            "Text after removing stop words: ['diversity', 'means', 'getting', 'shot', 'blacks', 'raped', 'mexicans', 'blown', 'arabs', 'shitty', 'customer', 'service', 'indians']\n",
            "Text after Lemmatization: ['diversity', 'mean', 'getting', 'shot', 'black', 'raped', 'mexican', 'blown', 'arab', 'shitty', 'customer', 'service', 'indian']\n",
            "Final pre-processed text: diversity mean getting shot black raped mexican blown arab shitty customer service indian\n",
            "Text after removing HTML tags: i am still searching for a peaceful sand nigger muslim islam is the religion of peace they keep telling us maga without muslims\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am still searching for a peaceful sand nigger muslim islam is the religion of peace they keep telling us maga without muslims\n",
            "Text after tokenization: ['i', 'am', 'still', 'searching', 'for', 'a', 'peaceful', 'sand', 'nigger', 'muslim', 'islam', 'is', 'the', 'religion', 'of', 'peace', 'they', 'keep', 'telling', 'us', 'maga', 'without', 'muslims']\n",
            "Text after removing stop words: ['still', 'searching', 'peaceful', 'sand', 'nigger', 'muslim', 'islam', 'religion', 'peace', 'keep', 'telling', 'us', 'maga', 'without', 'muslims']\n",
            "Text after Lemmatization: ['still', 'searching', 'peaceful', 'sand', 'nigger', 'muslim', 'islam', 'religion', 'peace', 'keep', 'telling', 'u', 'maga', 'without', 'muslim']\n",
            "Final pre-processed text: still searching peaceful sand nigger muslim islam religion peace keep telling u maga without muslim\n",
            "Text after removing HTML tags: fucking cheetahs have great athletic ability too who gives a fuck you can not build civilizations with basketballs i bring up asians because they are superior to blacks too niggers know whites are superior and that why they always come to white countries and try to fuck white women\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fucking cheetahs have great athletic ability too who gives a fuck you can not build civilizations with basketballs i bring up asians because they are superior to blacks too niggers know whites are superior and that why they always come to white countries and try to fuck white women\n",
            "Text after tokenization: ['fucking', 'cheetahs', 'have', 'great', 'athletic', 'ability', 'too', 'who', 'gives', 'a', 'fuck', 'you', 'can', 'not', 'build', 'civilizations', 'with', 'basketballs', 'i', 'bring', 'up', 'asians', 'because', 'they', 'are', 'superior', 'to', 'blacks', 'too', 'niggers', 'know', 'whites', 'are', 'superior', 'and', 'that', 'why', 'they', 'always', 'come', 'to', 'white', 'countries', 'and', 'try', 'to', 'fuck', 'white', 'women']\n",
            "Text after removing stop words: ['fucking', 'cheetahs', 'great', 'athletic', 'ability', 'gives', 'fuck', 'build', 'civilizations', 'basketballs', 'bring', 'asians', 'superior', 'blacks', 'niggers', 'know', 'whites', 'superior', 'always', 'come', 'white', 'countries', 'try', 'fuck', 'white', 'women']\n",
            "Text after Lemmatization: ['fucking', 'cheetah', 'great', 'athletic', 'ability', 'give', 'fuck', 'build', 'civilization', 'basketball', 'bring', 'asian', 'superior', 'black', 'nigger', 'know', 'white', 'superior', 'always', 'come', 'white', 'country', 'try', 'fuck', 'white', 'woman']\n",
            "Final pre-processed text: fucking cheetah great athletic ability give fuck build civilization basketball bring asian superior black nigger know white superior always come white country try fuck white woman\n",
            "Text after removing HTML tags: ay i am   italian bucko i can make meatballs and stromboli with the best of em but i am blonde and green eyes because the arabs did not invade my part of italy above rome\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ay i am   italian bucko i can make meatballs and stromboli with the best of em but i am blonde and green eyes because the arabs did not invade my part of italy above rome\n",
            "Text after tokenization: ['ay', 'i', 'am', 'italian', 'bucko', 'i', 'can', 'make', 'meatballs', 'and', 'stromboli', 'with', 'the', 'best', 'of', 'em', 'but', 'i', 'am', 'blonde', 'and', 'green', 'eyes', 'because', 'the', 'arabs', 'did', 'not', 'invade', 'my', 'part', 'of', 'italy', 'above', 'rome']\n",
            "Text after removing stop words: ['ay', 'italian', 'bucko', 'make', 'meatballs', 'stromboli', 'best', 'em', 'blonde', 'green', 'eyes', 'arabs', 'invade', 'part', 'italy', 'rome']\n",
            "Text after Lemmatization: ['ay', 'italian', 'bucko', 'make', 'meatball', 'stromboli', 'best', 'em', 'blonde', 'green', 'eye', 'arab', 'invade', 'part', 'italy', 'rome']\n",
            "Final pre-processed text: ay italian bucko make meatball stromboli best em blonde green eye arab invade part italy rome\n",
            "Text after removing HTML tags: stupid moslem terrorist forgot to do rockets and blew his dingus off that what happens when you make things when you are a stupid head\n",
            "Text after removing non-alphabetic characters and converting to lowercase: stupid moslem terrorist forgot to do rockets and blew his dingus off that what happens when you make things when you are a stupid head\n",
            "Text after tokenization: ['stupid', 'moslem', 'terrorist', 'forgot', 'to', 'do', 'rockets', 'and', 'blew', 'his', 'dingus', 'off', 'that', 'what', 'happens', 'when', 'you', 'make', 'things', 'when', 'you', 'are', 'a', 'stupid', 'head']\n",
            "Text after removing stop words: ['stupid', 'moslem', 'terrorist', 'forgot', 'rockets', 'blew', 'dingus', 'happens', 'make', 'things', 'stupid', 'head']\n",
            "Text after Lemmatization: ['stupid', 'moslem', 'terrorist', 'forgot', 'rocket', 'blew', 'dingus', 'happens', 'make', 'thing', 'stupid', 'head']\n",
            "Final pre-processed text: stupid moslem terrorist forgot rocket blew dingus happens make thing stupid head\n",
            "Text after removing HTML tags: you would not handle weev bantz sandnigger\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you would not handle weev bantz sandnigger\n",
            "Text after tokenization: ['you', 'would', 'not', 'handle', 'weev', 'bantz', 'sandnigger']\n",
            "Text after removing stop words: ['would', 'handle', 'weev', 'bantz', 'sandnigger']\n",
            "Text after Lemmatization: ['would', 'handle', 'weev', 'bantz', 'sandnigger']\n",
            "Final pre-processed text: would handle weev bantz sandnigger\n",
            "Text after removing HTML tags: white pill of the day\n",
            "Text after removing non-alphabetic characters and converting to lowercase: white pill of the day\n",
            "Text after tokenization: ['white', 'pill', 'of', 'the', 'day']\n",
            "Text after removing stop words: ['white', 'pill', 'day']\n",
            "Text after Lemmatization: ['white', 'pill', 'day']\n",
            "Final pre-processed text: white pill day\n",
            "Text after removing HTML tags: and the ghetto precincts still have to report gg rip\n",
            "Text after removing non-alphabetic characters and converting to lowercase: and the ghetto precincts still have to report gg rip\n",
            "Text after tokenization: ['and', 'the', 'ghetto', 'precincts', 'still', 'have', 'to', 'report', 'gg', 'rip']\n",
            "Text after removing stop words: ['ghetto', 'precincts', 'still', 'report', 'gg', 'rip']\n",
            "Text after Lemmatization: ['ghetto', 'precinct', 'still', 'report', 'gg', 'rip']\n",
            "Final pre-processed text: ghetto precinct still report gg rip\n",
            "Text after removing HTML tags: if the pearl clutching faggots in sweet home alabamy fuck this up there goes the whole trump agenda\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if the pearl clutching faggots in sweet home alabamy fuck this up there goes the whole trump agenda\n",
            "Text after tokenization: ['if', 'the', 'pearl', 'clutching', 'faggots', 'in', 'sweet', 'home', 'alabamy', 'fuck', 'this', 'up', 'there', 'goes', 'the', 'whole', 'trump', 'agenda']\n",
            "Text after removing stop words: ['pearl', 'clutching', 'faggots', 'sweet', 'home', 'alabamy', 'fuck', 'goes', 'whole', 'trump', 'agenda']\n",
            "Text after Lemmatization: ['pearl', 'clutching', 'faggot', 'sweet', 'home', 'alabamy', 'fuck', 'go', 'whole', 'trump', 'agenda']\n",
            "Final pre-processed text: pearl clutching faggot sweet home alabamy fuck go whole trump agenda\n",
            "Text after removing HTML tags: why are they destroying the hard drives in the middle of the night before there is even a chance for a recount they keep every record of every retarded little thing but all the votes have to be destroyed in the middle of the night\n",
            "Text after removing non-alphabetic characters and converting to lowercase: why are they destroying the hard drives in the middle of the night before there is even a chance for a recount they keep every record of every retarded little thing but all the votes have to be destroyed in the middle of the night\n",
            "Text after tokenization: ['why', 'are', 'they', 'destroying', 'the', 'hard', 'drives', 'in', 'the', 'middle', 'of', 'the', 'night', 'before', 'there', 'is', 'even', 'a', 'chance', 'for', 'a', 'recount', 'they', 'keep', 'every', 'record', 'of', 'every', 'retarded', 'little', 'thing', 'but', 'all', 'the', 'votes', 'have', 'to', 'be', 'destroyed', 'in', 'the', 'middle', 'of', 'the', 'night']\n",
            "Text after removing stop words: ['destroying', 'hard', 'drives', 'middle', 'night', 'even', 'chance', 'recount', 'keep', 'every', 'record', 'every', 'retarded', 'little', 'thing', 'votes', 'destroyed', 'middle', 'night']\n",
            "Text after Lemmatization: ['destroying', 'hard', 'drive', 'middle', 'night', 'even', 'chance', 'recount', 'keep', 'every', 'record', 'every', 'retarded', 'little', 'thing', 'vote', 'destroyed', 'middle', 'night']\n",
            "Final pre-processed text: destroying hard drive middle night even chance recount keep every record every retarded little thing vote destroyed middle night\n",
            "Text after removing HTML tags: there no such thing as hate speech you inbred goat fucking mudslime piece of nomadic sand nigger trash\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there no such thing as hate speech you inbred goat fucking mudslime piece of nomadic sand nigger trash\n",
            "Text after tokenization: ['there', 'no', 'such', 'thing', 'as', 'hate', 'speech', 'you', 'inbred', 'goat', 'fucking', 'mudslime', 'piece', 'of', 'nomadic', 'sand', 'nigger', 'trash']\n",
            "Text after removing stop words: ['thing', 'hate', 'speech', 'inbred', 'goat', 'fucking', 'mudslime', 'piece', 'nomadic', 'sand', 'nigger', 'trash']\n",
            "Text after Lemmatization: ['thing', 'hate', 'speech', 'inbred', 'goat', 'fucking', 'mudslime', 'piece', 'nomadic', 'sand', 'nigger', 'trash']\n",
            "Final pre-processed text: thing hate speech inbred goat fucking mudslime piece nomadic sand nigger trash\n",
            "Text after removing HTML tags: i swear between  and  it an endless repressed homosexual cacophany of moar black cocks i want moar black cocks an gay porn it really kind of sad guys seriously\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i swear between  and  it an endless repressed homosexual cacophany of moar black cocks i want moar black cocks an gay porn it really kind of sad guys seriously\n",
            "Text after tokenization: ['i', 'swear', 'between', 'and', 'it', 'an', 'endless', 'repressed', 'homosexual', 'cacophany', 'of', 'moar', 'black', 'cocks', 'i', 'want', 'moar', 'black', 'cocks', 'an', 'gay', 'porn', 'it', 'really', 'kind', 'of', 'sad', 'guys', 'seriously']\n",
            "Text after removing stop words: ['swear', 'endless', 'repressed', 'homosexual', 'cacophany', 'moar', 'black', 'cocks', 'want', 'moar', 'black', 'cocks', 'gay', 'porn', 'really', 'kind', 'sad', 'guys', 'seriously']\n",
            "Text after Lemmatization: ['swear', 'endless', 'repressed', 'homosexual', 'cacophany', 'moar', 'black', 'cock', 'want', 'moar', 'black', 'cock', 'gay', 'porn', 'really', 'kind', 'sad', 'guy', 'seriously']\n",
            "Final pre-processed text: swear endless repressed homosexual cacophany moar black cock want moar black cock gay porn really kind sad guy seriously\n",
            "Text after removing HTML tags: what kind of ironic about this is that a lot of deep ghetto blacks have asiatic eyes anyone else notice the same thing\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what kind of ironic about this is that a lot of deep ghetto blacks have asiatic eyes anyone else notice the same thing\n",
            "Text after tokenization: ['what', 'kind', 'of', 'ironic', 'about', 'this', 'is', 'that', 'a', 'lot', 'of', 'deep', 'ghetto', 'blacks', 'have', 'asiatic', 'eyes', 'anyone', 'else', 'notice', 'the', 'same', 'thing']\n",
            "Text after removing stop words: ['kind', 'ironic', 'lot', 'deep', 'ghetto', 'blacks', 'asiatic', 'eyes', 'anyone', 'else', 'notice', 'thing']\n",
            "Text after Lemmatization: ['kind', 'ironic', 'lot', 'deep', 'ghetto', 'black', 'asiatic', 'eye', 'anyone', 'else', 'notice', 'thing']\n",
            "Final pre-processed text: kind ironic lot deep ghetto black asiatic eye anyone else notice thing\n",
            "Text after removing HTML tags: in  we have to set up video outside the ghetto polling places so we can count the number of people entering and leaving it literally would not surprise me if almost nobody actually votes in these precincts and it just  fraud\n",
            "Text after removing non-alphabetic characters and converting to lowercase: in  we have to set up video outside the ghetto polling places so we can count the number of people entering and leaving it literally would not surprise me if almost nobody actually votes in these precincts and it just  fraud\n",
            "Text after tokenization: ['in', 'we', 'have', 'to', 'set', 'up', 'video', 'outside', 'the', 'ghetto', 'polling', 'places', 'so', 'we', 'can', 'count', 'the', 'number', 'of', 'people', 'entering', 'and', 'leaving', 'it', 'literally', 'would', 'not', 'surprise', 'me', 'if', 'almost', 'nobody', 'actually', 'votes', 'in', 'these', 'precincts', 'and', 'it', 'just', 'fraud']\n",
            "Text after removing stop words: ['set', 'video', 'outside', 'ghetto', 'polling', 'places', 'count', 'number', 'people', 'entering', 'leaving', 'literally', 'would', 'surprise', 'almost', 'nobody', 'actually', 'votes', 'precincts', 'fraud']\n",
            "Text after Lemmatization: ['set', 'video', 'outside', 'ghetto', 'polling', 'place', 'count', 'number', 'people', 'entering', 'leaving', 'literally', 'would', 'surprise', 'almost', 'nobody', 'actually', 'vote', 'precinct', 'fraud']\n",
            "Final pre-processed text: set video outside ghetto polling place count number people entering leaving literally would surprise almost nobody actually vote precinct fraud\n",
            "Text after removing HTML tags: the refugee boats are beginning to arrive from twitter now the leftists are cyber bombing virtual communities too so the real land of libya was not enough destruction and chaos for them hope the freehold of gab can handle the influx of refugees gabfam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the refugee boats are beginning to arrive from twitter now the leftists are cyber bombing virtual communities too so the real land of libya was not enough destruction and chaos for them hope the freehold of gab can handle the influx of refugees gabfam\n",
            "Text after tokenization: ['the', 'refugee', 'boats', 'are', 'beginning', 'to', 'arrive', 'from', 'twitter', 'now', 'the', 'leftists', 'are', 'cyber', 'bombing', 'virtual', 'communities', 'too', 'so', 'the', 'real', 'land', 'of', 'libya', 'was', 'not', 'enough', 'destruction', 'and', 'chaos', 'for', 'them', 'hope', 'the', 'freehold', 'of', 'gab', 'can', 'handle', 'the', 'influx', 'of', 'refugees', 'gabfam']\n",
            "Text after removing stop words: ['refugee', 'boats', 'beginning', 'arrive', 'twitter', 'leftists', 'cyber', 'bombing', 'virtual', 'communities', 'real', 'land', 'libya', 'enough', 'destruction', 'chaos', 'hope', 'freehold', 'gab', 'handle', 'influx', 'refugees', 'gabfam']\n",
            "Text after Lemmatization: ['refugee', 'boat', 'beginning', 'arrive', 'twitter', 'leftist', 'cyber', 'bombing', 'virtual', 'community', 'real', 'land', 'libya', 'enough', 'destruction', 'chaos', 'hope', 'freehold', 'gab', 'handle', 'influx', 'refugee', 'gabfam']\n",
            "Final pre-processed text: refugee boat beginning arrive twitter leftist cyber bombing virtual community real land libya enough destruction chaos hope freehold gab handle influx refugee gabfam\n",
            "Text after removing HTML tags: have they put women on subs yet in se asia i have heard many times that the donut dollies and some nurses had thriving businesses\n",
            "Text after removing non-alphabetic characters and converting to lowercase: have they put women on subs yet in se asia i have heard many times that the donut dollies and some nurses had thriving businesses\n",
            "Text after tokenization: ['have', 'they', 'put', 'women', 'on', 'subs', 'yet', 'in', 'se', 'asia', 'i', 'have', 'heard', 'many', 'times', 'that', 'the', 'donut', 'dollies', 'and', 'some', 'nurses', 'had', 'thriving', 'businesses']\n",
            "Text after removing stop words: ['put', 'women', 'subs', 'yet', 'se', 'asia', 'heard', 'many', 'times', 'donut', 'dollies', 'nurses', 'thriving', 'businesses']\n",
            "Text after Lemmatization: ['put', 'woman', 'sub', 'yet', 'se', 'asia', 'heard', 'many', 'time', 'donut', 'dolly', 'nurse', 'thriving', 'business']\n",
            "Final pre-processed text: put woman sub yet se asia heard many time donut dolly nurse thriving business\n",
            "Text after removing HTML tags: porn is just the way the jews are trying to stop whites from making more whites you cant get your hand pregnant and it turns young men into betas\n",
            "Text after removing non-alphabetic characters and converting to lowercase: porn is just the way the jews are trying to stop whites from making more whites you cant get your hand pregnant and it turns young men into betas\n",
            "Text after tokenization: ['porn', 'is', 'just', 'the', 'way', 'the', 'jews', 'are', 'trying', 'to', 'stop', 'whites', 'from', 'making', 'more', 'whites', 'you', 'cant', 'get', 'your', 'hand', 'pregnant', 'and', 'it', 'turns', 'young', 'men', 'into', 'betas']\n",
            "Text after removing stop words: ['porn', 'way', 'jews', 'trying', 'stop', 'whites', 'making', 'whites', 'cant', 'get', 'hand', 'pregnant', 'turns', 'young', 'men', 'betas']\n",
            "Text after Lemmatization: ['porn', 'way', 'jew', 'trying', 'stop', 'white', 'making', 'white', 'cant', 'get', 'hand', 'pregnant', 'turn', 'young', 'men', 'beta']\n",
            "Final pre-processed text: porn way jew trying stop white making white cant get hand pregnant turn young men beta\n",
            "Text after removing HTML tags: that a retarded way to look at it enjoy being a depressive idiot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: that a retarded way to look at it enjoy being a depressive idiot\n",
            "Text after tokenization: ['that', 'a', 'retarded', 'way', 'to', 'look', 'at', 'it', 'enjoy', 'being', 'a', 'depressive', 'idiot']\n",
            "Text after removing stop words: ['retarded', 'way', 'look', 'enjoy', 'depressive', 'idiot']\n",
            "Text after Lemmatization: ['retarded', 'way', 'look', 'enjoy', 'depressive', 'idiot']\n",
            "Final pre-processed text: retarded way look enjoy depressive idiot\n",
            "Text after removing HTML tags: indeed i was called a racist for opposing migration the idiots could not see that there are loads of non racial reasons to oppose unrestrained migration i shut them up by saying i would rather  qualified non white doctors were admitted to the uk than  white unskilled benefit seekers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: indeed i was called a racist for opposing migration the idiots could not see that there are loads of non racial reasons to oppose unrestrained migration i shut them up by saying i would rather  qualified non white doctors were admitted to the uk than  white unskilled benefit seekers\n",
            "Text after tokenization: ['indeed', 'i', 'was', 'called', 'a', 'racist', 'for', 'opposing', 'migration', 'the', 'idiots', 'could', 'not', 'see', 'that', 'there', 'are', 'loads', 'of', 'non', 'racial', 'reasons', 'to', 'oppose', 'unrestrained', 'migration', 'i', 'shut', 'them', 'up', 'by', 'saying', 'i', 'would', 'rather', 'qualified', 'non', 'white', 'doctors', 'were', 'admitted', 'to', 'the', 'uk', 'than', 'white', 'unskilled', 'benefit', 'seekers']\n",
            "Text after removing stop words: ['indeed', 'called', 'racist', 'opposing', 'migration', 'idiots', 'could', 'see', 'loads', 'non', 'racial', 'reasons', 'oppose', 'unrestrained', 'migration', 'shut', 'saying', 'would', 'rather', 'qualified', 'non', 'white', 'doctors', 'admitted', 'uk', 'white', 'unskilled', 'benefit', 'seekers']\n",
            "Text after Lemmatization: ['indeed', 'called', 'racist', 'opposing', 'migration', 'idiot', 'could', 'see', 'load', 'non', 'racial', 'reason', 'oppose', 'unrestrained', 'migration', 'shut', 'saying', 'would', 'rather', 'qualified', 'non', 'white', 'doctor', 'admitted', 'uk', 'white', 'unskilled', 'benefit', 'seeker']\n",
            "Final pre-processed text: indeed called racist opposing migration idiot could see load non racial reason oppose unrestrained migration shut saying would rather qualified non white doctor admitted uk white unskilled benefit seeker\n",
            "Text after removing HTML tags: sweden is why it is the height of stupidity to put women in charge of a country\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sweden is why it is the height of stupidity to put women in charge of a country\n",
            "Text after tokenization: ['sweden', 'is', 'why', 'it', 'is', 'the', 'height', 'of', 'stupidity', 'to', 'put', 'women', 'in', 'charge', 'of', 'a', 'country']\n",
            "Text after removing stop words: ['sweden', 'height', 'stupidity', 'put', 'women', 'charge', 'country']\n",
            "Text after Lemmatization: ['sweden', 'height', 'stupidity', 'put', 'woman', 'charge', 'country']\n",
            "Final pre-processed text: sweden height stupidity put woman charge country\n",
            "Text after removing HTML tags: what does fashbrain mean you just say words without knowing what they mean stupid people are funny keep saying stupid things retarded person you are making me laugh\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what does fashbrain mean you just say words without knowing what they mean stupid people are funny keep saying stupid things retarded person you are making me laugh\n",
            "Text after tokenization: ['what', 'does', 'fashbrain', 'mean', 'you', 'just', 'say', 'words', 'without', 'knowing', 'what', 'they', 'mean', 'stupid', 'people', 'are', 'funny', 'keep', 'saying', 'stupid', 'things', 'retarded', 'person', 'you', 'are', 'making', 'me', 'laugh']\n",
            "Text after removing stop words: ['fashbrain', 'mean', 'say', 'words', 'without', 'knowing', 'mean', 'stupid', 'people', 'funny', 'keep', 'saying', 'stupid', 'things', 'retarded', 'person', 'making', 'laugh']\n",
            "Text after Lemmatization: ['fashbrain', 'mean', 'say', 'word', 'without', 'knowing', 'mean', 'stupid', 'people', 'funny', 'keep', 'saying', 'stupid', 'thing', 'retarded', 'person', 'making', 'laugh']\n",
            "Final pre-processed text: fashbrain mean say word without knowing mean stupid people funny keep saying stupid thing retarded person making laugh\n",
            "Text after removing HTML tags: the democrats are in an uproar the president takes the unaffordable burden of the affordable care act off the backs of americans and the fucking democrats say people are going to be uninsured you mean like they were before the government stole their money to pay for illegal immigrant healthcare\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the democrats are in an uproar the president takes the unaffordable burden of the affordable care act off the backs of americans and the fucking democrats say people are going to be uninsured you mean like they were before the government stole their money to pay for illegal immigrant healthcare\n",
            "Text after tokenization: ['the', 'democrats', 'are', 'in', 'an', 'uproar', 'the', 'president', 'takes', 'the', 'unaffordable', 'burden', 'of', 'the', 'affordable', 'care', 'act', 'off', 'the', 'backs', 'of', 'americans', 'and', 'the', 'fucking', 'democrats', 'say', 'people', 'are', 'going', 'to', 'be', 'uninsured', 'you', 'mean', 'like', 'they', 'were', 'before', 'the', 'government', 'stole', 'their', 'money', 'to', 'pay', 'for', 'illegal', 'immigrant', 'healthcare']\n",
            "Text after removing stop words: ['democrats', 'uproar', 'president', 'takes', 'unaffordable', 'burden', 'affordable', 'care', 'act', 'backs', 'americans', 'fucking', 'democrats', 'say', 'people', 'going', 'uninsured', 'mean', 'like', 'government', 'stole', 'money', 'pay', 'illegal', 'immigrant', 'healthcare']\n",
            "Text after Lemmatization: ['democrat', 'uproar', 'president', 'take', 'unaffordable', 'burden', 'affordable', 'care', 'act', 'back', 'american', 'fucking', 'democrat', 'say', 'people', 'going', 'uninsured', 'mean', 'like', 'government', 'stole', 'money', 'pay', 'illegal', 'immigrant', 'healthcare']\n",
            "Final pre-processed text: democrat uproar president take unaffordable burden affordable care act back american fucking democrat say people going uninsured mean like government stole money pay illegal immigrant healthcare\n",
            "Text after removing HTML tags: good luck collecting mate the arabs do not play the guilt game the way we whites do\n",
            "Text after removing non-alphabetic characters and converting to lowercase: good luck collecting mate the arabs do not play the guilt game the way we whites do\n",
            "Text after tokenization: ['good', 'luck', 'collecting', 'mate', 'the', 'arabs', 'do', 'not', 'play', 'the', 'guilt', 'game', 'the', 'way', 'we', 'whites', 'do']\n",
            "Text after removing stop words: ['good', 'luck', 'collecting', 'mate', 'arabs', 'play', 'guilt', 'game', 'way', 'whites']\n",
            "Text after Lemmatization: ['good', 'luck', 'collecting', 'mate', 'arab', 'play', 'guilt', 'game', 'way', 'white']\n",
            "Final pre-processed text: good luck collecting mate arab play guilt game way white\n",
            "Text after removing HTML tags: take this you evil nazi fool does the roman salute and doesnt know we jews invented hitler so that conservatives wouldnt listen to patriots like him nazism is just another form of mind control to instigate division among whites prevent them from joining forces as a tribe\n",
            "Text after removing non-alphabetic characters and converting to lowercase: take this you evil nazi fool does the roman salute and doesnt know we jews invented hitler so that conservatives wouldnt listen to patriots like him nazism is just another form of mind control to instigate division among whites prevent them from joining forces as a tribe\n",
            "Text after tokenization: ['take', 'this', 'you', 'evil', 'nazi', 'fool', 'does', 'the', 'roman', 'salute', 'and', 'doesnt', 'know', 'we', 'jews', 'invented', 'hitler', 'so', 'that', 'conservatives', 'wouldnt', 'listen', 'to', 'patriots', 'like', 'him', 'nazism', 'is', 'just', 'another', 'form', 'of', 'mind', 'control', 'to', 'instigate', 'division', 'among', 'whites', 'prevent', 'them', 'from', 'joining', 'forces', 'as', 'a', 'tribe']\n",
            "Text after removing stop words: ['take', 'evil', 'nazi', 'fool', 'roman', 'salute', 'doesnt', 'know', 'jews', 'invented', 'hitler', 'conservatives', 'wouldnt', 'listen', 'patriots', 'like', 'nazism', 'another', 'form', 'mind', 'control', 'instigate', 'division', 'among', 'whites', 'prevent', 'joining', 'forces', 'tribe']\n",
            "Text after Lemmatization: ['take', 'evil', 'nazi', 'fool', 'roman', 'salute', 'doesnt', 'know', 'jew', 'invented', 'hitler', 'conservative', 'wouldnt', 'listen', 'patriot', 'like', 'nazism', 'another', 'form', 'mind', 'control', 'instigate', 'division', 'among', 'white', 'prevent', 'joining', 'force', 'tribe']\n",
            "Final pre-processed text: take evil nazi fool roman salute doesnt know jew invented hitler conservative wouldnt listen patriot like nazism another form mind control instigate division among white prevent joining force tribe\n",
            "Text after removing HTML tags: there should be a law that anyone mentioning the hatch act must be bitch slapped\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there should be a law that anyone mentioning the hatch act must be bitch slapped\n",
            "Text after tokenization: ['there', 'should', 'be', 'a', 'law', 'that', 'anyone', 'mentioning', 'the', 'hatch', 'act', 'must', 'be', 'bitch', 'slapped']\n",
            "Text after removing stop words: ['law', 'anyone', 'mentioning', 'hatch', 'act', 'must', 'bitch', 'slapped']\n",
            "Text after Lemmatization: ['law', 'anyone', 'mentioning', 'hatch', 'act', 'must', 'bitch', 'slapped']\n",
            "Final pre-processed text: law anyone mentioning hatch act must bitch slapped\n",
            "Text after removing HTML tags: millenials are embracing the poverty that their retarded liberal predecessors have created\n",
            "Text after removing non-alphabetic characters and converting to lowercase: millenials are embracing the poverty that their retarded liberal predecessors have created\n",
            "Text after tokenization: ['millenials', 'are', 'embracing', 'the', 'poverty', 'that', 'their', 'retarded', 'liberal', 'predecessors', 'have', 'created']\n",
            "Text after removing stop words: ['millenials', 'embracing', 'poverty', 'retarded', 'liberal', 'predecessors', 'created']\n",
            "Text after Lemmatization: ['millenials', 'embracing', 'poverty', 'retarded', 'liberal', 'predecessor', 'created']\n",
            "Final pre-processed text: millenials embracing poverty retarded liberal predecessor created\n",
            "Text after removing HTML tags: maybe not unless of course you use the proper term instead of muslim refugee it may be more clear if you use street shitting sand niggers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: maybe not unless of course you use the proper term instead of muslim refugee it may be more clear if you use street shitting sand niggers\n",
            "Text after tokenization: ['maybe', 'not', 'unless', 'of', 'course', 'you', 'use', 'the', 'proper', 'term', 'instead', 'of', 'muslim', 'refugee', 'it', 'may', 'be', 'more', 'clear', 'if', 'you', 'use', 'street', 'shitting', 'sand', 'niggers']\n",
            "Text after removing stop words: ['maybe', 'unless', 'course', 'use', 'proper', 'term', 'instead', 'muslim', 'refugee', 'may', 'clear', 'use', 'street', 'shitting', 'sand', 'niggers']\n",
            "Text after Lemmatization: ['maybe', 'unless', 'course', 'use', 'proper', 'term', 'instead', 'muslim', 'refugee', 'may', 'clear', 'use', 'street', 'shitting', 'sand', 'nigger']\n",
            "Final pre-processed text: maybe unless course use proper term instead muslim refugee may clear use street shitting sand nigger\n",
            "Text after removing HTML tags: ok jews there are no gay blacks old enough to have white hair\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ok jews there are no gay blacks old enough to have white hair\n",
            "Text after tokenization: ['ok', 'jews', 'there', 'are', 'no', 'gay', 'blacks', 'old', 'enough', 'to', 'have', 'white', 'hair']\n",
            "Text after removing stop words: ['ok', 'jews', 'gay', 'blacks', 'old', 'enough', 'white', 'hair']\n",
            "Text after Lemmatization: ['ok', 'jew', 'gay', 'black', 'old', 'enough', 'white', 'hair']\n",
            "Final pre-processed text: ok jew gay black old enough white hair\n",
            "Text after removing HTML tags: obama allowed the cia a free hand in syria resulting in the absolute jihadi chaos which benefited the worthless kikes this is how we ended up backing moderate kebab that behead  year old christian kids trump ended this and allowed russia to massacre the sandniggers you are fucking retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase: obama allowed the cia a free hand in syria resulting in the absolute jihadi chaos which benefited the worthless kikes this is how we ended up backing moderate kebab that behead  year old christian kids trump ended this and allowed russia to massacre the sandniggers you are fucking retarded\n",
            "Text after tokenization: ['obama', 'allowed', 'the', 'cia', 'a', 'free', 'hand', 'in', 'syria', 'resulting', 'in', 'the', 'absolute', 'jihadi', 'chaos', 'which', 'benefited', 'the', 'worthless', 'kikes', 'this', 'is', 'how', 'we', 'ended', 'up', 'backing', 'moderate', 'kebab', 'that', 'behead', 'year', 'old', 'christian', 'kids', 'trump', 'ended', 'this', 'and', 'allowed', 'russia', 'to', 'massacre', 'the', 'sandniggers', 'you', 'are', 'fucking', 'retarded']\n",
            "Text after removing stop words: ['obama', 'allowed', 'cia', 'free', 'hand', 'syria', 'resulting', 'absolute', 'jihadi', 'chaos', 'benefited', 'worthless', 'kikes', 'ended', 'backing', 'moderate', 'kebab', 'behead', 'year', 'old', 'christian', 'kids', 'trump', 'ended', 'allowed', 'russia', 'massacre', 'sandniggers', 'fucking', 'retarded']\n",
            "Text after Lemmatization: ['obama', 'allowed', 'cia', 'free', 'hand', 'syria', 'resulting', 'absolute', 'jihadi', 'chaos', 'benefited', 'worthless', 'kike', 'ended', 'backing', 'moderate', 'kebab', 'behead', 'year', 'old', 'christian', 'kid', 'trump', 'ended', 'allowed', 'russia', 'massacre', 'sandniggers', 'fucking', 'retarded']\n",
            "Final pre-processed text: obama allowed cia free hand syria resulting absolute jihadi chaos benefited worthless kike ended backing moderate kebab behead year old christian kid trump ended allowed russia massacre sandniggers fucking retarded\n",
            "Text after removing HTML tags: the jew programming is not silent on the matter though is it according to them white people keep money and jobs out of the hands of blacks who are forced to become dindus and chimp out with retard level violence if only we gave them reparations they would be superintelligent space explorers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the jew programming is not silent on the matter though is it according to them white people keep money and jobs out of the hands of blacks who are forced to become dindus and chimp out with retard level violence if only we gave them reparations they would be superintelligent space explorers\n",
            "Text after tokenization: ['the', 'jew', 'programming', 'is', 'not', 'silent', 'on', 'the', 'matter', 'though', 'is', 'it', 'according', 'to', 'them', 'white', 'people', 'keep', 'money', 'and', 'jobs', 'out', 'of', 'the', 'hands', 'of', 'blacks', 'who', 'are', 'forced', 'to', 'become', 'dindus', 'and', 'chimp', 'out', 'with', 'retard', 'level', 'violence', 'if', 'only', 'we', 'gave', 'them', 'reparations', 'they', 'would', 'be', 'superintelligent', 'space', 'explorers']\n",
            "Text after removing stop words: ['jew', 'programming', 'silent', 'matter', 'though', 'according', 'white', 'people', 'keep', 'money', 'jobs', 'hands', 'blacks', 'forced', 'become', 'dindus', 'chimp', 'retard', 'level', 'violence', 'gave', 'reparations', 'would', 'superintelligent', 'space', 'explorers']\n",
            "Text after Lemmatization: ['jew', 'programming', 'silent', 'matter', 'though', 'according', 'white', 'people', 'keep', 'money', 'job', 'hand', 'black', 'forced', 'become', 'dindus', 'chimp', 'retard', 'level', 'violence', 'gave', 'reparation', 'would', 'superintelligent', 'space', 'explorer']\n",
            "Final pre-processed text: jew programming silent matter though according white people keep money job hand black forced become dindus chimp retard level violence gave reparation would superintelligent space explorer\n",
            "Text after removing HTML tags: yet when a nigger was made good on the north side his fellow simians literally camped out on the lawn of the 4 th precinct for weeks\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yet when a nigger was made good on the north side his fellow simians literally camped out on the lawn of the   th precinct for weeks\n",
            "Text after tokenization: ['yet', 'when', 'a', 'nigger', 'was', 'made', 'good', 'on', 'the', 'north', 'side', 'his', 'fellow', 'simians', 'literally', 'camped', 'out', 'on', 'the', 'lawn', 'of', 'the', 'th', 'precinct', 'for', 'weeks']\n",
            "Text after removing stop words: ['yet', 'nigger', 'made', 'good', 'north', 'side', 'fellow', 'simians', 'literally', 'camped', 'lawn', 'th', 'precinct', 'weeks']\n",
            "Text after Lemmatization: ['yet', 'nigger', 'made', 'good', 'north', 'side', 'fellow', 'simian', 'literally', 'camped', 'lawn', 'th', 'precinct', 'week']\n",
            "Final pre-processed text: yet nigger made good north side fellow simian literally camped lawn th precinct week\n",
            "Text after removing HTML tags: let the crusades against rag heads continue kill all the women beating child raping goat fucking sand niggers take no prisoners\n",
            "Text after removing non-alphabetic characters and converting to lowercase: let the crusades against rag heads continue kill all the women beating child raping goat fucking sand niggers take no prisoners\n",
            "Text after tokenization: ['let', 'the', 'crusades', 'against', 'rag', 'heads', 'continue', 'kill', 'all', 'the', 'women', 'beating', 'child', 'raping', 'goat', 'fucking', 'sand', 'niggers', 'take', 'no', 'prisoners']\n",
            "Text after removing stop words: ['let', 'crusades', 'rag', 'heads', 'continue', 'kill', 'women', 'beating', 'child', 'raping', 'goat', 'fucking', 'sand', 'niggers', 'take', 'prisoners']\n",
            "Text after Lemmatization: ['let', 'crusade', 'rag', 'head', 'continue', 'kill', 'woman', 'beating', 'child', 'raping', 'goat', 'fucking', 'sand', 'nigger', 'take', 'prisoner']\n",
            "Final pre-processed text: let crusade rag head continue kill woman beating child raping goat fucking sand nigger take prisoner\n",
            "Text after removing HTML tags: for all you white people time to wake the fk up white people freed the blacks and gave them a future in the  the democrats stepped in with pres johnson and bought them back into slavery with welfare by destroying the family no father needed and urban ghettos were born\n",
            "Text after removing non-alphabetic characters and converting to lowercase: for all you white people time to wake the fk up white people freed the blacks and gave them a future in the  the democrats stepped in with pres johnson and bought them back into slavery with welfare by destroying the family no father needed and urban ghettos were born\n",
            "Text after tokenization: ['for', 'all', 'you', 'white', 'people', 'time', 'to', 'wake', 'the', 'fk', 'up', 'white', 'people', 'freed', 'the', 'blacks', 'and', 'gave', 'them', 'a', 'future', 'in', 'the', 'the', 'democrats', 'stepped', 'in', 'with', 'pres', 'johnson', 'and', 'bought', 'them', 'back', 'into', 'slavery', 'with', 'welfare', 'by', 'destroying', 'the', 'family', 'no', 'father', 'needed', 'and', 'urban', 'ghettos', 'were', 'born']\n",
            "Text after removing stop words: ['white', 'people', 'time', 'wake', 'fk', 'white', 'people', 'freed', 'blacks', 'gave', 'future', 'democrats', 'stepped', 'pres', 'johnson', 'bought', 'back', 'slavery', 'welfare', 'destroying', 'family', 'father', 'needed', 'urban', 'ghettos', 'born']\n",
            "Text after Lemmatization: ['white', 'people', 'time', 'wake', 'fk', 'white', 'people', 'freed', 'black', 'gave', 'future', 'democrat', 'stepped', 'pres', 'johnson', 'bought', 'back', 'slavery', 'welfare', 'destroying', 'family', 'father', 'needed', 'urban', 'ghetto', 'born']\n",
            "Final pre-processed text: white people time wake fk white people freed black gave future democrat stepped pres johnson bought back slavery welfare destroying family father needed urban ghetto born\n",
            "Text after removing HTML tags: pretty good  million citizens and most of us have guns i have my collectors licence and go shooting almost every weekend you know why having a license to get a gun is good because niggers can not get them hows that nigger problem going\n",
            "Text after removing non-alphabetic characters and converting to lowercase: pretty good  million citizens and most of us have guns i have my collectors licence and go shooting almost every weekend you know why having a license to get a gun is good because niggers can not get them hows that nigger problem going\n",
            "Text after tokenization: ['pretty', 'good', 'million', 'citizens', 'and', 'most', 'of', 'us', 'have', 'guns', 'i', 'have', 'my', 'collectors', 'licence', 'and', 'go', 'shooting', 'almost', 'every', 'weekend', 'you', 'know', 'why', 'having', 'a', 'license', 'to', 'get', 'a', 'gun', 'is', 'good', 'because', 'niggers', 'can', 'not', 'get', 'them', 'hows', 'that', 'nigger', 'problem', 'going']\n",
            "Text after removing stop words: ['pretty', 'good', 'million', 'citizens', 'us', 'guns', 'collectors', 'licence', 'go', 'shooting', 'almost', 'every', 'weekend', 'know', 'license', 'get', 'gun', 'good', 'niggers', 'get', 'hows', 'nigger', 'problem', 'going']\n",
            "Text after Lemmatization: ['pretty', 'good', 'million', 'citizen', 'u', 'gun', 'collector', 'licence', 'go', 'shooting', 'almost', 'every', 'weekend', 'know', 'license', 'get', 'gun', 'good', 'nigger', 'get', 'hows', 'nigger', 'problem', 'going']\n",
            "Final pre-processed text: pretty good million citizen u gun collector licence go shooting almost every weekend know license get gun good nigger get hows nigger problem going\n",
            "Text after removing HTML tags: does not islam in western countries use a similar process\n",
            "Text after removing non-alphabetic characters and converting to lowercase: does not islam in western countries use a similar process\n",
            "Text after tokenization: ['does', 'not', 'islam', 'in', 'western', 'countries', 'use', 'a', 'similar', 'process']\n",
            "Text after removing stop words: ['islam', 'western', 'countries', 'use', 'similar', 'process']\n",
            "Text after Lemmatization: ['islam', 'western', 'country', 'use', 'similar', 'process']\n",
            "Final pre-processed text: islam western country use similar process\n",
            "Text after removing HTML tags: jews dominate the donor class both republican and democrat why fixate on the jewish question you will never solve as a tide of hispanics blacks breed you into the third world\n",
            "Text after removing non-alphabetic characters and converting to lowercase: jews dominate the donor class both republican and democrat why fixate on the jewish question you will never solve as a tide of hispanics blacks breed you into the third world\n",
            "Text after tokenization: ['jews', 'dominate', 'the', 'donor', 'class', 'both', 'republican', 'and', 'democrat', 'why', 'fixate', 'on', 'the', 'jewish', 'question', 'you', 'will', 'never', 'solve', 'as', 'a', 'tide', 'of', 'hispanics', 'blacks', 'breed', 'you', 'into', 'the', 'third', 'world']\n",
            "Text after removing stop words: ['jews', 'dominate', 'donor', 'class', 'republican', 'democrat', 'fixate', 'jewish', 'question', 'never', 'solve', 'tide', 'hispanics', 'blacks', 'breed', 'third', 'world']\n",
            "Text after Lemmatization: ['jew', 'dominate', 'donor', 'class', 'republican', 'democrat', 'fixate', 'jewish', 'question', 'never', 'solve', 'tide', 'hispanic', 'black', 'breed', 'third', 'world']\n",
            "Final pre-processed text: jew dominate donor class republican democrat fixate jewish question never solve tide hispanic black breed third world\n",
            "Text after removing HTML tags: twatter is dominated by the regressive left who deal in emotion not logic multiculturalism is a cancer that has brought down civilizations throughout history on gab it okay to be white and okay to speak freely\n",
            "Text after removing non-alphabetic characters and converting to lowercase: twatter is dominated by the regressive left who deal in emotion not logic multiculturalism is a cancer that has brought down civilizations throughout history on gab it okay to be white and okay to speak freely\n",
            "Text after tokenization: ['twatter', 'is', 'dominated', 'by', 'the', 'regressive', 'left', 'who', 'deal', 'in', 'emotion', 'not', 'logic', 'multiculturalism', 'is', 'a', 'cancer', 'that', 'has', 'brought', 'down', 'civilizations', 'throughout', 'history', 'on', 'gab', 'it', 'okay', 'to', 'be', 'white', 'and', 'okay', 'to', 'speak', 'freely']\n",
            "Text after removing stop words: ['twatter', 'dominated', 'regressive', 'left', 'deal', 'emotion', 'logic', 'multiculturalism', 'cancer', 'brought', 'civilizations', 'throughout', 'history', 'gab', 'okay', 'white', 'okay', 'speak', 'freely']\n",
            "Text after Lemmatization: ['twatter', 'dominated', 'regressive', 'left', 'deal', 'emotion', 'logic', 'multiculturalism', 'cancer', 'brought', 'civilization', 'throughout', 'history', 'gab', 'okay', 'white', 'okay', 'speak', 'freely']\n",
            "Final pre-processed text: twatter dominated regressive left deal emotion logic multiculturalism cancer brought civilization throughout history gab okay white okay speak freely\n",
            "Text after removing HTML tags: who the fuck believes this retarded half nigger\n",
            "Text after removing non-alphabetic characters and converting to lowercase: who the fuck believes this retarded half nigger\n",
            "Text after tokenization: ['who', 'the', 'fuck', 'believes', 'this', 'retarded', 'half', 'nigger']\n",
            "Text after removing stop words: ['fuck', 'believes', 'retarded', 'half', 'nigger']\n",
            "Text after Lemmatization: ['fuck', 'belief', 'retarded', 'half', 'nigger']\n",
            "Final pre-processed text: fuck belief retarded half nigger\n",
            "Text after removing HTML tags: europe imports savages gets ghettos and crime and is surprised wherever they infest white nations this happens it proved by simple observation and experience and no where do they support the system or pay into it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: europe imports savages gets ghettos and crime and is surprised wherever they infest white nations this happens it proved by simple observation and experience and no where do they support the system or pay into it\n",
            "Text after tokenization: ['europe', 'imports', 'savages', 'gets', 'ghettos', 'and', 'crime', 'and', 'is', 'surprised', 'wherever', 'they', 'infest', 'white', 'nations', 'this', 'happens', 'it', 'proved', 'by', 'simple', 'observation', 'and', 'experience', 'and', 'no', 'where', 'do', 'they', 'support', 'the', 'system', 'or', 'pay', 'into', 'it']\n",
            "Text after removing stop words: ['europe', 'imports', 'savages', 'gets', 'ghettos', 'crime', 'surprised', 'wherever', 'infest', 'white', 'nations', 'happens', 'proved', 'simple', 'observation', 'experience', 'support', 'system', 'pay']\n",
            "Text after Lemmatization: ['europe', 'import', 'savage', 'get', 'ghetto', 'crime', 'surprised', 'wherever', 'infest', 'white', 'nation', 'happens', 'proved', 'simple', 'observation', 'experience', 'support', 'system', 'pay']\n",
            "Final pre-processed text: europe import savage get ghetto crime surprised wherever infest white nation happens proved simple observation experience support system pay\n",
            "Text after removing HTML tags: the warsaw ghetto uprising was the jewish resistance that arose within the warsaw ghetto in german occupied poland during world war ii and which opposed nazi germany effort to transport the remaining ghetto population to treblinka extermination camp\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the warsaw ghetto uprising was the jewish resistance that arose within the warsaw ghetto in german occupied poland during world war ii and which opposed nazi germany effort to transport the remaining ghetto population to treblinka extermination camp\n",
            "Text after tokenization: ['the', 'warsaw', 'ghetto', 'uprising', 'was', 'the', 'jewish', 'resistance', 'that', 'arose', 'within', 'the', 'warsaw', 'ghetto', 'in', 'german', 'occupied', 'poland', 'during', 'world', 'war', 'ii', 'and', 'which', 'opposed', 'nazi', 'germany', 'effort', 'to', 'transport', 'the', 'remaining', 'ghetto', 'population', 'to', 'treblinka', 'extermination', 'camp']\n",
            "Text after removing stop words: ['warsaw', 'ghetto', 'uprising', 'jewish', 'resistance', 'arose', 'within', 'warsaw', 'ghetto', 'german', 'occupied', 'poland', 'world', 'war', 'ii', 'opposed', 'nazi', 'germany', 'effort', 'transport', 'remaining', 'ghetto', 'population', 'treblinka', 'extermination', 'camp']\n",
            "Text after Lemmatization: ['warsaw', 'ghetto', 'uprising', 'jewish', 'resistance', 'arose', 'within', 'warsaw', 'ghetto', 'german', 'occupied', 'poland', 'world', 'war', 'ii', 'opposed', 'nazi', 'germany', 'effort', 'transport', 'remaining', 'ghetto', 'population', 'treblinka', 'extermination', 'camp']\n",
            "Final pre-processed text: warsaw ghetto uprising jewish resistance arose within warsaw ghetto german occupied poland world war ii opposed nazi germany effort transport remaining ghetto population treblinka extermination camp\n",
            "Text after removing HTML tags: sickened by the degenerative behaviour it brings the promotion the perversion and desire to push it in the faces of heterosexuals and children and the fact that they are now trying to turn children into faggots sickens me\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sickened by the degenerative behaviour it brings the promotion the perversion and desire to push it in the faces of heterosexuals and children and the fact that they are now trying to turn children into faggots sickens me\n",
            "Text after tokenization: ['sickened', 'by', 'the', 'degenerative', 'behaviour', 'it', 'brings', 'the', 'promotion', 'the', 'perversion', 'and', 'desire', 'to', 'push', 'it', 'in', 'the', 'faces', 'of', 'heterosexuals', 'and', 'children', 'and', 'the', 'fact', 'that', 'they', 'are', 'now', 'trying', 'to', 'turn', 'children', 'into', 'faggots', 'sickens', 'me']\n",
            "Text after removing stop words: ['sickened', 'degenerative', 'behaviour', 'brings', 'promotion', 'perversion', 'desire', 'push', 'faces', 'heterosexuals', 'children', 'fact', 'trying', 'turn', 'children', 'faggots', 'sickens']\n",
            "Text after Lemmatization: ['sickened', 'degenerative', 'behaviour', 'brings', 'promotion', 'perversion', 'desire', 'push', 'face', 'heterosexual', 'child', 'fact', 'trying', 'turn', 'child', 'faggot', 'sickens']\n",
            "Final pre-processed text: sickened degenerative behaviour brings promotion perversion desire push face heterosexual child fact trying turn child faggot sickens\n",
            "Text after removing HTML tags: the jews want to use the muslims as an army to wipeout us and then they will wipe out most of the muslims\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the jews want to use the muslims as an army to wipeout us and then they will wipe out most of the muslims\n",
            "Text after tokenization: ['the', 'jews', 'want', 'to', 'use', 'the', 'muslims', 'as', 'an', 'army', 'to', 'wipeout', 'us', 'and', 'then', 'they', 'will', 'wipe', 'out', 'most', 'of', 'the', 'muslims']\n",
            "Text after removing stop words: ['jews', 'want', 'use', 'muslims', 'army', 'wipeout', 'us', 'wipe', 'muslims']\n",
            "Text after Lemmatization: ['jew', 'want', 'use', 'muslim', 'army', 'wipeout', 'u', 'wipe', 'muslim']\n",
            "Final pre-processed text: jew want use muslim army wipeout u wipe muslim\n",
            "Text after removing HTML tags: i want  walls all non white immigration stopped  million beaners deported blacks repatriated asians monitored and muslims removed the jews get the oven mix breeds sterilized fuck just get rid of the asians too\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i want  walls all non white immigration stopped  million beaners deported blacks repatriated asians monitored and muslims removed the jews get the oven mix breeds sterilized fuck just get rid of the asians too\n",
            "Text after tokenization: ['i', 'want', 'walls', 'all', 'non', 'white', 'immigration', 'stopped', 'million', 'beaners', 'deported', 'blacks', 'repatriated', 'asians', 'monitored', 'and', 'muslims', 'removed', 'the', 'jews', 'get', 'the', 'oven', 'mix', 'breeds', 'sterilized', 'fuck', 'just', 'get', 'rid', 'of', 'the', 'asians', 'too']\n",
            "Text after removing stop words: ['want', 'walls', 'non', 'white', 'immigration', 'stopped', 'million', 'beaners', 'deported', 'blacks', 'repatriated', 'asians', 'monitored', 'muslims', 'removed', 'jews', 'get', 'oven', 'mix', 'breeds', 'sterilized', 'fuck', 'get', 'rid', 'asians']\n",
            "Text after Lemmatization: ['want', 'wall', 'non', 'white', 'immigration', 'stopped', 'million', 'beaner', 'deported', 'black', 'repatriated', 'asian', 'monitored', 'muslim', 'removed', 'jew', 'get', 'oven', 'mix', 'breed', 'sterilized', 'fuck', 'get', 'rid', 'asian']\n",
            "Final pre-processed text: want wall non white immigration stopped million beaner deported black repatriated asian monitored muslim removed jew get oven mix breed sterilized fuck get rid asian\n",
            "Text after removing HTML tags: i am very happy that wtp have a voice in the wh now muslim refugees w no skills will be welfare babies forever and a day we are  trillion in debt by the gop bush family and another  trillion by the dem obama family  the other\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am very happy that wtp have a voice in the wh now muslim refugees w no skills will be welfare babies forever and a day we are  trillion in debt by the gop bush family and another  trillion by the dem obama family  the other\n",
            "Text after tokenization: ['i', 'am', 'very', 'happy', 'that', 'wtp', 'have', 'a', 'voice', 'in', 'the', 'wh', 'now', 'muslim', 'refugees', 'w', 'no', 'skills', 'will', 'be', 'welfare', 'babies', 'forever', 'and', 'a', 'day', 'we', 'are', 'trillion', 'in', 'debt', 'by', 'the', 'gop', 'bush', 'family', 'and', 'another', 'trillion', 'by', 'the', 'dem', 'obama', 'family', 'the', 'other']\n",
            "Text after removing stop words: ['happy', 'wtp', 'voice', 'wh', 'muslim', 'refugees', 'w', 'skills', 'welfare', 'babies', 'forever', 'day', 'trillion', 'debt', 'gop', 'bush', 'family', 'another', 'trillion', 'dem', 'obama', 'family']\n",
            "Text after Lemmatization: ['happy', 'wtp', 'voice', 'wh', 'muslim', 'refugee', 'w', 'skill', 'welfare', 'baby', 'forever', 'day', 'trillion', 'debt', 'gop', 'bush', 'family', 'another', 'trillion', 'dem', 'obama', 'family']\n",
            "Final pre-processed text: happy wtp voice wh muslim refugee w skill welfare baby forever day trillion debt gop bush family another trillion dem obama family\n",
            "Text after removing HTML tags: well if they arent rabid liberal leftists maybe a strong privately done vetting process needs to be conducted dont wanna have potential arabs comin out\n",
            "Text after removing non-alphabetic characters and converting to lowercase: well if they arent rabid liberal leftists maybe a strong privately done vetting process needs to be conducted dont wanna have potential arabs comin out\n",
            "Text after tokenization: ['well', 'if', 'they', 'arent', 'rabid', 'liberal', 'leftists', 'maybe', 'a', 'strong', 'privately', 'done', 'vetting', 'process', 'needs', 'to', 'be', 'conducted', 'dont', 'wan', 'na', 'have', 'potential', 'arabs', 'comin', 'out']\n",
            "Text after removing stop words: ['well', 'arent', 'rabid', 'liberal', 'leftists', 'maybe', 'strong', 'privately', 'done', 'vetting', 'process', 'needs', 'conducted', 'dont', 'wan', 'na', 'potential', 'arabs', 'comin']\n",
            "Text after Lemmatization: ['well', 'arent', 'rabid', 'liberal', 'leftist', 'maybe', 'strong', 'privately', 'done', 'vetting', 'process', 'need', 'conducted', 'dont', 'wan', 'na', 'potential', 'arab', 'comin']\n",
            "Final pre-processed text: well arent rabid liberal leftist maybe strong privately done vetting process need conducted dont wan na potential arab comin\n",
            "Text after removing HTML tags: as the catholic church today meddles in usa politics by trying to bring in mexican immigrants and shielding illegal alien mexicans the same catholic church worked hard to flood the usa with immigrants  years ago catholics were the first great destroyers of the usa\n",
            "Text after removing non-alphabetic characters and converting to lowercase: as the catholic church today meddles in usa politics by trying to bring in mexican immigrants and shielding illegal alien mexicans the same catholic church worked hard to flood the usa with immigrants  years ago catholics were the first great destroyers of the usa\n",
            "Text after tokenization: ['as', 'the', 'catholic', 'church', 'today', 'meddles', 'in', 'usa', 'politics', 'by', 'trying', 'to', 'bring', 'in', 'mexican', 'immigrants', 'and', 'shielding', 'illegal', 'alien', 'mexicans', 'the', 'same', 'catholic', 'church', 'worked', 'hard', 'to', 'flood', 'the', 'usa', 'with', 'immigrants', 'years', 'ago', 'catholics', 'were', 'the', 'first', 'great', 'destroyers', 'of', 'the', 'usa']\n",
            "Text after removing stop words: ['catholic', 'church', 'today', 'meddles', 'usa', 'politics', 'trying', 'bring', 'mexican', 'immigrants', 'shielding', 'illegal', 'alien', 'mexicans', 'catholic', 'church', 'worked', 'hard', 'flood', 'usa', 'immigrants', 'years', 'ago', 'catholics', 'first', 'great', 'destroyers', 'usa']\n",
            "Text after Lemmatization: ['catholic', 'church', 'today', 'meddles', 'usa', 'politics', 'trying', 'bring', 'mexican', 'immigrant', 'shielding', 'illegal', 'alien', 'mexican', 'catholic', 'church', 'worked', 'hard', 'flood', 'usa', 'immigrant', 'year', 'ago', 'catholic', 'first', 'great', 'destroyer', 'usa']\n",
            "Final pre-processed text: catholic church today meddles usa politics trying bring mexican immigrant shielding illegal alien mexican catholic church worked hard flood usa immigrant year ago catholic first great destroyer usa\n",
            "Text after removing HTML tags: no jews are retarded because they keep caving in to the towel heads demands letting them have more power to crush our only allies in the me so as i said jews are retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase: no jews are retarded because they keep caving in to the towel heads demands letting them have more power to crush our only allies in the me so as i said jews are retarded\n",
            "Text after tokenization: ['no', 'jews', 'are', 'retarded', 'because', 'they', 'keep', 'caving', 'in', 'to', 'the', 'towel', 'heads', 'demands', 'letting', 'them', 'have', 'more', 'power', 'to', 'crush', 'our', 'only', 'allies', 'in', 'the', 'me', 'so', 'as', 'i', 'said', 'jews', 'are', 'retarded']\n",
            "Text after removing stop words: ['jews', 'retarded', 'keep', 'caving', 'towel', 'heads', 'demands', 'letting', 'power', 'crush', 'allies', 'said', 'jews', 'retarded']\n",
            "Text after Lemmatization: ['jew', 'retarded', 'keep', 'caving', 'towel', 'head', 'demand', 'letting', 'power', 'crush', 'ally', 'said', 'jew', 'retarded']\n",
            "Final pre-processed text: jew retarded keep caving towel head demand letting power crush ally said jew retarded\n",
            "Text after removing HTML tags: t there are many more murders and violent felonies committed than we ever hear much about those we hear of are what is selected by the kike controlled media to further their narratives those they deep six are more mundane but equally deadly often stemming from deep ghetto nihilism\n",
            "Text after removing non-alphabetic characters and converting to lowercase: t there are many more murders and violent felonies committed than we ever hear much about those we hear of are what is selected by the kike controlled media to further their narratives those they deep six are more mundane but equally deadly often stemming from deep ghetto nihilism\n",
            "Text after tokenization: ['t', 'there', 'are', 'many', 'more', 'murders', 'and', 'violent', 'felonies', 'committed', 'than', 'we', 'ever', 'hear', 'much', 'about', 'those', 'we', 'hear', 'of', 'are', 'what', 'is', 'selected', 'by', 'the', 'kike', 'controlled', 'media', 'to', 'further', 'their', 'narratives', 'those', 'they', 'deep', 'six', 'are', 'more', 'mundane', 'but', 'equally', 'deadly', 'often', 'stemming', 'from', 'deep', 'ghetto', 'nihilism']\n",
            "Text after removing stop words: ['many', 'murders', 'violent', 'felonies', 'committed', 'ever', 'hear', 'much', 'hear', 'selected', 'kike', 'controlled', 'media', 'narratives', 'deep', 'six', 'mundane', 'equally', 'deadly', 'often', 'stemming', 'deep', 'ghetto', 'nihilism']\n",
            "Text after Lemmatization: ['many', 'murder', 'violent', 'felony', 'committed', 'ever', 'hear', 'much', 'hear', 'selected', 'kike', 'controlled', 'medium', 'narrative', 'deep', 'six', 'mundane', 'equally', 'deadly', 'often', 'stemming', 'deep', 'ghetto', 'nihilism']\n",
            "Final pre-processed text: many murder violent felony committed ever hear much hear selected kike controlled medium narrative deep six mundane equally deadly often stemming deep ghetto nihilism\n",
            "Text after removing HTML tags: saying  of the population which you were clearly ignorant of was responsible for controlling the streets of all iranian cities with mass mobs was just a retarded idea and you got your face rubbed in the dirt does it all go up your nose\n",
            "Text after removing non-alphabetic characters and converting to lowercase: saying  of the population which you were clearly ignorant of was responsible for controlling the streets of all iranian cities with mass mobs was just a retarded idea and you got your face rubbed in the dirt does it all go up your nose\n",
            "Text after tokenization: ['saying', 'of', 'the', 'population', 'which', 'you', 'were', 'clearly', 'ignorant', 'of', 'was', 'responsible', 'for', 'controlling', 'the', 'streets', 'of', 'all', 'iranian', 'cities', 'with', 'mass', 'mobs', 'was', 'just', 'a', 'retarded', 'idea', 'and', 'you', 'got', 'your', 'face', 'rubbed', 'in', 'the', 'dirt', 'does', 'it', 'all', 'go', 'up', 'your', 'nose']\n",
            "Text after removing stop words: ['saying', 'population', 'clearly', 'ignorant', 'responsible', 'controlling', 'streets', 'iranian', 'cities', 'mass', 'mobs', 'retarded', 'idea', 'got', 'face', 'rubbed', 'dirt', 'go', 'nose']\n",
            "Text after Lemmatization: ['saying', 'population', 'clearly', 'ignorant', 'responsible', 'controlling', 'street', 'iranian', 'city', 'mass', 'mob', 'retarded', 'idea', 'got', 'face', 'rubbed', 'dirt', 'go', 'nose']\n",
            "Final pre-processed text: saying population clearly ignorant responsible controlling street iranian city mass mob retarded idea got face rubbed dirt go nose\n",
            "Text after removing HTML tags: women football announcers are so bad ratings dropped another  as nfl ncaa used women announcers this weekend to make up for the sexual harassment of nfl owners\n",
            "Text after removing non-alphabetic characters and converting to lowercase: women football announcers are so bad ratings dropped another  as nfl ncaa used women announcers this weekend to make up for the sexual harassment of nfl owners\n",
            "Text after tokenization: ['women', 'football', 'announcers', 'are', 'so', 'bad', 'ratings', 'dropped', 'another', 'as', 'nfl', 'ncaa', 'used', 'women', 'announcers', 'this', 'weekend', 'to', 'make', 'up', 'for', 'the', 'sexual', 'harassment', 'of', 'nfl', 'owners']\n",
            "Text after removing stop words: ['women', 'football', 'announcers', 'bad', 'ratings', 'dropped', 'another', 'nfl', 'ncaa', 'used', 'women', 'announcers', 'weekend', 'make', 'sexual', 'harassment', 'nfl', 'owners']\n",
            "Text after Lemmatization: ['woman', 'football', 'announcer', 'bad', 'rating', 'dropped', 'another', 'nfl', 'ncaa', 'used', 'woman', 'announcer', 'weekend', 'make', 'sexual', 'harassment', 'nfl', 'owner']\n",
            "Final pre-processed text: woman football announcer bad rating dropped another nfl ncaa used woman announcer weekend make sexual harassment nfl owner\n",
            "Text after removing HTML tags: the only way sharia may could be a worse leader for britain is if she were a gay muslim faux married to a trannie\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the only way sharia may could be a worse leader for britain is if she were a gay muslim faux married to a trannie\n",
            "Text after tokenization: ['the', 'only', 'way', 'sharia', 'may', 'could', 'be', 'a', 'worse', 'leader', 'for', 'britain', 'is', 'if', 'she', 'were', 'a', 'gay', 'muslim', 'faux', 'married', 'to', 'a', 'trannie']\n",
            "Text after removing stop words: ['way', 'sharia', 'may', 'could', 'worse', 'leader', 'britain', 'gay', 'muslim', 'faux', 'married', 'trannie']\n",
            "Text after Lemmatization: ['way', 'sharia', 'may', 'could', 'worse', 'leader', 'britain', 'gay', 'muslim', 'faux', 'married', 'trannie']\n",
            "Final pre-processed text: way sharia may could worse leader britain gay muslim faux married trannie\n",
            "Text after removing HTML tags: building an army via hostile legislation to freedoms constitution will come back bite the liberal lilly white asses cause guess where they d be headed when loosed\n",
            "Text after removing non-alphabetic characters and converting to lowercase: building an army via hostile legislation to freedoms constitution will come back bite the liberal lilly white asses cause guess where they d be headed when loosed\n",
            "Text after tokenization: ['building', 'an', 'army', 'via', 'hostile', 'legislation', 'to', 'freedoms', 'constitution', 'will', 'come', 'back', 'bite', 'the', 'liberal', 'lilly', 'white', 'asses', 'cause', 'guess', 'where', 'they', 'd', 'be', 'headed', 'when', 'loosed']\n",
            "Text after removing stop words: ['building', 'army', 'via', 'hostile', 'legislation', 'freedoms', 'constitution', 'come', 'back', 'bite', 'liberal', 'lilly', 'white', 'asses', 'cause', 'guess', 'headed', 'loosed']\n",
            "Text after Lemmatization: ['building', 'army', 'via', 'hostile', 'legislation', 'freedom', 'constitution', 'come', 'back', 'bite', 'liberal', 'lilly', 'white', 'ass', 'cause', 'guess', 'headed', 'loosed']\n",
            "Final pre-processed text: building army via hostile legislation freedom constitution come back bite liberal lilly white ass cause guess headed loosed\n",
            "Text after removing HTML tags: i feel bad for frequent fliers i am trying to avoid planes as much as i can the whole airline industry is turning into one big ghetto walmart\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i feel bad for frequent fliers i am trying to avoid planes as much as i can the whole airline industry is turning into one big ghetto walmart\n",
            "Text after tokenization: ['i', 'feel', 'bad', 'for', 'frequent', 'fliers', 'i', 'am', 'trying', 'to', 'avoid', 'planes', 'as', 'much', 'as', 'i', 'can', 'the', 'whole', 'airline', 'industry', 'is', 'turning', 'into', 'one', 'big', 'ghetto', 'walmart']\n",
            "Text after removing stop words: ['feel', 'bad', 'frequent', 'fliers', 'trying', 'avoid', 'planes', 'much', 'whole', 'airline', 'industry', 'turning', 'one', 'big', 'ghetto', 'walmart']\n",
            "Text after Lemmatization: ['feel', 'bad', 'frequent', 'flier', 'trying', 'avoid', 'plane', 'much', 'whole', 'airline', 'industry', 'turning', 'one', 'big', 'ghetto', 'walmart']\n",
            "Final pre-processed text: feel bad frequent flier trying avoid plane much whole airline industry turning one big ghetto walmart\n",
            "Text after removing HTML tags: clueless boomer jews dont want refugees because muslims hate them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: clueless boomer jews dont want refugees because muslims hate them\n",
            "Text after tokenization: ['clueless', 'boomer', 'jews', 'dont', 'want', 'refugees', 'because', 'muslims', 'hate', 'them']\n",
            "Text after removing stop words: ['clueless', 'boomer', 'jews', 'dont', 'want', 'refugees', 'muslims', 'hate']\n",
            "Text after Lemmatization: ['clueless', 'boomer', 'jew', 'dont', 'want', 'refugee', 'muslim', 'hate']\n",
            "Final pre-processed text: clueless boomer jew dont want refugee muslim hate\n",
            "Text after removing HTML tags: sheriff clark thinks the ghetto can be fixed i think i know more about n ggers than he does send them all back to africa that the fix\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sheriff clark thinks the ghetto can be fixed i think i know more about n ggers than he does send them all back to africa that the fix\n",
            "Text after tokenization: ['sheriff', 'clark', 'thinks', 'the', 'ghetto', 'can', 'be', 'fixed', 'i', 'think', 'i', 'know', 'more', 'about', 'n', 'ggers', 'than', 'he', 'does', 'send', 'them', 'all', 'back', 'to', 'africa', 'that', 'the', 'fix']\n",
            "Text after removing stop words: ['sheriff', 'clark', 'thinks', 'ghetto', 'fixed', 'think', 'know', 'n', 'ggers', 'send', 'back', 'africa', 'fix']\n",
            "Text after Lemmatization: ['sheriff', 'clark', 'think', 'ghetto', 'fixed', 'think', 'know', 'n', 'ggers', 'send', 'back', 'africa', 'fix']\n",
            "Final pre-processed text: sheriff clark think ghetto fixed think know n ggers send back africa fix\n",
            "Text after removing HTML tags: voila only way you can save your country from being sucked dry as netanyahu said otherwise they will bring it down from within its all strategy really they bring down big countries by trojan horse thats why they sending all refugees to germany too its all a scam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: voila only way you can save your country from being sucked dry as netanyahu said otherwise they will bring it down from within its all strategy really they bring down big countries by trojan horse thats why they sending all refugees to germany too its all a scam\n",
            "Text after tokenization: ['voila', 'only', 'way', 'you', 'can', 'save', 'your', 'country', 'from', 'being', 'sucked', 'dry', 'as', 'netanyahu', 'said', 'otherwise', 'they', 'will', 'bring', 'it', 'down', 'from', 'within', 'its', 'all', 'strategy', 'really', 'they', 'bring', 'down', 'big', 'countries', 'by', 'trojan', 'horse', 'thats', 'why', 'they', 'sending', 'all', 'refugees', 'to', 'germany', 'too', 'its', 'all', 'a', 'scam']\n",
            "Text after removing stop words: ['voila', 'way', 'save', 'country', 'sucked', 'dry', 'netanyahu', 'said', 'otherwise', 'bring', 'within', 'strategy', 'really', 'bring', 'big', 'countries', 'trojan', 'horse', 'thats', 'sending', 'refugees', 'germany', 'scam']\n",
            "Text after Lemmatization: ['voila', 'way', 'save', 'country', 'sucked', 'dry', 'netanyahu', 'said', 'otherwise', 'bring', 'within', 'strategy', 'really', 'bring', 'big', 'country', 'trojan', 'horse', 'thats', 'sending', 'refugee', 'germany', 'scam']\n",
            "Final pre-processed text: voila way save country sucked dry netanyahu said otherwise bring within strategy really bring big country trojan horse thats sending refugee germany scam\n",
            "Text after removing HTML tags: any society with the ability to do these things would have no problem defending itself against early white men africa has not changed in thousands of years and when white people tried to help they were slaughtered and africa returned to the dark ages your ancestors were always primitive\n",
            "Text after removing non-alphabetic characters and converting to lowercase: any society with the ability to do these things would have no problem defending itself against early white men africa has not changed in thousands of years and when white people tried to help they were slaughtered and africa returned to the dark ages your ancestors were always primitive\n",
            "Text after tokenization: ['any', 'society', 'with', 'the', 'ability', 'to', 'do', 'these', 'things', 'would', 'have', 'no', 'problem', 'defending', 'itself', 'against', 'early', 'white', 'men', 'africa', 'has', 'not', 'changed', 'in', 'thousands', 'of', 'years', 'and', 'when', 'white', 'people', 'tried', 'to', 'help', 'they', 'were', 'slaughtered', 'and', 'africa', 'returned', 'to', 'the', 'dark', 'ages', 'your', 'ancestors', 'were', 'always', 'primitive']\n",
            "Text after removing stop words: ['society', 'ability', 'things', 'would', 'problem', 'defending', 'early', 'white', 'men', 'africa', 'changed', 'thousands', 'years', 'white', 'people', 'tried', 'help', 'slaughtered', 'africa', 'returned', 'dark', 'ages', 'ancestors', 'always', 'primitive']\n",
            "Text after Lemmatization: ['society', 'ability', 'thing', 'would', 'problem', 'defending', 'early', 'white', 'men', 'africa', 'changed', 'thousand', 'year', 'white', 'people', 'tried', 'help', 'slaughtered', 'africa', 'returned', 'dark', 'age', 'ancestor', 'always', 'primitive']\n",
            "Final pre-processed text: society ability thing would problem defending early white men africa changed thousand year white people tried help slaughtered africa returned dark age ancestor always primitive\n",
            "Text after removing HTML tags: yeah or you can start from the negative look at nonwhite countries and nonwhite ghettos in white countries and go from there it is not a difficult question\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yeah or you can start from the negative look at nonwhite countries and nonwhite ghettos in white countries and go from there it is not a difficult question\n",
            "Text after tokenization: ['yeah', 'or', 'you', 'can', 'start', 'from', 'the', 'negative', 'look', 'at', 'nonwhite', 'countries', 'and', 'nonwhite', 'ghettos', 'in', 'white', 'countries', 'and', 'go', 'from', 'there', 'it', 'is', 'not', 'a', 'difficult', 'question']\n",
            "Text after removing stop words: ['yeah', 'start', 'negative', 'look', 'nonwhite', 'countries', 'nonwhite', 'ghettos', 'white', 'countries', 'go', 'difficult', 'question']\n",
            "Text after Lemmatization: ['yeah', 'start', 'negative', 'look', 'nonwhite', 'country', 'nonwhite', 'ghetto', 'white', 'country', 'go', 'difficult', 'question']\n",
            "Final pre-processed text: yeah start negative look nonwhite country nonwhite ghetto white country go difficult question\n",
            "Text after removing HTML tags: vietnamese are obviously asians mizoram and assamese with slanted eyes are clearly asians living assimilating in india they are not genetically indian what are you not understanding race is different from culture\n",
            "Text after removing non-alphabetic characters and converting to lowercase: vietnamese are obviously asians mizoram and assamese with slanted eyes are clearly asians living assimilating in india they are not genetically indian what are you not understanding race is different from culture\n",
            "Text after tokenization: ['vietnamese', 'are', 'obviously', 'asians', 'mizoram', 'and', 'assamese', 'with', 'slanted', 'eyes', 'are', 'clearly', 'asians', 'living', 'assimilating', 'in', 'india', 'they', 'are', 'not', 'genetically', 'indian', 'what', 'are', 'you', 'not', 'understanding', 'race', 'is', 'different', 'from', 'culture']\n",
            "Text after removing stop words: ['vietnamese', 'obviously', 'asians', 'mizoram', 'assamese', 'slanted', 'eyes', 'clearly', 'asians', 'living', 'assimilating', 'india', 'genetically', 'indian', 'understanding', 'race', 'different', 'culture']\n",
            "Text after Lemmatization: ['vietnamese', 'obviously', 'asian', 'mizoram', 'assamese', 'slanted', 'eye', 'clearly', 'asian', 'living', 'assimilating', 'india', 'genetically', 'indian', 'understanding', 'race', 'different', 'culture']\n",
            "Final pre-processed text: vietnamese obviously asian mizoram assamese slanted eye clearly asian living assimilating india genetically indian understanding race different culture\n",
            "Text after removing HTML tags: are you dumb when it says other than german that means a ton of austrians dutch english aussies canadians americans french poles hungarians czechs all border germany too fix your shit you are trying to sell holocaust denial but can not even read numbers in the news\n",
            "Text after removing non-alphabetic characters and converting to lowercase: are you dumb when it says other than german that means a ton of austrians dutch english aussies canadians americans french poles hungarians czechs all border germany too fix your shit you are trying to sell holocaust denial but can not even read numbers in the news\n",
            "Text after tokenization: ['are', 'you', 'dumb', 'when', 'it', 'says', 'other', 'than', 'german', 'that', 'means', 'a', 'ton', 'of', 'austrians', 'dutch', 'english', 'aussies', 'canadians', 'americans', 'french', 'poles', 'hungarians', 'czechs', 'all', 'border', 'germany', 'too', 'fix', 'your', 'shit', 'you', 'are', 'trying', 'to', 'sell', 'holocaust', 'denial', 'but', 'can', 'not', 'even', 'read', 'numbers', 'in', 'the', 'news']\n",
            "Text after removing stop words: ['dumb', 'says', 'german', 'means', 'ton', 'austrians', 'dutch', 'english', 'aussies', 'canadians', 'americans', 'french', 'poles', 'hungarians', 'czechs', 'border', 'germany', 'fix', 'shit', 'trying', 'sell', 'holocaust', 'denial', 'even', 'read', 'numbers', 'news']\n",
            "Text after Lemmatization: ['dumb', 'say', 'german', 'mean', 'ton', 'austrian', 'dutch', 'english', 'aussie', 'canadian', 'american', 'french', 'pole', 'hungarian', 'czech', 'border', 'germany', 'fix', 'shit', 'trying', 'sell', 'holocaust', 'denial', 'even', 'read', 'number', 'news']\n",
            "Final pre-processed text: dumb say german mean ton austrian dutch english aussie canadian american french pole hungarian czech border germany fix shit trying sell holocaust denial even read number news\n",
            "Text after removing HTML tags: i have some pictures of arabs getting kicked up the arse but when i look for them i see some meme and decide to just post that i think i remember the congo edition lol\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i have some pictures of arabs getting kicked up the arse but when i look for them i see some meme and decide to just post that i think i remember the congo edition lol\n",
            "Text after tokenization: ['i', 'have', 'some', 'pictures', 'of', 'arabs', 'getting', 'kicked', 'up', 'the', 'arse', 'but', 'when', 'i', 'look', 'for', 'them', 'i', 'see', 'some', 'meme', 'and', 'decide', 'to', 'just', 'post', 'that', 'i', 'think', 'i', 'remember', 'the', 'congo', 'edition', 'lol']\n",
            "Text after removing stop words: ['pictures', 'arabs', 'getting', 'kicked', 'arse', 'look', 'see', 'meme', 'decide', 'post', 'think', 'remember', 'congo', 'edition', 'lol']\n",
            "Text after Lemmatization: ['picture', 'arab', 'getting', 'kicked', 'arse', 'look', 'see', 'meme', 'decide', 'post', 'think', 'remember', 'congo', 'edition', 'lol']\n",
            "Final pre-processed text: picture arab getting kicked arse look see meme decide post think remember congo edition lol\n",
            "Text after removing HTML tags: deport all  million illegal aliens and replace them by bringing back white americans from abroad it ll do wonders for the white demographics trust me\n",
            "Text after removing non-alphabetic characters and converting to lowercase: deport all  million illegal aliens and replace them by bringing back white americans from abroad it ll do wonders for the white demographics trust me\n",
            "Text after tokenization: ['deport', 'all', 'million', 'illegal', 'aliens', 'and', 'replace', 'them', 'by', 'bringing', 'back', 'white', 'americans', 'from', 'abroad', 'it', 'll', 'do', 'wonders', 'for', 'the', 'white', 'demographics', 'trust', 'me']\n",
            "Text after removing stop words: ['deport', 'million', 'illegal', 'aliens', 'replace', 'bringing', 'back', 'white', 'americans', 'abroad', 'wonders', 'white', 'demographics', 'trust']\n",
            "Text after Lemmatization: ['deport', 'million', 'illegal', 'alien', 'replace', 'bringing', 'back', 'white', 'american', 'abroad', 'wonder', 'white', 'demographic', 'trust']\n",
            "Final pre-processed text: deport million illegal alien replace bringing back white american abroad wonder white demographic trust\n",
            "Text after removing HTML tags:  go back under your white sheet hood\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  go back under your white sheet hood\n",
            "Text after tokenization: ['go', 'back', 'under', 'your', 'white', 'sheet', 'hood']\n",
            "Text after removing stop words: ['go', 'back', 'white', 'sheet', 'hood']\n",
            "Text after Lemmatization: ['go', 'back', 'white', 'sheet', 'hood']\n",
            "Final pre-processed text: go back white sheet hood\n",
            "Text after removing HTML tags: the woman she spoke of was never raped not by six white men not by anybody\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the woman she spoke of was never raped not by six white men not by anybody\n",
            "Text after tokenization: ['the', 'woman', 'she', 'spoke', 'of', 'was', 'never', 'raped', 'not', 'by', 'six', 'white', 'men', 'not', 'by', 'anybody']\n",
            "Text after removing stop words: ['woman', 'spoke', 'never', 'raped', 'six', 'white', 'men', 'anybody']\n",
            "Text after Lemmatization: ['woman', 'spoke', 'never', 'raped', 'six', 'white', 'men', 'anybody']\n",
            "Final pre-processed text: woman spoke never raped six white men anybody\n",
            "Text after removing HTML tags: the animosity shitskins like this express towards whites is undying but they are like flies celebrating feces without whites western civilization collapses and they revert to ways seen in their homelands broke headless bereft of the things they love in white nations\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the animosity shitskins like this express towards whites is undying but they are like flies celebrating feces without whites western civilization collapses and they revert to ways seen in their homelands broke headless bereft of the things they love in white nations\n",
            "Text after tokenization: ['the', 'animosity', 'shitskins', 'like', 'this', 'express', 'towards', 'whites', 'is', 'undying', 'but', 'they', 'are', 'like', 'flies', 'celebrating', 'feces', 'without', 'whites', 'western', 'civilization', 'collapses', 'and', 'they', 'revert', 'to', 'ways', 'seen', 'in', 'their', 'homelands', 'broke', 'headless', 'bereft', 'of', 'the', 'things', 'they', 'love', 'in', 'white', 'nations']\n",
            "Text after removing stop words: ['animosity', 'shitskins', 'like', 'express', 'towards', 'whites', 'undying', 'like', 'flies', 'celebrating', 'feces', 'without', 'whites', 'western', 'civilization', 'collapses', 'revert', 'ways', 'seen', 'homelands', 'broke', 'headless', 'bereft', 'things', 'love', 'white', 'nations']\n",
            "Text after Lemmatization: ['animosity', 'shitskins', 'like', 'express', 'towards', 'white', 'undying', 'like', 'fly', 'celebrating', 'feces', 'without', 'white', 'western', 'civilization', 'collapse', 'revert', 'way', 'seen', 'homeland', 'broke', 'headless', 'bereft', 'thing', 'love', 'white', 'nation']\n",
            "Final pre-processed text: animosity shitskins like express towards white undying like fly celebrating feces without white western civilization collapse revert way seen homeland broke headless bereft thing love white nation\n",
            "Text after removing HTML tags: the damore v google lawsuit reveals the shocking institutionalized racism present in modern sjw tech companies not only was the numerical presence of women celebrated at google solely due to their gender but the presence of caucasians and males was mocked with boos\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the damore v google lawsuit reveals the shocking institutionalized racism present in modern sjw tech companies not only was the numerical presence of women celebrated at google solely due to their gender but the presence of caucasians and males was mocked with boos\n",
            "Text after tokenization: ['the', 'damore', 'v', 'google', 'lawsuit', 'reveals', 'the', 'shocking', 'institutionalized', 'racism', 'present', 'in', 'modern', 'sjw', 'tech', 'companies', 'not', 'only', 'was', 'the', 'numerical', 'presence', 'of', 'women', 'celebrated', 'at', 'google', 'solely', 'due', 'to', 'their', 'gender', 'but', 'the', 'presence', 'of', 'caucasians', 'and', 'males', 'was', 'mocked', 'with', 'boos']\n",
            "Text after removing stop words: ['damore', 'v', 'google', 'lawsuit', 'reveals', 'shocking', 'institutionalized', 'racism', 'present', 'modern', 'sjw', 'tech', 'companies', 'numerical', 'presence', 'women', 'celebrated', 'google', 'solely', 'due', 'gender', 'presence', 'caucasians', 'males', 'mocked', 'boos']\n",
            "Text after Lemmatization: ['damore', 'v', 'google', 'lawsuit', 'reveals', 'shocking', 'institutionalized', 'racism', 'present', 'modern', 'sjw', 'tech', 'company', 'numerical', 'presence', 'woman', 'celebrated', 'google', 'solely', 'due', 'gender', 'presence', 'caucasian', 'male', 'mocked', 'boo']\n",
            "Final pre-processed text: damore v google lawsuit reveals shocking institutionalized racism present modern sjw tech company numerical presence woman celebrated google solely due gender presence caucasian male mocked boo\n",
            "Text after removing HTML tags: fucking fat kike\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fucking fat kike\n",
            "Text after tokenization: ['fucking', 'fat', 'kike']\n",
            "Text after removing stop words: ['fucking', 'fat', 'kike']\n",
            "Text after Lemmatization: ['fucking', 'fat', 'kike']\n",
            "Final pre-processed text: fucking fat kike\n",
            "Text after removing HTML tags: ooo are we slut shaming i love a good slut shaming\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ooo are we slut shaming i love a good slut shaming\n",
            "Text after tokenization: ['ooo', 'are', 'we', 'slut', 'shaming', 'i', 'love', 'a', 'good', 'slut', 'shaming']\n",
            "Text after removing stop words: ['ooo', 'slut', 'shaming', 'love', 'good', 'slut', 'shaming']\n",
            "Text after Lemmatization: ['ooo', 'slut', 'shaming', 'love', 'good', 'slut', 'shaming']\n",
            "Final pre-processed text: ooo slut shaming love good slut shaming\n",
            "Text after removing HTML tags: most cities have a ghetto street named mlk blvd\n",
            "Text after removing non-alphabetic characters and converting to lowercase: most cities have a ghetto street named mlk blvd\n",
            "Text after tokenization: ['most', 'cities', 'have', 'a', 'ghetto', 'street', 'named', 'mlk', 'blvd']\n",
            "Text after removing stop words: ['cities', 'ghetto', 'street', 'named', 'mlk', 'blvd']\n",
            "Text after Lemmatization: ['city', 'ghetto', 'street', 'named', 'mlk', 'blvd']\n",
            "Final pre-processed text: city ghetto street named mlk blvd\n",
            "Text after removing HTML tags: if you can keep your daughter a virgin until  there is a  chance she will not have a illegitimate baby by some ghetto thug on welfare warn your daughters color does matter and so does culture and religion\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if you can keep your daughter a virgin until  there is a  chance she will not have a illegitimate baby by some ghetto thug on welfare warn your daughters color does matter and so does culture and religion\n",
            "Text after tokenization: ['if', 'you', 'can', 'keep', 'your', 'daughter', 'a', 'virgin', 'until', 'there', 'is', 'a', 'chance', 'she', 'will', 'not', 'have', 'a', 'illegitimate', 'baby', 'by', 'some', 'ghetto', 'thug', 'on', 'welfare', 'warn', 'your', 'daughters', 'color', 'does', 'matter', 'and', 'so', 'does', 'culture', 'and', 'religion']\n",
            "Text after removing stop words: ['keep', 'daughter', 'virgin', 'chance', 'illegitimate', 'baby', 'ghetto', 'thug', 'welfare', 'warn', 'daughters', 'color', 'matter', 'culture', 'religion']\n",
            "Text after Lemmatization: ['keep', 'daughter', 'virgin', 'chance', 'illegitimate', 'baby', 'ghetto', 'thug', 'welfare', 'warn', 'daughter', 'color', 'matter', 'culture', 'religion']\n",
            "Final pre-processed text: keep daughter virgin chance illegitimate baby ghetto thug welfare warn daughter color matter culture religion\n",
            "Text after removing HTML tags: deport fr martin to sweden considering he wants for america whom the media jesuits pretend to love the same mass migration that happened to sweden let him tweet from the no go zones and escort white women at night as an act of real charity\n",
            "Text after removing non-alphabetic characters and converting to lowercase: deport fr martin to sweden considering he wants for america whom the media jesuits pretend to love the same mass migration that happened to sweden let him tweet from the no go zones and escort white women at night as an act of real charity\n",
            "Text after tokenization: ['deport', 'fr', 'martin', 'to', 'sweden', 'considering', 'he', 'wants', 'for', 'america', 'whom', 'the', 'media', 'jesuits', 'pretend', 'to', 'love', 'the', 'same', 'mass', 'migration', 'that', 'happened', 'to', 'sweden', 'let', 'him', 'tweet', 'from', 'the', 'no', 'go', 'zones', 'and', 'escort', 'white', 'women', 'at', 'night', 'as', 'an', 'act', 'of', 'real', 'charity']\n",
            "Text after removing stop words: ['deport', 'fr', 'martin', 'sweden', 'considering', 'wants', 'america', 'media', 'jesuits', 'pretend', 'love', 'mass', 'migration', 'happened', 'sweden', 'let', 'tweet', 'go', 'zones', 'escort', 'white', 'women', 'night', 'act', 'real', 'charity']\n",
            "Text after Lemmatization: ['deport', 'fr', 'martin', 'sweden', 'considering', 'want', 'america', 'medium', 'jesuit', 'pretend', 'love', 'mass', 'migration', 'happened', 'sweden', 'let', 'tweet', 'go', 'zone', 'escort', 'white', 'woman', 'night', 'act', 'real', 'charity']\n",
            "Final pre-processed text: deport fr martin sweden considering want america medium jesuit pretend love mass migration happened sweden let tweet go zone escort white woman night act real charity\n",
            "Text after removing HTML tags: you can say potato nigger all ya want just do not say the word sun my arms are already red from it and i have no more sunblock\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you can say potato nigger all ya want just do not say the word sun my arms are already red from it and i have no more sunblock\n",
            "Text after tokenization: ['you', 'can', 'say', 'potato', 'nigger', 'all', 'ya', 'want', 'just', 'do', 'not', 'say', 'the', 'word', 'sun', 'my', 'arms', 'are', 'already', 'red', 'from', 'it', 'and', 'i', 'have', 'no', 'more', 'sunblock']\n",
            "Text after removing stop words: ['say', 'potato', 'nigger', 'ya', 'want', 'say', 'word', 'sun', 'arms', 'already', 'red', 'sunblock']\n",
            "Text after Lemmatization: ['say', 'potato', 'nigger', 'ya', 'want', 'say', 'word', 'sun', 'arm', 'already', 'red', 'sunblock']\n",
            "Final pre-processed text: say potato nigger ya want say word sun arm already red sunblock\n",
            "Text after removing HTML tags: i meant nazi like in their tactics threats intimidation violence during the scottish indy referendum not goose stepping all around glasgow\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i meant nazi like in their tactics threats intimidation violence during the scottish indy referendum not goose stepping all around glasgow\n",
            "Text after tokenization: ['i', 'meant', 'nazi', 'like', 'in', 'their', 'tactics', 'threats', 'intimidation', 'violence', 'during', 'the', 'scottish', 'indy', 'referendum', 'not', 'goose', 'stepping', 'all', 'around', 'glasgow']\n",
            "Text after removing stop words: ['meant', 'nazi', 'like', 'tactics', 'threats', 'intimidation', 'violence', 'scottish', 'indy', 'referendum', 'goose', 'stepping', 'around', 'glasgow']\n",
            "Text after Lemmatization: ['meant', 'nazi', 'like', 'tactic', 'threat', 'intimidation', 'violence', 'scottish', 'indy', 'referendum', 'goose', 'stepping', 'around', 'glasgow']\n",
            "Final pre-processed text: meant nazi like tactic threat intimidation violence scottish indy referendum goose stepping around glasgow\n",
            "Text after removing HTML tags: my wife comes from a shithole country she has been saying this for more than  years she says we have to move to a whiter city because the illegals and refugees are turning our city into a 3 rd world shithole\n",
            "Text after removing non-alphabetic characters and converting to lowercase: my wife comes from a shithole country she has been saying this for more than  years she says we have to move to a whiter city because the illegals and refugees are turning our city into a   rd world shithole\n",
            "Text after tokenization: ['my', 'wife', 'comes', 'from', 'a', 'shithole', 'country', 'she', 'has', 'been', 'saying', 'this', 'for', 'more', 'than', 'years', 'she', 'says', 'we', 'have', 'to', 'move', 'to', 'a', 'whiter', 'city', 'because', 'the', 'illegals', 'and', 'refugees', 'are', 'turning', 'our', 'city', 'into', 'a', 'rd', 'world', 'shithole']\n",
            "Text after removing stop words: ['wife', 'comes', 'shithole', 'country', 'saying', 'years', 'says', 'move', 'whiter', 'city', 'illegals', 'refugees', 'turning', 'city', 'rd', 'world', 'shithole']\n",
            "Text after Lemmatization: ['wife', 'come', 'shithole', 'country', 'saying', 'year', 'say', 'move', 'whiter', 'city', 'illegals', 'refugee', 'turning', 'city', 'rd', 'world', 'shithole']\n",
            "Final pre-processed text: wife come shithole country saying year say move whiter city illegals refugee turning city rd world shithole\n",
            "Text after removing HTML tags: wewereneverasked daca is clearly not consensual it is abusive to legal immigrants and citizens doctor sekzi \n",
            "Text after removing non-alphabetic characters and converting to lowercase: wewereneverasked daca is clearly not consensual it is abusive to legal immigrants and citizens doctor sekzi \n",
            "Text after tokenization: ['wewereneverasked', 'daca', 'is', 'clearly', 'not', 'consensual', 'it', 'is', 'abusive', 'to', 'legal', 'immigrants', 'and', 'citizens', 'doctor', 'sekzi']\n",
            "Text after removing stop words: ['wewereneverasked', 'daca', 'clearly', 'consensual', 'abusive', 'legal', 'immigrants', 'citizens', 'doctor', 'sekzi']\n",
            "Text after Lemmatization: ['wewereneverasked', 'daca', 'clearly', 'consensual', 'abusive', 'legal', 'immigrant', 'citizen', 'doctor', 'sekzi']\n",
            "Final pre-processed text: wewereneverasked daca clearly consensual abusive legal immigrant citizen doctor sekzi\n",
            "Text after removing HTML tags: imagine if we had like pastries instead of genitalia like bro put your chocolate bar into my glazed cake no homo tho\n",
            "Text after removing non-alphabetic characters and converting to lowercase: imagine if we had like pastries instead of genitalia like bro put your chocolate bar into my glazed cake no homo tho\n",
            "Text after tokenization: ['imagine', 'if', 'we', 'had', 'like', 'pastries', 'instead', 'of', 'genitalia', 'like', 'bro', 'put', 'your', 'chocolate', 'bar', 'into', 'my', 'glazed', 'cake', 'no', 'homo', 'tho']\n",
            "Text after removing stop words: ['imagine', 'like', 'pastries', 'instead', 'genitalia', 'like', 'bro', 'put', 'chocolate', 'bar', 'glazed', 'cake', 'homo', 'tho']\n",
            "Text after Lemmatization: ['imagine', 'like', 'pastry', 'instead', 'genitalia', 'like', 'bro', 'put', 'chocolate', 'bar', 'glazed', 'cake', 'homo', 'tho']\n",
            "Final pre-processed text: imagine like pastry instead genitalia like bro put chocolate bar glazed cake homo tho\n",
            "Text after removing HTML tags: but i bet they think that sperg retard who shot up the black church was a white hero too fucking retarded to understand the damage he did to their own cause\n",
            "Text after removing non-alphabetic characters and converting to lowercase: but i bet they think that sperg retard who shot up the black church was a white hero too fucking retarded to understand the damage he did to their own cause\n",
            "Text after tokenization: ['but', 'i', 'bet', 'they', 'think', 'that', 'sperg', 'retard', 'who', 'shot', 'up', 'the', 'black', 'church', 'was', 'a', 'white', 'hero', 'too', 'fucking', 'retarded', 'to', 'understand', 'the', 'damage', 'he', 'did', 'to', 'their', 'own', 'cause']\n",
            "Text after removing stop words: ['bet', 'think', 'sperg', 'retard', 'shot', 'black', 'church', 'white', 'hero', 'fucking', 'retarded', 'understand', 'damage', 'cause']\n",
            "Text after Lemmatization: ['bet', 'think', 'sperg', 'retard', 'shot', 'black', 'church', 'white', 'hero', 'fucking', 'retarded', 'understand', 'damage', 'cause']\n",
            "Final pre-processed text: bet think sperg retard shot black church white hero fucking retarded understand damage cause\n",
            "Text after removing HTML tags: you just compared east asians to sandniggers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you just compared east asians to sandniggers\n",
            "Text after tokenization: ['you', 'just', 'compared', 'east', 'asians', 'to', 'sandniggers']\n",
            "Text after removing stop words: ['compared', 'east', 'asians', 'sandniggers']\n",
            "Text after Lemmatization: ['compared', 'east', 'asian', 'sandniggers']\n",
            "Final pre-processed text: compared east asian sandniggers\n",
            "Text after removing HTML tags: i almost feel bad for her since dindus tend to target blacks who do not go along with the groupthink of blaming da white devil and screeching that wewuzkangz it heartbreaking sometimes\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i almost feel bad for her since dindus tend to target blacks who do not go along with the groupthink of blaming da white devil and screeching that wewuzkangz it heartbreaking sometimes\n",
            "Text after tokenization: ['i', 'almost', 'feel', 'bad', 'for', 'her', 'since', 'dindus', 'tend', 'to', 'target', 'blacks', 'who', 'do', 'not', 'go', 'along', 'with', 'the', 'groupthink', 'of', 'blaming', 'da', 'white', 'devil', 'and', 'screeching', 'that', 'wewuzkangz', 'it', 'heartbreaking', 'sometimes']\n",
            "Text after removing stop words: ['almost', 'feel', 'bad', 'since', 'dindus', 'tend', 'target', 'blacks', 'go', 'along', 'groupthink', 'blaming', 'da', 'white', 'devil', 'screeching', 'wewuzkangz', 'heartbreaking', 'sometimes']\n",
            "Text after Lemmatization: ['almost', 'feel', 'bad', 'since', 'dindus', 'tend', 'target', 'black', 'go', 'along', 'groupthink', 'blaming', 'da', 'white', 'devil', 'screeching', 'wewuzkangz', 'heartbreaking', 'sometimes']\n",
            "Final pre-processed text: almost feel bad since dindus tend target black go along groupthink blaming da white devil screeching wewuzkangz heartbreaking sometimes\n",
            "Text after removing HTML tags: thousands of children in your glasgow constituency are being raped by muslim and gypsy grooming gangs do you have anything to say to them ms sturgeon\n",
            "Text after removing non-alphabetic characters and converting to lowercase: thousands of children in your glasgow constituency are being raped by muslim and gypsy grooming gangs do you have anything to say to them ms sturgeon\n",
            "Text after tokenization: ['thousands', 'of', 'children', 'in', 'your', 'glasgow', 'constituency', 'are', 'being', 'raped', 'by', 'muslim', 'and', 'gypsy', 'grooming', 'gangs', 'do', 'you', 'have', 'anything', 'to', 'say', 'to', 'them', 'ms', 'sturgeon']\n",
            "Text after removing stop words: ['thousands', 'children', 'glasgow', 'constituency', 'raped', 'muslim', 'gypsy', 'grooming', 'gangs', 'anything', 'say', 'ms', 'sturgeon']\n",
            "Text after Lemmatization: ['thousand', 'child', 'glasgow', 'constituency', 'raped', 'muslim', 'gypsy', 'grooming', 'gang', 'anything', 'say', 'm', 'sturgeon']\n",
            "Final pre-processed text: thousand child glasgow constituency raped muslim gypsy grooming gang anything say m sturgeon\n",
            "Text after removing HTML tags: gab day i think it a great day to get the word out to all of our friends family coworkers and fellow countryman about where to find a real uncensored free speech community to replace that globalist internet spying ghetto called twitter let make today gabday\n",
            "Text after removing non-alphabetic characters and converting to lowercase: gab day i think it a great day to get the word out to all of our friends family coworkers and fellow countryman about where to find a real uncensored free speech community to replace that globalist internet spying ghetto called twitter let make today gabday\n",
            "Text after tokenization: ['gab', 'day', 'i', 'think', 'it', 'a', 'great', 'day', 'to', 'get', 'the', 'word', 'out', 'to', 'all', 'of', 'our', 'friends', 'family', 'coworkers', 'and', 'fellow', 'countryman', 'about', 'where', 'to', 'find', 'a', 'real', 'uncensored', 'free', 'speech', 'community', 'to', 'replace', 'that', 'globalist', 'internet', 'spying', 'ghetto', 'called', 'twitter', 'let', 'make', 'today', 'gabday']\n",
            "Text after removing stop words: ['gab', 'day', 'think', 'great', 'day', 'get', 'word', 'friends', 'family', 'coworkers', 'fellow', 'countryman', 'find', 'real', 'uncensored', 'free', 'speech', 'community', 'replace', 'globalist', 'internet', 'spying', 'ghetto', 'called', 'twitter', 'let', 'make', 'today', 'gabday']\n",
            "Text after Lemmatization: ['gab', 'day', 'think', 'great', 'day', 'get', 'word', 'friend', 'family', 'coworkers', 'fellow', 'countryman', 'find', 'real', 'uncensored', 'free', 'speech', 'community', 'replace', 'globalist', 'internet', 'spying', 'ghetto', 'called', 'twitter', 'let', 'make', 'today', 'gabday']\n",
            "Final pre-processed text: gab day think great day get word friend family coworkers fellow countryman find real uncensored free speech community replace globalist internet spying ghetto called twitter let make today gabday\n",
            "Text after removing HTML tags: this cathocuck literally argues that to be a good christian you need to be a cuckold ok folks no more making white babies go to the nearest ghetto and adopt a dindu you heard the man cuck schmitz email \n",
            "Text after removing non-alphabetic characters and converting to lowercase: this cathocuck literally argues that to be a good christian you need to be a cuckold ok folks no more making white babies go to the nearest ghetto and adopt a dindu you heard the man cuck schmitz email \n",
            "Text after tokenization: ['this', 'cathocuck', 'literally', 'argues', 'that', 'to', 'be', 'a', 'good', 'christian', 'you', 'need', 'to', 'be', 'a', 'cuckold', 'ok', 'folks', 'no', 'more', 'making', 'white', 'babies', 'go', 'to', 'the', 'nearest', 'ghetto', 'and', 'adopt', 'a', 'dindu', 'you', 'heard', 'the', 'man', 'cuck', 'schmitz', 'email']\n",
            "Text after removing stop words: ['cathocuck', 'literally', 'argues', 'good', 'christian', 'need', 'cuckold', 'ok', 'folks', 'making', 'white', 'babies', 'go', 'nearest', 'ghetto', 'adopt', 'dindu', 'heard', 'man', 'cuck', 'schmitz', 'email']\n",
            "Text after Lemmatization: ['cathocuck', 'literally', 'argues', 'good', 'christian', 'need', 'cuckold', 'ok', 'folk', 'making', 'white', 'baby', 'go', 'nearest', 'ghetto', 'adopt', 'dindu', 'heard', 'man', 'cuck', 'schmitz', 'email']\n",
            "Final pre-processed text: cathocuck literally argues good christian need cuckold ok folk making white baby go nearest ghetto adopt dindu heard man cuck schmitz email\n",
            "Text after removing HTML tags: as the only white man on earth who can actually think you can imagine how painful everyday life in this miserablist of all jewtopias is for me sharemypain\n",
            "Text after removing non-alphabetic characters and converting to lowercase: as the only white man on earth who can actually think you can imagine how painful everyday life in this miserablist of all jewtopias is for me sharemypain\n",
            "Text after tokenization: ['as', 'the', 'only', 'white', 'man', 'on', 'earth', 'who', 'can', 'actually', 'think', 'you', 'can', 'imagine', 'how', 'painful', 'everyday', 'life', 'in', 'this', 'miserablist', 'of', 'all', 'jewtopias', 'is', 'for', 'me', 'sharemypain']\n",
            "Text after removing stop words: ['white', 'man', 'earth', 'actually', 'think', 'imagine', 'painful', 'everyday', 'life', 'miserablist', 'jewtopias', 'sharemypain']\n",
            "Text after Lemmatization: ['white', 'man', 'earth', 'actually', 'think', 'imagine', 'painful', 'everyday', 'life', 'miserablist', 'jewtopias', 'sharemypain']\n",
            "Final pre-processed text: white man earth actually think imagine painful everyday life miserablist jewtopias sharemypain\n",
            "Text after removing HTML tags: southafrica is proof whites built a beautiful and safe country the us is proof look at auto industry detroit look at steel mill towns look at every nice neighborhood and city turn to trashed out ghettos when blacks move in truth is not racist its truth\n",
            "Text after removing non-alphabetic characters and converting to lowercase: southafrica is proof whites built a beautiful and safe country the us is proof look at auto industry detroit look at steel mill towns look at every nice neighborhood and city turn to trashed out ghettos when blacks move in truth is not racist its truth\n",
            "Text after tokenization: ['southafrica', 'is', 'proof', 'whites', 'built', 'a', 'beautiful', 'and', 'safe', 'country', 'the', 'us', 'is', 'proof', 'look', 'at', 'auto', 'industry', 'detroit', 'look', 'at', 'steel', 'mill', 'towns', 'look', 'at', 'every', 'nice', 'neighborhood', 'and', 'city', 'turn', 'to', 'trashed', 'out', 'ghettos', 'when', 'blacks', 'move', 'in', 'truth', 'is', 'not', 'racist', 'its', 'truth']\n",
            "Text after removing stop words: ['southafrica', 'proof', 'whites', 'built', 'beautiful', 'safe', 'country', 'us', 'proof', 'look', 'auto', 'industry', 'detroit', 'look', 'steel', 'mill', 'towns', 'look', 'every', 'nice', 'neighborhood', 'city', 'turn', 'trashed', 'ghettos', 'blacks', 'move', 'truth', 'racist', 'truth']\n",
            "Text after Lemmatization: ['southafrica', 'proof', 'white', 'built', 'beautiful', 'safe', 'country', 'u', 'proof', 'look', 'auto', 'industry', 'detroit', 'look', 'steel', 'mill', 'town', 'look', 'every', 'nice', 'neighborhood', 'city', 'turn', 'trashed', 'ghetto', 'black', 'move', 'truth', 'racist', 'truth']\n",
            "Final pre-processed text: southafrica proof white built beautiful safe country u proof look auto industry detroit look steel mill town look every nice neighborhood city turn trashed ghetto black move truth racist truth\n",
            "Text after removing HTML tags: innocent refugees are just looking to work hard and haiti has some wonderful opportunities in construction and skilled trades also bartending for past their prime celebrities on vacation is a hot trend conaninhaiti\n",
            "Text after removing non-alphabetic characters and converting to lowercase: innocent refugees are just looking to work hard and haiti has some wonderful opportunities in construction and skilled trades also bartending for past their prime celebrities on vacation is a hot trend conaninhaiti\n",
            "Text after tokenization: ['innocent', 'refugees', 'are', 'just', 'looking', 'to', 'work', 'hard', 'and', 'haiti', 'has', 'some', 'wonderful', 'opportunities', 'in', 'construction', 'and', 'skilled', 'trades', 'also', 'bartending', 'for', 'past', 'their', 'prime', 'celebrities', 'on', 'vacation', 'is', 'a', 'hot', 'trend', 'conaninhaiti']\n",
            "Text after removing stop words: ['innocent', 'refugees', 'looking', 'work', 'hard', 'haiti', 'wonderful', 'opportunities', 'construction', 'skilled', 'trades', 'also', 'bartending', 'past', 'prime', 'celebrities', 'vacation', 'hot', 'trend', 'conaninhaiti']\n",
            "Text after Lemmatization: ['innocent', 'refugee', 'looking', 'work', 'hard', 'haiti', 'wonderful', 'opportunity', 'construction', 'skilled', 'trade', 'also', 'bartending', 'past', 'prime', 'celebrity', 'vacation', 'hot', 'trend', 'conaninhaiti']\n",
            "Final pre-processed text: innocent refugee looking work hard haiti wonderful opportunity construction skilled trade also bartending past prime celebrity vacation hot trend conaninhaiti\n",
            "Text after removing HTML tags: he shot at a cop she was harbarging him her word she said she did not but she did and she was not charged i always get spellbound by her golden locks fluttering in the gentle ghetto breeze\n",
            "Text after removing non-alphabetic characters and converting to lowercase: he shot at a cop she was harbarging him her word she said she did not but she did and she was not charged i always get spellbound by her golden locks fluttering in the gentle ghetto breeze\n",
            "Text after tokenization: ['he', 'shot', 'at', 'a', 'cop', 'she', 'was', 'harbarging', 'him', 'her', 'word', 'she', 'said', 'she', 'did', 'not', 'but', 'she', 'did', 'and', 'she', 'was', 'not', 'charged', 'i', 'always', 'get', 'spellbound', 'by', 'her', 'golden', 'locks', 'fluttering', 'in', 'the', 'gentle', 'ghetto', 'breeze']\n",
            "Text after removing stop words: ['shot', 'cop', 'harbarging', 'word', 'said', 'charged', 'always', 'get', 'spellbound', 'golden', 'locks', 'fluttering', 'gentle', 'ghetto', 'breeze']\n",
            "Text after Lemmatization: ['shot', 'cop', 'harbarging', 'word', 'said', 'charged', 'always', 'get', 'spellbound', 'golden', 'lock', 'fluttering', 'gentle', 'ghetto', 'breeze']\n",
            "Final pre-processed text: shot cop harbarging word said charged always get spellbound golden lock fluttering gentle ghetto breeze\n",
            "Text after removing HTML tags: pure and utter jewish behavior yes everyone who laughs and makes fun of your retarded ass is engaging in behavior you have said nothing of consequence because you have no logical leg to stand on you are fertilizer you just do not realize it yet faggy tough guy routine ahahahahahahahaha faggot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: pure and utter jewish behavior yes everyone who laughs and makes fun of your retarded ass is engaging in behavior you have said nothing of consequence because you have no logical leg to stand on you are fertilizer you just do not realize it yet faggy tough guy routine ahahahahahahahaha faggot\n",
            "Text after tokenization: ['pure', 'and', 'utter', 'jewish', 'behavior', 'yes', 'everyone', 'who', 'laughs', 'and', 'makes', 'fun', 'of', 'your', 'retarded', 'ass', 'is', 'engaging', 'in', 'behavior', 'you', 'have', 'said', 'nothing', 'of', 'consequence', 'because', 'you', 'have', 'no', 'logical', 'leg', 'to', 'stand', 'on', 'you', 'are', 'fertilizer', 'you', 'just', 'do', 'not', 'realize', 'it', 'yet', 'faggy', 'tough', 'guy', 'routine', 'ahahahahahahahaha', 'faggot']\n",
            "Text after removing stop words: ['pure', 'utter', 'jewish', 'behavior', 'yes', 'everyone', 'laughs', 'makes', 'fun', 'retarded', 'ass', 'engaging', 'behavior', 'said', 'nothing', 'consequence', 'logical', 'leg', 'stand', 'fertilizer', 'realize', 'yet', 'faggy', 'tough', 'guy', 'routine', 'ahahahahahahahaha', 'faggot']\n",
            "Text after Lemmatization: ['pure', 'utter', 'jewish', 'behavior', 'yes', 'everyone', 'laugh', 'make', 'fun', 'retarded', 'as', 'engaging', 'behavior', 'said', 'nothing', 'consequence', 'logical', 'leg', 'stand', 'fertilizer', 'realize', 'yet', 'faggy', 'tough', 'guy', 'routine', 'ahahahahahahahaha', 'faggot']\n",
            "Final pre-processed text: pure utter jewish behavior yes everyone laugh make fun retarded as engaging behavior said nothing consequence logical leg stand fertilizer realize yet faggy tough guy routine ahahahahahahahaha faggot\n",
            "Text after removing HTML tags: what  ‍ did was big he followed by a lot of israel first cuckservatives many heard accurate criticism of jews for the first time in their lives because of nehlen some unfollowed but some will begin to question\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what    did was big he followed by a lot of israel first cuckservatives many heard accurate criticism of jews for the first time in their lives because of nehlen some unfollowed but some will begin to question\n",
            "Text after tokenization: ['what', 'did', 'was', 'big', 'he', 'followed', 'by', 'a', 'lot', 'of', 'israel', 'first', 'cuckservatives', 'many', 'heard', 'accurate', 'criticism', 'of', 'jews', 'for', 'the', 'first', 'time', 'in', 'their', 'lives', 'because', 'of', 'nehlen', 'some', 'unfollowed', 'but', 'some', 'will', 'begin', 'to', 'question']\n",
            "Text after removing stop words: ['big', 'followed', 'lot', 'israel', 'first', 'cuckservatives', 'many', 'heard', 'accurate', 'criticism', 'jews', 'first', 'time', 'lives', 'nehlen', 'unfollowed', 'begin', 'question']\n",
            "Text after Lemmatization: ['big', 'followed', 'lot', 'israel', 'first', 'cuckservatives', 'many', 'heard', 'accurate', 'criticism', 'jew', 'first', 'time', 'life', 'nehlen', 'unfollowed', 'begin', 'question']\n",
            "Final pre-processed text: big followed lot israel first cuckservatives many heard accurate criticism jew first time life nehlen unfollowed begin question\n",
            "Text after removing HTML tags: even in my younger retard liberal years i was always against abortion never seemed right to me\n",
            "Text after removing non-alphabetic characters and converting to lowercase: even in my younger retard liberal years i was always against abortion never seemed right to me\n",
            "Text after tokenization: ['even', 'in', 'my', 'younger', 'retard', 'liberal', 'years', 'i', 'was', 'always', 'against', 'abortion', 'never', 'seemed', 'right', 'to', 'me']\n",
            "Text after removing stop words: ['even', 'younger', 'retard', 'liberal', 'years', 'always', 'abortion', 'never', 'seemed', 'right']\n",
            "Text after Lemmatization: ['even', 'younger', 'retard', 'liberal', 'year', 'always', 'abortion', 'never', 'seemed', 'right']\n",
            "Final pre-processed text: even younger retard liberal year always abortion never seemed right\n",
            "Text after removing HTML tags: from the sky  bringing back the great king of the asians turkey is asia minor before and after mars reigns supreme before and after wars mars is the god of war\n",
            "Text after removing non-alphabetic characters and converting to lowercase: from the sky  bringing back the great king of the asians turkey is asia minor before and after mars reigns supreme before and after wars mars is the god of war\n",
            "Text after tokenization: ['from', 'the', 'sky', 'bringing', 'back', 'the', 'great', 'king', 'of', 'the', 'asians', 'turkey', 'is', 'asia', 'minor', 'before', 'and', 'after', 'mars', 'reigns', 'supreme', 'before', 'and', 'after', 'wars', 'mars', 'is', 'the', 'god', 'of', 'war']\n",
            "Text after removing stop words: ['sky', 'bringing', 'back', 'great', 'king', 'asians', 'turkey', 'asia', 'minor', 'mars', 'reigns', 'supreme', 'wars', 'mars', 'god', 'war']\n",
            "Text after Lemmatization: ['sky', 'bringing', 'back', 'great', 'king', 'asian', 'turkey', 'asia', 'minor', 'mar', 'reign', 'supreme', 'war', 'mar', 'god', 'war']\n",
            "Final pre-processed text: sky bringing back great king asian turkey asia minor mar reign supreme war mar god war\n",
            "Text after removing HTML tags: shapiro is born and raised in cali his business is in la hes just your typical upper class privileged white man that lives far away from the immigrant community he probably grew up in a jewish bubble in the burbs just like many brooklyn jews\n",
            "Text after removing non-alphabetic characters and converting to lowercase: shapiro is born and raised in cali his business is in la hes just your typical upper class privileged white man that lives far away from the immigrant community he probably grew up in a jewish bubble in the burbs just like many brooklyn jews\n",
            "Text after tokenization: ['shapiro', 'is', 'born', 'and', 'raised', 'in', 'cali', 'his', 'business', 'is', 'in', 'la', 'hes', 'just', 'your', 'typical', 'upper', 'class', 'privileged', 'white', 'man', 'that', 'lives', 'far', 'away', 'from', 'the', 'immigrant', 'community', 'he', 'probably', 'grew', 'up', 'in', 'a', 'jewish', 'bubble', 'in', 'the', 'burbs', 'just', 'like', 'many', 'brooklyn', 'jews']\n",
            "Text after removing stop words: ['shapiro', 'born', 'raised', 'cali', 'business', 'la', 'hes', 'typical', 'upper', 'class', 'privileged', 'white', 'man', 'lives', 'far', 'away', 'immigrant', 'community', 'probably', 'grew', 'jewish', 'bubble', 'burbs', 'like', 'many', 'brooklyn', 'jews']\n",
            "Text after Lemmatization: ['shapiro', 'born', 'raised', 'cali', 'business', 'la', 'he', 'typical', 'upper', 'class', 'privileged', 'white', 'man', 'life', 'far', 'away', 'immigrant', 'community', 'probably', 'grew', 'jewish', 'bubble', 'burbs', 'like', 'many', 'brooklyn', 'jew']\n",
            "Final pre-processed text: shapiro born raised cali business la he typical upper class privileged white man life far away immigrant community probably grew jewish bubble burbs like many brooklyn jew\n",
            "Text after removing HTML tags: hey guys i just saw the greates name for don lemon ahem nigger cunt\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hey guys i just saw the greates name for don lemon ahem nigger cunt\n",
            "Text after tokenization: ['hey', 'guys', 'i', 'just', 'saw', 'the', 'greates', 'name', 'for', 'don', 'lemon', 'ahem', 'nigger', 'cunt']\n",
            "Text after removing stop words: ['hey', 'guys', 'saw', 'greates', 'name', 'lemon', 'ahem', 'nigger', 'cunt']\n",
            "Text after Lemmatization: ['hey', 'guy', 'saw', 'greates', 'name', 'lemon', 'ahem', 'nigger', 'cunt']\n",
            "Final pre-processed text: hey guy saw greates name lemon ahem nigger cunt\n",
            "Text after removing HTML tags: yes these are our youth this guy has not even gone to trial yet we do not know all the details but heres  ‍ everytime under the bus anyone white while his based niggers rampage everywhere and he does not say shit\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yes these are our youth this guy has not even gone to trial yet we do not know all the details but heres    everytime under the bus anyone white while his based niggers rampage everywhere and he does not say shit\n",
            "Text after tokenization: ['yes', 'these', 'are', 'our', 'youth', 'this', 'guy', 'has', 'not', 'even', 'gone', 'to', 'trial', 'yet', 'we', 'do', 'not', 'know', 'all', 'the', 'details', 'but', 'heres', 'everytime', 'under', 'the', 'bus', 'anyone', 'white', 'while', 'his', 'based', 'niggers', 'rampage', 'everywhere', 'and', 'he', 'does', 'not', 'say', 'shit']\n",
            "Text after removing stop words: ['yes', 'youth', 'guy', 'even', 'gone', 'trial', 'yet', 'know', 'details', 'heres', 'everytime', 'bus', 'anyone', 'white', 'based', 'niggers', 'rampage', 'everywhere', 'say', 'shit']\n",
            "Text after Lemmatization: ['yes', 'youth', 'guy', 'even', 'gone', 'trial', 'yet', 'know', 'detail', 'here', 'everytime', 'bus', 'anyone', 'white', 'based', 'nigger', 'rampage', 'everywhere', 'say', 'shit']\n",
            "Final pre-processed text: yes youth guy even gone trial yet know detail here everytime bus anyone white based nigger rampage everywhere say shit\n",
            "Text after removing HTML tags: i did suggest trump could find ways to get other budgets to pay for it and that idea is perfect way to do it after all no objects to them doing levees to keep would be floodwater under control this is a immigrant levee to keep immigrants under control\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i did suggest trump could find ways to get other budgets to pay for it and that idea is perfect way to do it after all no objects to them doing levees to keep would be floodwater under control this is a immigrant levee to keep immigrants under control\n",
            "Text after tokenization: ['i', 'did', 'suggest', 'trump', 'could', 'find', 'ways', 'to', 'get', 'other', 'budgets', 'to', 'pay', 'for', 'it', 'and', 'that', 'idea', 'is', 'perfect', 'way', 'to', 'do', 'it', 'after', 'all', 'no', 'objects', 'to', 'them', 'doing', 'levees', 'to', 'keep', 'would', 'be', 'floodwater', 'under', 'control', 'this', 'is', 'a', 'immigrant', 'levee', 'to', 'keep', 'immigrants', 'under', 'control']\n",
            "Text after removing stop words: ['suggest', 'trump', 'could', 'find', 'ways', 'get', 'budgets', 'pay', 'idea', 'perfect', 'way', 'objects', 'levees', 'keep', 'would', 'floodwater', 'control', 'immigrant', 'levee', 'keep', 'immigrants', 'control']\n",
            "Text after Lemmatization: ['suggest', 'trump', 'could', 'find', 'way', 'get', 'budget', 'pay', 'idea', 'perfect', 'way', 'object', 'levee', 'keep', 'would', 'floodwater', 'control', 'immigrant', 'levee', 'keep', 'immigrant', 'control']\n",
            "Final pre-processed text: suggest trump could find way get budget pay idea perfect way object levee keep would floodwater control immigrant levee keep immigrant control\n",
            "Text after removing HTML tags: girls should have some sort of hat pin and stick it in the muslim scum eyes\n",
            "Text after removing non-alphabetic characters and converting to lowercase: girls should have some sort of hat pin and stick it in the muslim scum eyes\n",
            "Text after tokenization: ['girls', 'should', 'have', 'some', 'sort', 'of', 'hat', 'pin', 'and', 'stick', 'it', 'in', 'the', 'muslim', 'scum', 'eyes']\n",
            "Text after removing stop words: ['girls', 'sort', 'hat', 'pin', 'stick', 'muslim', 'scum', 'eyes']\n",
            "Text after Lemmatization: ['girl', 'sort', 'hat', 'pin', 'stick', 'muslim', 'scum', 'eye']\n",
            "Final pre-processed text: girl sort hat pin stick muslim scum eye\n",
            "Text after removing HTML tags: yep the us census bureau officially classifies arabs and jews as white\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yep the us census bureau officially classifies arabs and jews as white\n",
            "Text after tokenization: ['yep', 'the', 'us', 'census', 'bureau', 'officially', 'classifies', 'arabs', 'and', 'jews', 'as', 'white']\n",
            "Text after removing stop words: ['yep', 'us', 'census', 'bureau', 'officially', 'classifies', 'arabs', 'jews', 'white']\n",
            "Text after Lemmatization: ['yep', 'u', 'census', 'bureau', 'officially', 'classifies', 'arab', 'jew', 'white']\n",
            "Final pre-processed text: yep u census bureau officially classifies arab jew white\n",
            "Text after removing HTML tags: yeah agreed totally different to genuine sub saharan refugees who are mainly christian\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yeah agreed totally different to genuine sub saharan refugees who are mainly christian\n",
            "Text after tokenization: ['yeah', 'agreed', 'totally', 'different', 'to', 'genuine', 'sub', 'saharan', 'refugees', 'who', 'are', 'mainly', 'christian']\n",
            "Text after removing stop words: ['yeah', 'agreed', 'totally', 'different', 'genuine', 'sub', 'saharan', 'refugees', 'mainly', 'christian']\n",
            "Text after Lemmatization: ['yeah', 'agreed', 'totally', 'different', 'genuine', 'sub', 'saharan', 'refugee', 'mainly', 'christian']\n",
            "Final pre-processed text: yeah agreed totally different genuine sub saharan refugee mainly christian\n",
            "Text after removing HTML tags: fat retarded liberals are stuffing their faces with your food stamp money right now and they hate your guts\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fat retarded liberals are stuffing their faces with your food stamp money right now and they hate your guts\n",
            "Text after tokenization: ['fat', 'retarded', 'liberals', 'are', 'stuffing', 'their', 'faces', 'with', 'your', 'food', 'stamp', 'money', 'right', 'now', 'and', 'they', 'hate', 'your', 'guts']\n",
            "Text after removing stop words: ['fat', 'retarded', 'liberals', 'stuffing', 'faces', 'food', 'stamp', 'money', 'right', 'hate', 'guts']\n",
            "Text after Lemmatization: ['fat', 'retarded', 'liberal', 'stuffing', 'face', 'food', 'stamp', 'money', 'right', 'hate', 'gut']\n",
            "Final pre-processed text: fat retarded liberal stuffing face food stamp money right hate gut\n",
            "Text after removing HTML tags: really you could try bitch 🙂\n",
            "Text after removing non-alphabetic characters and converting to lowercase: really you could try bitch  \n",
            "Text after tokenization: ['really', 'you', 'could', 'try', 'bitch']\n",
            "Text after removing stop words: ['really', 'could', 'try', 'bitch']\n",
            "Text after Lemmatization: ['really', 'could', 'try', 'bitch']\n",
            "Final pre-processed text: really could try bitch\n",
            "Text after removing HTML tags: they will pay to have the holocaust industry accuse them of atrocities just like takes place in poland\n",
            "Text after removing non-alphabetic characters and converting to lowercase: they will pay to have the holocaust industry accuse them of atrocities just like takes place in poland\n",
            "Text after tokenization: ['they', 'will', 'pay', 'to', 'have', 'the', 'holocaust', 'industry', 'accuse', 'them', 'of', 'atrocities', 'just', 'like', 'takes', 'place', 'in', 'poland']\n",
            "Text after removing stop words: ['pay', 'holocaust', 'industry', 'accuse', 'atrocities', 'like', 'takes', 'place', 'poland']\n",
            "Text after Lemmatization: ['pay', 'holocaust', 'industry', 'accuse', 'atrocity', 'like', 'take', 'place', 'poland']\n",
            "Final pre-processed text: pay holocaust industry accuse atrocity like take place poland\n",
            "Text after removing HTML tags: islam exists its a problem and should be erased period dont defend sand niggers nothing is more disgusting than someone who defends these animals all muslims are still muslim and all muslims should die by the sword of god deusvult\n",
            "Text after removing non-alphabetic characters and converting to lowercase: islam exists its a problem and should be erased period dont defend sand niggers nothing is more disgusting than someone who defends these animals all muslims are still muslim and all muslims should die by the sword of god deusvult\n",
            "Text after tokenization: ['islam', 'exists', 'its', 'a', 'problem', 'and', 'should', 'be', 'erased', 'period', 'dont', 'defend', 'sand', 'niggers', 'nothing', 'is', 'more', 'disgusting', 'than', 'someone', 'who', 'defends', 'these', 'animals', 'all', 'muslims', 'are', 'still', 'muslim', 'and', 'all', 'muslims', 'should', 'die', 'by', 'the', 'sword', 'of', 'god', 'deusvult']\n",
            "Text after removing stop words: ['islam', 'exists', 'problem', 'erased', 'period', 'dont', 'defend', 'sand', 'niggers', 'nothing', 'disgusting', 'someone', 'defends', 'animals', 'muslims', 'still', 'muslim', 'muslims', 'die', 'sword', 'god', 'deusvult']\n",
            "Text after Lemmatization: ['islam', 'exists', 'problem', 'erased', 'period', 'dont', 'defend', 'sand', 'nigger', 'nothing', 'disgusting', 'someone', 'defends', 'animal', 'muslim', 'still', 'muslim', 'muslim', 'die', 'sword', 'god', 'deusvult']\n",
            "Final pre-processed text: islam exists problem erased period dont defend sand nigger nothing disgusting someone defends animal muslim still muslim muslim die sword god deusvult\n",
            "Text after removing HTML tags: defending sand niggers top kek\n",
            "Text after removing non-alphabetic characters and converting to lowercase: defending sand niggers top kek\n",
            "Text after tokenization: ['defending', 'sand', 'niggers', 'top', 'kek']\n",
            "Text after removing stop words: ['defending', 'sand', 'niggers', 'top', 'kek']\n",
            "Text after Lemmatization: ['defending', 'sand', 'nigger', 'top', 'kek']\n",
            "Final pre-processed text: defending sand nigger top kek\n",
            "Text after removing HTML tags: well if we all go around killing people our system devolves from order to chaos ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: well if we all go around killing people our system devolves from order to chaos ghetto\n",
            "Text after tokenization: ['well', 'if', 'we', 'all', 'go', 'around', 'killing', 'people', 'our', 'system', 'devolves', 'from', 'order', 'to', 'chaos', 'ghetto']\n",
            "Text after removing stop words: ['well', 'go', 'around', 'killing', 'people', 'system', 'devolves', 'order', 'chaos', 'ghetto']\n",
            "Text after Lemmatization: ['well', 'go', 'around', 'killing', 'people', 'system', 'devolves', 'order', 'chaos', 'ghetto']\n",
            "Final pre-processed text: well go around killing people system devolves order chaos ghetto\n",
            "Text after removing HTML tags: justin trudeau is a deeply retarded asshole canada is fucked with this idiot in charge\n",
            "Text after removing non-alphabetic characters and converting to lowercase: justin trudeau is a deeply retarded asshole canada is fucked with this idiot in charge\n",
            "Text after tokenization: ['justin', 'trudeau', 'is', 'a', 'deeply', 'retarded', 'asshole', 'canada', 'is', 'fucked', 'with', 'this', 'idiot', 'in', 'charge']\n",
            "Text after removing stop words: ['justin', 'trudeau', 'deeply', 'retarded', 'asshole', 'canada', 'fucked', 'idiot', 'charge']\n",
            "Text after Lemmatization: ['justin', 'trudeau', 'deeply', 'retarded', 'asshole', 'canada', 'fucked', 'idiot', 'charge']\n",
            "Final pre-processed text: justin trudeau deeply retarded asshole canada fucked idiot charge\n",
            "Text after removing HTML tags:  maybe he was referring to young immigrants that he and his peso friends can molest after all muslims accept sex with children\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  maybe he was referring to young immigrants that he and his peso friends can molest after all muslims accept sex with children\n",
            "Text after tokenization: ['maybe', 'he', 'was', 'referring', 'to', 'young', 'immigrants', 'that', 'he', 'and', 'his', 'peso', 'friends', 'can', 'molest', 'after', 'all', 'muslims', 'accept', 'sex', 'with', 'children']\n",
            "Text after removing stop words: ['maybe', 'referring', 'young', 'immigrants', 'peso', 'friends', 'molest', 'muslims', 'accept', 'sex', 'children']\n",
            "Text after Lemmatization: ['maybe', 'referring', 'young', 'immigrant', 'peso', 'friend', 'molest', 'muslim', 'accept', 'sex', 'child']\n",
            "Final pre-processed text: maybe referring young immigrant peso friend molest muslim accept sex child\n",
            "Text after removing HTML tags: the muslim woman who refused the garb\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the muslim woman who refused the garb\n",
            "Text after tokenization: ['the', 'muslim', 'woman', 'who', 'refused', 'the', 'garb']\n",
            "Text after removing stop words: ['muslim', 'woman', 'refused', 'garb']\n",
            "Text after Lemmatization: ['muslim', 'woman', 'refused', 'garb']\n",
            "Final pre-processed text: muslim woman refused garb\n",
            "Text after removing HTML tags: i agree some people in america do not know the truths about this political system of sharia law i do not want that in america i will not accept such savagry on women and children\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i agree some people in america do not know the truths about this political system of sharia law i do not want that in america i will not accept such savagry on women and children\n",
            "Text after tokenization: ['i', 'agree', 'some', 'people', 'in', 'america', 'do', 'not', 'know', 'the', 'truths', 'about', 'this', 'political', 'system', 'of', 'sharia', 'law', 'i', 'do', 'not', 'want', 'that', 'in', 'america', 'i', 'will', 'not', 'accept', 'such', 'savagry', 'on', 'women', 'and', 'children']\n",
            "Text after removing stop words: ['agree', 'people', 'america', 'know', 'truths', 'political', 'system', 'sharia', 'law', 'want', 'america', 'accept', 'savagry', 'women', 'children']\n",
            "Text after Lemmatization: ['agree', 'people', 'america', 'know', 'truth', 'political', 'system', 'sharia', 'law', 'want', 'america', 'accept', 'savagry', 'woman', 'child']\n",
            "Final pre-processed text: agree people america know truth political system sharia law want america accept savagry woman child\n",
            "Text after removing HTML tags: imagine being so retarded you think calling nationalsocialism leftist is an argument while ignoring the literal jewish communist foundations your ideology is based on\n",
            "Text after removing non-alphabetic characters and converting to lowercase: imagine being so retarded you think calling nationalsocialism leftist is an argument while ignoring the literal jewish communist foundations your ideology is based on\n",
            "Text after tokenization: ['imagine', 'being', 'so', 'retarded', 'you', 'think', 'calling', 'nationalsocialism', 'leftist', 'is', 'an', 'argument', 'while', 'ignoring', 'the', 'literal', 'jewish', 'communist', 'foundations', 'your', 'ideology', 'is', 'based', 'on']\n",
            "Text after removing stop words: ['imagine', 'retarded', 'think', 'calling', 'nationalsocialism', 'leftist', 'argument', 'ignoring', 'literal', 'jewish', 'communist', 'foundations', 'ideology', 'based']\n",
            "Text after Lemmatization: ['imagine', 'retarded', 'think', 'calling', 'nationalsocialism', 'leftist', 'argument', 'ignoring', 'literal', 'jewish', 'communist', 'foundation', 'ideology', 'based']\n",
            "Final pre-processed text: imagine retarded think calling nationalsocialism leftist argument ignoring literal jewish communist foundation ideology based\n",
            "Text after removing HTML tags:  she should go meet muslims where sharia law is the rule and hang out there and then come back and tell us how she was treated wearing something as simple as she wearing in that pic she d be gang raped and beaten then charged with being a slut\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  she should go meet muslims where sharia law is the rule and hang out there and then come back and tell us how she was treated wearing something as simple as she wearing in that pic she d be gang raped and beaten then charged with being a slut\n",
            "Text after tokenization: ['she', 'should', 'go', 'meet', 'muslims', 'where', 'sharia', 'law', 'is', 'the', 'rule', 'and', 'hang', 'out', 'there', 'and', 'then', 'come', 'back', 'and', 'tell', 'us', 'how', 'she', 'was', 'treated', 'wearing', 'something', 'as', 'simple', 'as', 'she', 'wearing', 'in', 'that', 'pic', 'she', 'd', 'be', 'gang', 'raped', 'and', 'beaten', 'then', 'charged', 'with', 'being', 'a', 'slut']\n",
            "Text after removing stop words: ['go', 'meet', 'muslims', 'sharia', 'law', 'rule', 'hang', 'come', 'back', 'tell', 'us', 'treated', 'wearing', 'something', 'simple', 'wearing', 'pic', 'gang', 'raped', 'beaten', 'charged', 'slut']\n",
            "Text after Lemmatization: ['go', 'meet', 'muslim', 'sharia', 'law', 'rule', 'hang', 'come', 'back', 'tell', 'u', 'treated', 'wearing', 'something', 'simple', 'wearing', 'pic', 'gang', 'raped', 'beaten', 'charged', 'slut']\n",
            "Final pre-processed text: go meet muslim sharia law rule hang come back tell u treated wearing something simple wearing pic gang raped beaten charged slut\n",
            "Text after removing HTML tags: but thats good tho who doesnt want to sterilise niggers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: but thats good tho who doesnt want to sterilise niggers\n",
            "Text after tokenization: ['but', 'thats', 'good', 'tho', 'who', 'doesnt', 'want', 'to', 'sterilise', 'niggers']\n",
            "Text after removing stop words: ['thats', 'good', 'tho', 'doesnt', 'want', 'sterilise', 'niggers']\n",
            "Text after Lemmatization: ['thats', 'good', 'tho', 'doesnt', 'want', 'sterilise', 'nigger']\n",
            "Final pre-processed text: thats good tho doesnt want sterilise nigger\n",
            "Text after removing HTML tags: al jizz era wants the white man down we will hang every sand nigger from every street lamp in town before that happens\n",
            "Text after removing non-alphabetic characters and converting to lowercase: al jizz era wants the white man down we will hang every sand nigger from every street lamp in town before that happens\n",
            "Text after tokenization: ['al', 'jizz', 'era', 'wants', 'the', 'white', 'man', 'down', 'we', 'will', 'hang', 'every', 'sand', 'nigger', 'from', 'every', 'street', 'lamp', 'in', 'town', 'before', 'that', 'happens']\n",
            "Text after removing stop words: ['al', 'jizz', 'era', 'wants', 'white', 'man', 'hang', 'every', 'sand', 'nigger', 'every', 'street', 'lamp', 'town', 'happens']\n",
            "Text after Lemmatization: ['al', 'jizz', 'era', 'want', 'white', 'man', 'hang', 'every', 'sand', 'nigger', 'every', 'street', 'lamp', 'town', 'happens']\n",
            "Final pre-processed text: al jizz era want white man hang every sand nigger every street lamp town happens\n",
            "Text after removing HTML tags: flipping channels hit on this wwe wtf is this like gay soft porn\n",
            "Text after removing non-alphabetic characters and converting to lowercase: flipping channels hit on this wwe wtf is this like gay soft porn\n",
            "Text after tokenization: ['flipping', 'channels', 'hit', 'on', 'this', 'wwe', 'wtf', 'is', 'this', 'like', 'gay', 'soft', 'porn']\n",
            "Text after removing stop words: ['flipping', 'channels', 'hit', 'wwe', 'wtf', 'like', 'gay', 'soft', 'porn']\n",
            "Text after Lemmatization: ['flipping', 'channel', 'hit', 'wwe', 'wtf', 'like', 'gay', 'soft', 'porn']\n",
            "Final pre-processed text: flipping channel hit wwe wtf like gay soft porn\n",
            "Text after removing HTML tags: this is why jews and niggers hate cultural appropriation mr bond makes better rap music getting shoahed from youtube twice a month than this slob makes with tens of thousands of white guilt dollars\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this is why jews and niggers hate cultural appropriation mr bond makes better rap music getting shoahed from youtube twice a month than this slob makes with tens of thousands of white guilt dollars\n",
            "Text after tokenization: ['this', 'is', 'why', 'jews', 'and', 'niggers', 'hate', 'cultural', 'appropriation', 'mr', 'bond', 'makes', 'better', 'rap', 'music', 'getting', 'shoahed', 'from', 'youtube', 'twice', 'a', 'month', 'than', 'this', 'slob', 'makes', 'with', 'tens', 'of', 'thousands', 'of', 'white', 'guilt', 'dollars']\n",
            "Text after removing stop words: ['jews', 'niggers', 'hate', 'cultural', 'appropriation', 'mr', 'bond', 'makes', 'better', 'rap', 'music', 'getting', 'shoahed', 'youtube', 'twice', 'month', 'slob', 'makes', 'tens', 'thousands', 'white', 'guilt', 'dollars']\n",
            "Text after Lemmatization: ['jew', 'nigger', 'hate', 'cultural', 'appropriation', 'mr', 'bond', 'make', 'better', 'rap', 'music', 'getting', 'shoahed', 'youtube', 'twice', 'month', 'slob', 'make', 'ten', 'thousand', 'white', 'guilt', 'dollar']\n",
            "Final pre-processed text: jew nigger hate cultural appropriation mr bond make better rap music getting shoahed youtube twice month slob make ten thousand white guilt dollar\n",
            "Text after removing HTML tags: black panther is bullshit the reality is darker than any african thousands of years later they are still killing each other at an alarming pace and they have turned south africa into a ghetto where murdering white families is their latest endeavor\n",
            "Text after removing non-alphabetic characters and converting to lowercase: black panther is bullshit the reality is darker than any african thousands of years later they are still killing each other at an alarming pace and they have turned south africa into a ghetto where murdering white families is their latest endeavor\n",
            "Text after tokenization: ['black', 'panther', 'is', 'bullshit', 'the', 'reality', 'is', 'darker', 'than', 'any', 'african', 'thousands', 'of', 'years', 'later', 'they', 'are', 'still', 'killing', 'each', 'other', 'at', 'an', 'alarming', 'pace', 'and', 'they', 'have', 'turned', 'south', 'africa', 'into', 'a', 'ghetto', 'where', 'murdering', 'white', 'families', 'is', 'their', 'latest', 'endeavor']\n",
            "Text after removing stop words: ['black', 'panther', 'bullshit', 'reality', 'darker', 'african', 'thousands', 'years', 'later', 'still', 'killing', 'alarming', 'pace', 'turned', 'south', 'africa', 'ghetto', 'murdering', 'white', 'families', 'latest', 'endeavor']\n",
            "Text after Lemmatization: ['black', 'panther', 'bullshit', 'reality', 'darker', 'african', 'thousand', 'year', 'later', 'still', 'killing', 'alarming', 'pace', 'turned', 'south', 'africa', 'ghetto', 'murdering', 'white', 'family', 'latest', 'endeavor']\n",
            "Final pre-processed text: black panther bullshit reality darker african thousand year later still killing alarming pace turned south africa ghetto murdering white family latest endeavor\n",
            "Text after removing HTML tags: new testament do you even read any idea why christian women cover their hair until  but i say unto you that whosoever looketh on a woman to lust after her hath committed adultery with her already in his heart matthew  yes this applies to dispensationalists\n",
            "Text after removing non-alphabetic characters and converting to lowercase: new testament do you even read any idea why christian women cover their hair until  but i say unto you that whosoever looketh on a woman to lust after her hath committed adultery with her already in his heart matthew  yes this applies to dispensationalists\n",
            "Text after tokenization: ['new', 'testament', 'do', 'you', 'even', 'read', 'any', 'idea', 'why', 'christian', 'women', 'cover', 'their', 'hair', 'until', 'but', 'i', 'say', 'unto', 'you', 'that', 'whosoever', 'looketh', 'on', 'a', 'woman', 'to', 'lust', 'after', 'her', 'hath', 'committed', 'adultery', 'with', 'her', 'already', 'in', 'his', 'heart', 'matthew', 'yes', 'this', 'applies', 'to', 'dispensationalists']\n",
            "Text after removing stop words: ['new', 'testament', 'even', 'read', 'idea', 'christian', 'women', 'cover', 'hair', 'say', 'unto', 'whosoever', 'looketh', 'woman', 'lust', 'hath', 'committed', 'adultery', 'already', 'heart', 'matthew', 'yes', 'applies', 'dispensationalists']\n",
            "Text after Lemmatization: ['new', 'testament', 'even', 'read', 'idea', 'christian', 'woman', 'cover', 'hair', 'say', 'unto', 'whosoever', 'looketh', 'woman', 'lust', 'hath', 'committed', 'adultery', 'already', 'heart', 'matthew', 'yes', 'applies', 'dispensationalists']\n",
            "Final pre-processed text: new testament even read idea christian woman cover hair say unto whosoever looketh woman lust hath committed adultery already heart matthew yes applies dispensationalists\n",
            "Text after removing HTML tags: lol the ns germans are all gay they are too busy having sex with each other while their women are being raped \n",
            "Text after removing non-alphabetic characters and converting to lowercase: lol the ns germans are all gay they are too busy having sex with each other while their women are being raped \n",
            "Text after tokenization: ['lol', 'the', 'ns', 'germans', 'are', 'all', 'gay', 'they', 'are', 'too', 'busy', 'having', 'sex', 'with', 'each', 'other', 'while', 'their', 'women', 'are', 'being', 'raped']\n",
            "Text after removing stop words: ['lol', 'ns', 'germans', 'gay', 'busy', 'sex', 'women', 'raped']\n",
            "Text after Lemmatization: ['lol', 'n', 'german', 'gay', 'busy', 'sex', 'woman', 'raped']\n",
            "Final pre-processed text: lol n german gay busy sex woman raped\n",
            "Text after removing HTML tags: lol faggots can not throw a grenade  meters\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lol faggots can not throw a grenade  meters\n",
            "Text after tokenization: ['lol', 'faggots', 'can', 'not', 'throw', 'a', 'grenade', 'meters']\n",
            "Text after removing stop words: ['lol', 'faggots', 'throw', 'grenade', 'meters']\n",
            "Text after Lemmatization: ['lol', 'faggot', 'throw', 'grenade', 'meter']\n",
            "Final pre-processed text: lol faggot throw grenade meter\n",
            "Text after removing HTML tags: for all the taxes they take out of my paycheck the least they could do is send me a picture of the ghetto family i am supporting to hang on my fridge\n",
            "Text after removing non-alphabetic characters and converting to lowercase: for all the taxes they take out of my paycheck the least they could do is send me a picture of the ghetto family i am supporting to hang on my fridge\n",
            "Text after tokenization: ['for', 'all', 'the', 'taxes', 'they', 'take', 'out', 'of', 'my', 'paycheck', 'the', 'least', 'they', 'could', 'do', 'is', 'send', 'me', 'a', 'picture', 'of', 'the', 'ghetto', 'family', 'i', 'am', 'supporting', 'to', 'hang', 'on', 'my', 'fridge']\n",
            "Text after removing stop words: ['taxes', 'take', 'paycheck', 'least', 'could', 'send', 'picture', 'ghetto', 'family', 'supporting', 'hang', 'fridge']\n",
            "Text after Lemmatization: ['tax', 'take', 'paycheck', 'least', 'could', 'send', 'picture', 'ghetto', 'family', 'supporting', 'hang', 'fridge']\n",
            "Final pre-processed text: tax take paycheck least could send picture ghetto family supporting hang fridge\n",
            "Text after removing HTML tags: question do greeks know how to tie nooses because i have yet to hear anything about niggers or sand niggers rapefugees getting lynched 😂\n",
            "Text after removing non-alphabetic characters and converting to lowercase: question do greeks know how to tie nooses because i have yet to hear anything about niggers or sand niggers rapefugees getting lynched  \n",
            "Text after tokenization: ['question', 'do', 'greeks', 'know', 'how', 'to', 'tie', 'nooses', 'because', 'i', 'have', 'yet', 'to', 'hear', 'anything', 'about', 'niggers', 'or', 'sand', 'niggers', 'rapefugees', 'getting', 'lynched']\n",
            "Text after removing stop words: ['question', 'greeks', 'know', 'tie', 'nooses', 'yet', 'hear', 'anything', 'niggers', 'sand', 'niggers', 'rapefugees', 'getting', 'lynched']\n",
            "Text after Lemmatization: ['question', 'greek', 'know', 'tie', 'noose', 'yet', 'hear', 'anything', 'nigger', 'sand', 'nigger', 'rapefugees', 'getting', 'lynched']\n",
            "Final pre-processed text: question greek know tie noose yet hear anything nigger sand nigger rapefugees getting lynched\n",
            "Text after removing HTML tags: well he needs to stop pimping them out then or these hoes have become too uppity flashmaster trump needs to discipline them like any good ghetto pimp would\n",
            "Text after removing non-alphabetic characters and converting to lowercase: well he needs to stop pimping them out then or these hoes have become too uppity flashmaster trump needs to discipline them like any good ghetto pimp would\n",
            "Text after tokenization: ['well', 'he', 'needs', 'to', 'stop', 'pimping', 'them', 'out', 'then', 'or', 'these', 'hoes', 'have', 'become', 'too', 'uppity', 'flashmaster', 'trump', 'needs', 'to', 'discipline', 'them', 'like', 'any', 'good', 'ghetto', 'pimp', 'would']\n",
            "Text after removing stop words: ['well', 'needs', 'stop', 'pimping', 'hoes', 'become', 'uppity', 'flashmaster', 'trump', 'needs', 'discipline', 'like', 'good', 'ghetto', 'pimp', 'would']\n",
            "Text after Lemmatization: ['well', 'need', 'stop', 'pimping', 'hoe', 'become', 'uppity', 'flashmaster', 'trump', 'need', 'discipline', 'like', 'good', 'ghetto', 'pimp', 'would']\n",
            "Final pre-processed text: well need stop pimping hoe become uppity flashmaster trump need discipline like good ghetto pimp would\n",
            "Text after removing HTML tags: more telling was the photo on the bridge with all but one person desperately trying to help the one that did not was a muslim walking past talking on the phone did not even care\n",
            "Text after removing non-alphabetic characters and converting to lowercase: more telling was the photo on the bridge with all but one person desperately trying to help the one that did not was a muslim walking past talking on the phone did not even care\n",
            "Text after tokenization: ['more', 'telling', 'was', 'the', 'photo', 'on', 'the', 'bridge', 'with', 'all', 'but', 'one', 'person', 'desperately', 'trying', 'to', 'help', 'the', 'one', 'that', 'did', 'not', 'was', 'a', 'muslim', 'walking', 'past', 'talking', 'on', 'the', 'phone', 'did', 'not', 'even', 'care']\n",
            "Text after removing stop words: ['telling', 'photo', 'bridge', 'one', 'person', 'desperately', 'trying', 'help', 'one', 'muslim', 'walking', 'past', 'talking', 'phone', 'even', 'care']\n",
            "Text after Lemmatization: ['telling', 'photo', 'bridge', 'one', 'person', 'desperately', 'trying', 'help', 'one', 'muslim', 'walking', 'past', 'talking', 'phone', 'even', 'care']\n",
            "Final pre-processed text: telling photo bridge one person desperately trying help one muslim walking past talking phone even care\n",
            "Text after removing HTML tags: i too hate the current shitty times when black races are everywhere and the new anti white gestapo laws hunt us down i do not want to see white sluts and black nigs arabs couples on the street makes me sick\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i too hate the current shitty times when black races are everywhere and the new anti white gestapo laws hunt us down i do not want to see white sluts and black nigs arabs couples on the street makes me sick\n",
            "Text after tokenization: ['i', 'too', 'hate', 'the', 'current', 'shitty', 'times', 'when', 'black', 'races', 'are', 'everywhere', 'and', 'the', 'new', 'anti', 'white', 'gestapo', 'laws', 'hunt', 'us', 'down', 'i', 'do', 'not', 'want', 'to', 'see', 'white', 'sluts', 'and', 'black', 'nigs', 'arabs', 'couples', 'on', 'the', 'street', 'makes', 'me', 'sick']\n",
            "Text after removing stop words: ['hate', 'current', 'shitty', 'times', 'black', 'races', 'everywhere', 'new', 'anti', 'white', 'gestapo', 'laws', 'hunt', 'us', 'want', 'see', 'white', 'sluts', 'black', 'nigs', 'arabs', 'couples', 'street', 'makes', 'sick']\n",
            "Text after Lemmatization: ['hate', 'current', 'shitty', 'time', 'black', 'race', 'everywhere', 'new', 'anti', 'white', 'gestapo', 'law', 'hunt', 'u', 'want', 'see', 'white', 'slut', 'black', 'nigs', 'arab', 'couple', 'street', 'make', 'sick']\n",
            "Final pre-processed text: hate current shitty time black race everywhere new anti white gestapo law hunt u want see white slut black nigs arab couple street make sick\n",
            "Text after removing HTML tags: bronson woukd shoot your withered bugman face you one man ghetto\n",
            "Text after removing non-alphabetic characters and converting to lowercase: bronson woukd shoot your withered bugman face you one man ghetto\n",
            "Text after tokenization: ['bronson', 'woukd', 'shoot', 'your', 'withered', 'bugman', 'face', 'you', 'one', 'man', 'ghetto']\n",
            "Text after removing stop words: ['bronson', 'woukd', 'shoot', 'withered', 'bugman', 'face', 'one', 'man', 'ghetto']\n",
            "Text after Lemmatization: ['bronson', 'woukd', 'shoot', 'withered', 'bugman', 'face', 'one', 'man', 'ghetto']\n",
            "Final pre-processed text: bronson woukd shoot withered bugman face one man ghetto\n",
            "Text after removing HTML tags: should vigilantes hunt down and execute sand nigger rapist and general rapist and pedos since the governments do not do enough\n",
            "Text after removing non-alphabetic characters and converting to lowercase: should vigilantes hunt down and execute sand nigger rapist and general rapist and pedos since the governments do not do enough\n",
            "Text after tokenization: ['should', 'vigilantes', 'hunt', 'down', 'and', 'execute', 'sand', 'nigger', 'rapist', 'and', 'general', 'rapist', 'and', 'pedos', 'since', 'the', 'governments', 'do', 'not', 'do', 'enough']\n",
            "Text after removing stop words: ['vigilantes', 'hunt', 'execute', 'sand', 'nigger', 'rapist', 'general', 'rapist', 'pedos', 'since', 'governments', 'enough']\n",
            "Text after Lemmatization: ['vigilante', 'hunt', 'execute', 'sand', 'nigger', 'rapist', 'general', 'rapist', 'pedos', 'since', 'government', 'enough']\n",
            "Final pre-processed text: vigilante hunt execute sand nigger rapist general rapist pedos since government enough\n",
            "Text after removing HTML tags: cheeky chinaman\n",
            "Text after removing non-alphabetic characters and converting to lowercase: cheeky chinaman\n",
            "Text after tokenization: ['cheeky', 'chinaman']\n",
            "Text after removing stop words: ['cheeky', 'chinaman']\n",
            "Text after Lemmatization: ['cheeky', 'chinaman']\n",
            "Final pre-processed text: cheeky chinaman\n",
            "Text after removing HTML tags: proof sand niggers have modernized look at this nice selfie\n",
            "Text after removing non-alphabetic characters and converting to lowercase: proof sand niggers have modernized look at this nice selfie\n",
            "Text after tokenization: ['proof', 'sand', 'niggers', 'have', 'modernized', 'look', 'at', 'this', 'nice', 'selfie']\n",
            "Text after removing stop words: ['proof', 'sand', 'niggers', 'modernized', 'look', 'nice', 'selfie']\n",
            "Text after Lemmatization: ['proof', 'sand', 'nigger', 'modernized', 'look', 'nice', 'selfie']\n",
            "Final pre-processed text: proof sand nigger modernized look nice selfie\n",
            "Text after removing HTML tags: this propaganda campaign worked with gay marriage the supreme court overturned the will of the people and christians are now being persecuted by their own government that was always the gay marriage endgame and a helpless populace is gun control endgame\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this propaganda campaign worked with gay marriage the supreme court overturned the will of the people and christians are now being persecuted by their own government that was always the gay marriage endgame and a helpless populace is gun control endgame\n",
            "Text after tokenization: ['this', 'propaganda', 'campaign', 'worked', 'with', 'gay', 'marriage', 'the', 'supreme', 'court', 'overturned', 'the', 'will', 'of', 'the', 'people', 'and', 'christians', 'are', 'now', 'being', 'persecuted', 'by', 'their', 'own', 'government', 'that', 'was', 'always', 'the', 'gay', 'marriage', 'endgame', 'and', 'a', 'helpless', 'populace', 'is', 'gun', 'control', 'endgame']\n",
            "Text after removing stop words: ['propaganda', 'campaign', 'worked', 'gay', 'marriage', 'supreme', 'court', 'overturned', 'people', 'christians', 'persecuted', 'government', 'always', 'gay', 'marriage', 'endgame', 'helpless', 'populace', 'gun', 'control', 'endgame']\n",
            "Text after Lemmatization: ['propaganda', 'campaign', 'worked', 'gay', 'marriage', 'supreme', 'court', 'overturned', 'people', 'christian', 'persecuted', 'government', 'always', 'gay', 'marriage', 'endgame', 'helpless', 'populace', 'gun', 'control', 'endgame']\n",
            "Final pre-processed text: propaganda campaign worked gay marriage supreme court overturned people christian persecuted government always gay marriage endgame helpless populace gun control endgame\n",
            "Text after removing HTML tags: no this is not the wall at block  in aushwitz been there myself and you are correct no holes this photo likely from the warsaw ghetto uprising\n",
            "Text after removing non-alphabetic characters and converting to lowercase: no this is not the wall at block  in aushwitz been there myself and you are correct no holes this photo likely from the warsaw ghetto uprising\n",
            "Text after tokenization: ['no', 'this', 'is', 'not', 'the', 'wall', 'at', 'block', 'in', 'aushwitz', 'been', 'there', 'myself', 'and', 'you', 'are', 'correct', 'no', 'holes', 'this', 'photo', 'likely', 'from', 'the', 'warsaw', 'ghetto', 'uprising']\n",
            "Text after removing stop words: ['wall', 'block', 'aushwitz', 'correct', 'holes', 'photo', 'likely', 'warsaw', 'ghetto', 'uprising']\n",
            "Text after Lemmatization: ['wall', 'block', 'aushwitz', 'correct', 'hole', 'photo', 'likely', 'warsaw', 'ghetto', 'uprising']\n",
            "Final pre-processed text: wall block aushwitz correct hole photo likely warsaw ghetto uprising\n",
            "Text after removing HTML tags: whoever made this thread has the big gay\n",
            "Text after removing non-alphabetic characters and converting to lowercase: whoever made this thread has the big gay\n",
            "Text after tokenization: ['whoever', 'made', 'this', 'thread', 'has', 'the', 'big', 'gay']\n",
            "Text after removing stop words: ['whoever', 'made', 'thread', 'big', 'gay']\n",
            "Text after Lemmatization: ['whoever', 'made', 'thread', 'big', 'gay']\n",
            "Final pre-processed text: whoever made thread big gay\n",
            "Text after removing HTML tags: oh and they wanted to disrupt the money going to the gop for the mid term elections that is why they have the shit stain kike kids saying on live tv anyone politician who takes money from the nra has blood on their hands\n",
            "Text after removing non-alphabetic characters and converting to lowercase: oh and they wanted to disrupt the money going to the gop for the mid term elections that is why they have the shit stain kike kids saying on live tv anyone politician who takes money from the nra has blood on their hands\n",
            "Text after tokenization: ['oh', 'and', 'they', 'wanted', 'to', 'disrupt', 'the', 'money', 'going', 'to', 'the', 'gop', 'for', 'the', 'mid', 'term', 'elections', 'that', 'is', 'why', 'they', 'have', 'the', 'shit', 'stain', 'kike', 'kids', 'saying', 'on', 'live', 'tv', 'anyone', 'politician', 'who', 'takes', 'money', 'from', 'the', 'nra', 'has', 'blood', 'on', 'their', 'hands']\n",
            "Text after removing stop words: ['oh', 'wanted', 'disrupt', 'money', 'going', 'gop', 'mid', 'term', 'elections', 'shit', 'stain', 'kike', 'kids', 'saying', 'live', 'tv', 'anyone', 'politician', 'takes', 'money', 'nra', 'blood', 'hands']\n",
            "Text after Lemmatization: ['oh', 'wanted', 'disrupt', 'money', 'going', 'gop', 'mid', 'term', 'election', 'shit', 'stain', 'kike', 'kid', 'saying', 'live', 'tv', 'anyone', 'politician', 'take', 'money', 'nra', 'blood', 'hand']\n",
            "Final pre-processed text: oh wanted disrupt money going gop mid term election shit stain kike kid saying live tv anyone politician take money nra blood hand\n",
            "Text after removing HTML tags: i remember that but did not know he was a kike\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i remember that but did not know he was a kike\n",
            "Text after tokenization: ['i', 'remember', 'that', 'but', 'did', 'not', 'know', 'he', 'was', 'a', 'kike']\n",
            "Text after removing stop words: ['remember', 'know', 'kike']\n",
            "Text after Lemmatization: ['remember', 'know', 'kike']\n",
            "Final pre-processed text: remember know kike\n",
            "Text after removing HTML tags: just throw a faggot in the mix\n",
            "Text after removing non-alphabetic characters and converting to lowercase: just throw a faggot in the mix\n",
            "Text after tokenization: ['just', 'throw', 'a', 'faggot', 'in', 'the', 'mix']\n",
            "Text after removing stop words: ['throw', 'faggot', 'mix']\n",
            "Text after Lemmatization: ['throw', 'faggot', 'mix']\n",
            "Final pre-processed text: throw faggot mix\n",
            "Text after removing HTML tags: i tell you what if they gave a fig about sex trafficking they would be cracking down on illegal immigration and ghetto communities where most of this stuff takes place they do not give a shit about girls in sexual slavery they just want a pr campaign for their nefarious purposes\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i tell you what if they gave a fig about sex trafficking they would be cracking down on illegal immigration and ghetto communities where most of this stuff takes place they do not give a shit about girls in sexual slavery they just want a pr campaign for their nefarious purposes\n",
            "Text after tokenization: ['i', 'tell', 'you', 'what', 'if', 'they', 'gave', 'a', 'fig', 'about', 'sex', 'trafficking', 'they', 'would', 'be', 'cracking', 'down', 'on', 'illegal', 'immigration', 'and', 'ghetto', 'communities', 'where', 'most', 'of', 'this', 'stuff', 'takes', 'place', 'they', 'do', 'not', 'give', 'a', 'shit', 'about', 'girls', 'in', 'sexual', 'slavery', 'they', 'just', 'want', 'a', 'pr', 'campaign', 'for', 'their', 'nefarious', 'purposes']\n",
            "Text after removing stop words: ['tell', 'gave', 'fig', 'sex', 'trafficking', 'would', 'cracking', 'illegal', 'immigration', 'ghetto', 'communities', 'stuff', 'takes', 'place', 'give', 'shit', 'girls', 'sexual', 'slavery', 'want', 'pr', 'campaign', 'nefarious', 'purposes']\n",
            "Text after Lemmatization: ['tell', 'gave', 'fig', 'sex', 'trafficking', 'would', 'cracking', 'illegal', 'immigration', 'ghetto', 'community', 'stuff', 'take', 'place', 'give', 'shit', 'girl', 'sexual', 'slavery', 'want', 'pr', 'campaign', 'nefarious', 'purpose']\n",
            "Final pre-processed text: tell gave fig sex trafficking would cracking illegal immigration ghetto community stuff take place give shit girl sexual slavery want pr campaign nefarious purpose\n",
            "Text after removing HTML tags: big fuckin deal both papa johns and the nfl are absolute shit for bugmen get a real friggin pizza and stop watching niggers play with their balls\n",
            "Text after removing non-alphabetic characters and converting to lowercase: big fuckin deal both papa johns and the nfl are absolute shit for bugmen get a real friggin pizza and stop watching niggers play with their balls\n",
            "Text after tokenization: ['big', 'fuckin', 'deal', 'both', 'papa', 'johns', 'and', 'the', 'nfl', 'are', 'absolute', 'shit', 'for', 'bugmen', 'get', 'a', 'real', 'friggin', 'pizza', 'and', 'stop', 'watching', 'niggers', 'play', 'with', 'their', 'balls']\n",
            "Text after removing stop words: ['big', 'fuckin', 'deal', 'papa', 'johns', 'nfl', 'absolute', 'shit', 'bugmen', 'get', 'real', 'friggin', 'pizza', 'stop', 'watching', 'niggers', 'play', 'balls']\n",
            "Text after Lemmatization: ['big', 'fuckin', 'deal', 'papa', 'john', 'nfl', 'absolute', 'shit', 'bugmen', 'get', 'real', 'friggin', 'pizza', 'stop', 'watching', 'nigger', 'play', 'ball']\n",
            "Final pre-processed text: big fuckin deal papa john nfl absolute shit bugmen get real friggin pizza stop watching nigger play ball\n",
            "Text after removing HTML tags: paul some jew or sandnigger is gonna buy gab just to purge you off of this platform so be careful\n",
            "Text after removing non-alphabetic characters and converting to lowercase: paul some jew or sandnigger is gonna buy gab just to purge you off of this platform so be careful\n",
            "Text after tokenization: ['paul', 'some', 'jew', 'or', 'sandnigger', 'is', 'gon', 'na', 'buy', 'gab', 'just', 'to', 'purge', 'you', 'off', 'of', 'this', 'platform', 'so', 'be', 'careful']\n",
            "Text after removing stop words: ['paul', 'jew', 'sandnigger', 'gon', 'na', 'buy', 'gab', 'purge', 'platform', 'careful']\n",
            "Text after Lemmatization: ['paul', 'jew', 'sandnigger', 'gon', 'na', 'buy', 'gab', 'purge', 'platform', 'careful']\n",
            "Final pre-processed text: paul jew sandnigger gon na buy gab purge platform careful\n",
            "Text after removing HTML tags: like that terrible violence of the american revolution that stopped the red coats from taking american guns we need to stop glorifying self defense and protect the lives and feelings of criminals\n",
            "Text after removing non-alphabetic characters and converting to lowercase: like that terrible violence of the american revolution that stopped the red coats from taking american guns we need to stop glorifying self defense and protect the lives and feelings of criminals\n",
            "Text after tokenization: ['like', 'that', 'terrible', 'violence', 'of', 'the', 'american', 'revolution', 'that', 'stopped', 'the', 'red', 'coats', 'from', 'taking', 'american', 'guns', 'we', 'need', 'to', 'stop', 'glorifying', 'self', 'defense', 'and', 'protect', 'the', 'lives', 'and', 'feelings', 'of', 'criminals']\n",
            "Text after removing stop words: ['like', 'terrible', 'violence', 'american', 'revolution', 'stopped', 'red', 'coats', 'taking', 'american', 'guns', 'need', 'stop', 'glorifying', 'self', 'defense', 'protect', 'lives', 'feelings', 'criminals']\n",
            "Text after Lemmatization: ['like', 'terrible', 'violence', 'american', 'revolution', 'stopped', 'red', 'coat', 'taking', 'american', 'gun', 'need', 'stop', 'glorifying', 'self', 'defense', 'protect', 'life', 'feeling', 'criminal']\n",
            "Final pre-processed text: like terrible violence american revolution stopped red coat taking american gun need stop glorifying self defense protect life feeling criminal\n",
            "Text after removing HTML tags: if you give up your guns their boots will be stomped on your face forever inbred muslims and tranny sluts that is until you are dead then the muzzies will throw their allies off roof tops\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if you give up your guns their boots will be stomped on your face forever inbred muslims and tranny sluts that is until you are dead then the muzzies will throw their allies off roof tops\n",
            "Text after tokenization: ['if', 'you', 'give', 'up', 'your', 'guns', 'their', 'boots', 'will', 'be', 'stomped', 'on', 'your', 'face', 'forever', 'inbred', 'muslims', 'and', 'tranny', 'sluts', 'that', 'is', 'until', 'you', 'are', 'dead', 'then', 'the', 'muzzies', 'will', 'throw', 'their', 'allies', 'off', 'roof', 'tops']\n",
            "Text after removing stop words: ['give', 'guns', 'boots', 'stomped', 'face', 'forever', 'inbred', 'muslims', 'tranny', 'sluts', 'dead', 'muzzies', 'throw', 'allies', 'roof', 'tops']\n",
            "Text after Lemmatization: ['give', 'gun', 'boot', 'stomped', 'face', 'forever', 'inbred', 'muslim', 'tranny', 'slut', 'dead', 'muzzies', 'throw', 'ally', 'roof', 'top']\n",
            "Final pre-processed text: give gun boot stomped face forever inbred muslim tranny slut dead muzzies throw ally roof top\n",
            "Text after removing HTML tags: then why are all the white girls being raped and murdered by blacks\n",
            "Text after removing non-alphabetic characters and converting to lowercase: then why are all the white girls being raped and murdered by blacks\n",
            "Text after tokenization: ['then', 'why', 'are', 'all', 'the', 'white', 'girls', 'being', 'raped', 'and', 'murdered', 'by', 'blacks']\n",
            "Text after removing stop words: ['white', 'girls', 'raped', 'murdered', 'blacks']\n",
            "Text after Lemmatization: ['white', 'girl', 'raped', 'murdered', 'black']\n",
            "Final pre-processed text: white girl raped murdered black\n",
            "Text after removing HTML tags: i am wondering why asian people are not offended that they are describing middle eastern goat fucking muslimes as asians in the uk\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am wondering why asian people are not offended that they are describing middle eastern goat fucking muslimes as asians in the uk\n",
            "Text after tokenization: ['i', 'am', 'wondering', 'why', 'asian', 'people', 'are', 'not', 'offended', 'that', 'they', 'are', 'describing', 'middle', 'eastern', 'goat', 'fucking', 'muslimes', 'as', 'asians', 'in', 'the', 'uk']\n",
            "Text after removing stop words: ['wondering', 'asian', 'people', 'offended', 'describing', 'middle', 'eastern', 'goat', 'fucking', 'muslimes', 'asians', 'uk']\n",
            "Text after Lemmatization: ['wondering', 'asian', 'people', 'offended', 'describing', 'middle', 'eastern', 'goat', 'fucking', 'muslimes', 'asian', 'uk']\n",
            "Final pre-processed text: wondering asian people offended describing middle eastern goat fucking muslimes asian uk\n",
            "Text after removing HTML tags: i have the merits to become an italian citizen dual and i say deport all sand niggers immediately\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i have the merits to become an italian citizen dual and i say deport all sand niggers immediately\n",
            "Text after tokenization: ['i', 'have', 'the', 'merits', 'to', 'become', 'an', 'italian', 'citizen', 'dual', 'and', 'i', 'say', 'deport', 'all', 'sand', 'niggers', 'immediately']\n",
            "Text after removing stop words: ['merits', 'become', 'italian', 'citizen', 'dual', 'say', 'deport', 'sand', 'niggers', 'immediately']\n",
            "Text after Lemmatization: ['merit', 'become', 'italian', 'citizen', 'dual', 'say', 'deport', 'sand', 'nigger', 'immediately']\n",
            "Final pre-processed text: merit become italian citizen dual say deport sand nigger immediately\n",
            "Text after removing HTML tags: oh well why did not you say you are nazi trailer trash to begin with let see — idf chick on the left or lisa white trash nazi gangbang slut on the right hm decisions decisions i will go with the idf chick run along nazi scum\n",
            "Text after removing non-alphabetic characters and converting to lowercase: oh well why did not you say you are nazi trailer trash to begin with let see   idf chick on the left or lisa white trash nazi gangbang slut on the right hm decisions decisions i will go with the idf chick run along nazi scum\n",
            "Text after tokenization: ['oh', 'well', 'why', 'did', 'not', 'you', 'say', 'you', 'are', 'nazi', 'trailer', 'trash', 'to', 'begin', 'with', 'let', 'see', 'idf', 'chick', 'on', 'the', 'left', 'or', 'lisa', 'white', 'trash', 'nazi', 'gangbang', 'slut', 'on', 'the', 'right', 'hm', 'decisions', 'decisions', 'i', 'will', 'go', 'with', 'the', 'idf', 'chick', 'run', 'along', 'nazi', 'scum']\n",
            "Text after removing stop words: ['oh', 'well', 'say', 'nazi', 'trailer', 'trash', 'begin', 'let', 'see', 'idf', 'chick', 'left', 'lisa', 'white', 'trash', 'nazi', 'gangbang', 'slut', 'right', 'hm', 'decisions', 'decisions', 'go', 'idf', 'chick', 'run', 'along', 'nazi', 'scum']\n",
            "Text after Lemmatization: ['oh', 'well', 'say', 'nazi', 'trailer', 'trash', 'begin', 'let', 'see', 'idf', 'chick', 'left', 'lisa', 'white', 'trash', 'nazi', 'gangbang', 'slut', 'right', 'hm', 'decision', 'decision', 'go', 'idf', 'chick', 'run', 'along', 'nazi', 'scum']\n",
            "Final pre-processed text: oh well say nazi trailer trash begin let see idf chick left lisa white trash nazi gangbang slut right hm decision decision go idf chick run along nazi scum\n",
            "Text after removing HTML tags: go take you medication faggot boy you sound like you belong on twatter who the fuck cares who follows who you fucking retard what are you  years old\n",
            "Text after removing non-alphabetic characters and converting to lowercase: go take you medication faggot boy you sound like you belong on twatter who the fuck cares who follows who you fucking retard what are you  years old\n",
            "Text after tokenization: ['go', 'take', 'you', 'medication', 'faggot', 'boy', 'you', 'sound', 'like', 'you', 'belong', 'on', 'twatter', 'who', 'the', 'fuck', 'cares', 'who', 'follows', 'who', 'you', 'fucking', 'retard', 'what', 'are', 'you', 'years', 'old']\n",
            "Text after removing stop words: ['go', 'take', 'medication', 'faggot', 'boy', 'sound', 'like', 'belong', 'twatter', 'fuck', 'cares', 'follows', 'fucking', 'retard', 'years', 'old']\n",
            "Text after Lemmatization: ['go', 'take', 'medication', 'faggot', 'boy', 'sound', 'like', 'belong', 'twatter', 'fuck', 'care', 'follows', 'fucking', 'retard', 'year', 'old']\n",
            "Final pre-processed text: go take medication faggot boy sound like belong twatter fuck care follows fucking retard year old\n",
            "Text after removing HTML tags: are you a kike you know making lists is a really kikey thing to do it is one big reason people hate kikes\n",
            "Text after removing non-alphabetic characters and converting to lowercase: are you a kike you know making lists is a really kikey thing to do it is one big reason people hate kikes\n",
            "Text after tokenization: ['are', 'you', 'a', 'kike', 'you', 'know', 'making', 'lists', 'is', 'a', 'really', 'kikey', 'thing', 'to', 'do', 'it', 'is', 'one', 'big', 'reason', 'people', 'hate', 'kikes']\n",
            "Text after removing stop words: ['kike', 'know', 'making', 'lists', 'really', 'kikey', 'thing', 'one', 'big', 'reason', 'people', 'hate', 'kikes']\n",
            "Text after Lemmatization: ['kike', 'know', 'making', 'list', 'really', 'kikey', 'thing', 'one', 'big', 'reason', 'people', 'hate', 'kike']\n",
            "Final pre-processed text: kike know making list really kikey thing one big reason people hate kike\n",
            "Text after removing HTML tags: hahaha fucking globalist had a bad night for sure keep crying you little bitches your power wanes by the day soon the french will turn on macron and the sand niggers in france will be treated with extreme prejudice germany uh well prayers work\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hahaha fucking globalist had a bad night for sure keep crying you little bitches your power wanes by the day soon the french will turn on macron and the sand niggers in france will be treated with extreme prejudice germany uh well prayers work\n",
            "Text after tokenization: ['hahaha', 'fucking', 'globalist', 'had', 'a', 'bad', 'night', 'for', 'sure', 'keep', 'crying', 'you', 'little', 'bitches', 'your', 'power', 'wanes', 'by', 'the', 'day', 'soon', 'the', 'french', 'will', 'turn', 'on', 'macron', 'and', 'the', 'sand', 'niggers', 'in', 'france', 'will', 'be', 'treated', 'with', 'extreme', 'prejudice', 'germany', 'uh', 'well', 'prayers', 'work']\n",
            "Text after removing stop words: ['hahaha', 'fucking', 'globalist', 'bad', 'night', 'sure', 'keep', 'crying', 'little', 'bitches', 'power', 'wanes', 'day', 'soon', 'french', 'turn', 'macron', 'sand', 'niggers', 'france', 'treated', 'extreme', 'prejudice', 'germany', 'uh', 'well', 'prayers', 'work']\n",
            "Text after Lemmatization: ['hahaha', 'fucking', 'globalist', 'bad', 'night', 'sure', 'keep', 'cry', 'little', 'bitch', 'power', 'wane', 'day', 'soon', 'french', 'turn', 'macron', 'sand', 'nigger', 'france', 'treated', 'extreme', 'prejudice', 'germany', 'uh', 'well', 'prayer', 'work']\n",
            "Final pre-processed text: hahaha fucking globalist bad night sure keep cry little bitch power wane day soon french turn macron sand nigger france treated extreme prejudice germany uh well prayer work\n",
            "Text after removing HTML tags: welcome to gabfam perhaps your interactions here will convince you to stop using that social media ghetto instagram not sure we have a freedom based alternative yet keep looking i will speakfreely computefreely\n",
            "Text after removing non-alphabetic characters and converting to lowercase: welcome to gabfam perhaps your interactions here will convince you to stop using that social media ghetto instagram not sure we have a freedom based alternative yet keep looking i will speakfreely computefreely\n",
            "Text after tokenization: ['welcome', 'to', 'gabfam', 'perhaps', 'your', 'interactions', 'here', 'will', 'convince', 'you', 'to', 'stop', 'using', 'that', 'social', 'media', 'ghetto', 'instagram', 'not', 'sure', 'we', 'have', 'a', 'freedom', 'based', 'alternative', 'yet', 'keep', 'looking', 'i', 'will', 'speakfreely', 'computefreely']\n",
            "Text after removing stop words: ['welcome', 'gabfam', 'perhaps', 'interactions', 'convince', 'stop', 'using', 'social', 'media', 'ghetto', 'instagram', 'sure', 'freedom', 'based', 'alternative', 'yet', 'keep', 'looking', 'speakfreely', 'computefreely']\n",
            "Text after Lemmatization: ['welcome', 'gabfam', 'perhaps', 'interaction', 'convince', 'stop', 'using', 'social', 'medium', 'ghetto', 'instagram', 'sure', 'freedom', 'based', 'alternative', 'yet', 'keep', 'looking', 'speakfreely', 'computefreely']\n",
            "Final pre-processed text: welcome gabfam perhaps interaction convince stop using social medium ghetto instagram sure freedom based alternative yet keep looking speakfreely computefreely\n",
            "Text after removing HTML tags:  fucking dead sand nigger drinks on me\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  fucking dead sand nigger drinks on me\n",
            "Text after tokenization: ['fucking', 'dead', 'sand', 'nigger', 'drinks', 'on', 'me']\n",
            "Text after removing stop words: ['fucking', 'dead', 'sand', 'nigger', 'drinks']\n",
            "Text after Lemmatization: ['fucking', 'dead', 'sand', 'nigger', 'drink']\n",
            "Final pre-processed text: fucking dead sand nigger drink\n",
            "Text after removing HTML tags: so do all women if the media teaches them to be whores\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so do all women if the media teaches them to be whores\n",
            "Text after tokenization: ['so', 'do', 'all', 'women', 'if', 'the', 'media', 'teaches', 'them', 'to', 'be', 'whores']\n",
            "Text after removing stop words: ['women', 'media', 'teaches', 'whores']\n",
            "Text after Lemmatization: ['woman', 'medium', 'teach', 'whore']\n",
            "Final pre-processed text: woman medium teach whore\n",
            "Text after removing HTML tags: speakfreely gabfam news i am sorry but someone needs to bitch slap jillstein and then bitch slap everyone or the one who has donated to her cause we the people have a president and it is trump presidenttump\n",
            "Text after removing non-alphabetic characters and converting to lowercase: speakfreely gabfam news i am sorry but someone needs to bitch slap jillstein and then bitch slap everyone or the one who has donated to her cause we the people have a president and it is trump presidenttump\n",
            "Text after tokenization: ['speakfreely', 'gabfam', 'news', 'i', 'am', 'sorry', 'but', 'someone', 'needs', 'to', 'bitch', 'slap', 'jillstein', 'and', 'then', 'bitch', 'slap', 'everyone', 'or', 'the', 'one', 'who', 'has', 'donated', 'to', 'her', 'cause', 'we', 'the', 'people', 'have', 'a', 'president', 'and', 'it', 'is', 'trump', 'presidenttump']\n",
            "Text after removing stop words: ['speakfreely', 'gabfam', 'news', 'sorry', 'someone', 'needs', 'bitch', 'slap', 'jillstein', 'bitch', 'slap', 'everyone', 'one', 'donated', 'cause', 'people', 'president', 'trump', 'presidenttump']\n",
            "Text after Lemmatization: ['speakfreely', 'gabfam', 'news', 'sorry', 'someone', 'need', 'bitch', 'slap', 'jillstein', 'bitch', 'slap', 'everyone', 'one', 'donated', 'cause', 'people', 'president', 'trump', 'presidenttump']\n",
            "Final pre-processed text: speakfreely gabfam news sorry someone need bitch slap jillstein bitch slap everyone one donated cause people president trump presidenttump\n",
            "Text after removing HTML tags: so many followers and nothing to say just remember niggers suck jewboys suck baby dick and faggots suck anything they can\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so many followers and nothing to say just remember niggers suck jewboys suck baby dick and faggots suck anything they can\n",
            "Text after tokenization: ['so', 'many', 'followers', 'and', 'nothing', 'to', 'say', 'just', 'remember', 'niggers', 'suck', 'jewboys', 'suck', 'baby', 'dick', 'and', 'faggots', 'suck', 'anything', 'they', 'can']\n",
            "Text after removing stop words: ['many', 'followers', 'nothing', 'say', 'remember', 'niggers', 'suck', 'jewboys', 'suck', 'baby', 'dick', 'faggots', 'suck', 'anything']\n",
            "Text after Lemmatization: ['many', 'follower', 'nothing', 'say', 'remember', 'nigger', 'suck', 'jewboys', 'suck', 'baby', 'dick', 'faggot', 'suck', 'anything']\n",
            "Final pre-processed text: many follower nothing say remember nigger suck jewboys suck baby dick faggot suck anything\n",
            "Text after removing HTML tags: why are you guys so fucking retarded it just amazing see your hate i hate amazon pigmes it all there fault rainforest cut down less o2 in air witch degrades the nazi brain from evolving little brown fucks it all there faults gay nazis whitesupreamist conspiracy theories lmfao 😂 😃 😃 😃 😄 😄 😅\n",
            "Text after removing non-alphabetic characters and converting to lowercase: why are you guys so fucking retarded it just amazing see your hate i hate amazon pigmes it all there fault rainforest cut down less o  in air witch degrades the nazi brain from evolving little brown fucks it all there faults gay nazis whitesupreamist conspiracy theories lmfao              \n",
            "Text after tokenization: ['why', 'are', 'you', 'guys', 'so', 'fucking', 'retarded', 'it', 'just', 'amazing', 'see', 'your', 'hate', 'i', 'hate', 'amazon', 'pigmes', 'it', 'all', 'there', 'fault', 'rainforest', 'cut', 'down', 'less', 'o', 'in', 'air', 'witch', 'degrades', 'the', 'nazi', 'brain', 'from', 'evolving', 'little', 'brown', 'fucks', 'it', 'all', 'there', 'faults', 'gay', 'nazis', 'whitesupreamist', 'conspiracy', 'theories', 'lmfao']\n",
            "Text after removing stop words: ['guys', 'fucking', 'retarded', 'amazing', 'see', 'hate', 'hate', 'amazon', 'pigmes', 'fault', 'rainforest', 'cut', 'less', 'air', 'witch', 'degrades', 'nazi', 'brain', 'evolving', 'little', 'brown', 'fucks', 'faults', 'gay', 'nazis', 'whitesupreamist', 'conspiracy', 'theories', 'lmfao']\n",
            "Text after Lemmatization: ['guy', 'fucking', 'retarded', 'amazing', 'see', 'hate', 'hate', 'amazon', 'pigmes', 'fault', 'rainforest', 'cut', 'le', 'air', 'witch', 'degrades', 'nazi', 'brain', 'evolving', 'little', 'brown', 'fuck', 'fault', 'gay', 'nazi', 'whitesupreamist', 'conspiracy', 'theory', 'lmfao']\n",
            "Final pre-processed text: guy fucking retarded amazing see hate hate amazon pigmes fault rainforest cut le air witch degrades nazi brain evolving little brown fuck fault gay nazi whitesupreamist conspiracy theory lmfao\n",
            "Text after removing HTML tags: i have seen more discrimination and harassment towards males in the work place than females guess that bound to happen with feminist in charge whyirejectfeminism\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i have seen more discrimination and harassment towards males in the work place than females guess that bound to happen with feminist in charge whyirejectfeminism\n",
            "Text after tokenization: ['i', 'have', 'seen', 'more', 'discrimination', 'and', 'harassment', 'towards', 'males', 'in', 'the', 'work', 'place', 'than', 'females', 'guess', 'that', 'bound', 'to', 'happen', 'with', 'feminist', 'in', 'charge', 'whyirejectfeminism']\n",
            "Text after removing stop words: ['seen', 'discrimination', 'harassment', 'towards', 'males', 'work', 'place', 'females', 'guess', 'bound', 'happen', 'feminist', 'charge', 'whyirejectfeminism']\n",
            "Text after Lemmatization: ['seen', 'discrimination', 'harassment', 'towards', 'male', 'work', 'place', 'female', 'guess', 'bound', 'happen', 'feminist', 'charge', 'whyirejectfeminism']\n",
            "Final pre-processed text: seen discrimination harassment towards male work place female guess bound happen feminist charge whyirejectfeminism\n",
            "Text after removing HTML tags: jews and vatican using the niggers to do dirty work jews used rome to kill jesus nothing new under sun fuck the vatican and the catholic church with is dead jesus on the cross the cocksuckers of tammuz pope kiss a a babies cock dirty pedo wankers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: jews and vatican using the niggers to do dirty work jews used rome to kill jesus nothing new under sun fuck the vatican and the catholic church with is dead jesus on the cross the cocksuckers of tammuz pope kiss a a babies cock dirty pedo wankers\n",
            "Text after tokenization: ['jews', 'and', 'vatican', 'using', 'the', 'niggers', 'to', 'do', 'dirty', 'work', 'jews', 'used', 'rome', 'to', 'kill', 'jesus', 'nothing', 'new', 'under', 'sun', 'fuck', 'the', 'vatican', 'and', 'the', 'catholic', 'church', 'with', 'is', 'dead', 'jesus', 'on', 'the', 'cross', 'the', 'cocksuckers', 'of', 'tammuz', 'pope', 'kiss', 'a', 'a', 'babies', 'cock', 'dirty', 'pedo', 'wankers']\n",
            "Text after removing stop words: ['jews', 'vatican', 'using', 'niggers', 'dirty', 'work', 'jews', 'used', 'rome', 'kill', 'jesus', 'nothing', 'new', 'sun', 'fuck', 'vatican', 'catholic', 'church', 'dead', 'jesus', 'cross', 'cocksuckers', 'tammuz', 'pope', 'kiss', 'babies', 'cock', 'dirty', 'pedo', 'wankers']\n",
            "Text after Lemmatization: ['jew', 'vatican', 'using', 'nigger', 'dirty', 'work', 'jew', 'used', 'rome', 'kill', 'jesus', 'nothing', 'new', 'sun', 'fuck', 'vatican', 'catholic', 'church', 'dead', 'jesus', 'cross', 'cocksucker', 'tammuz', 'pope', 'kiss', 'baby', 'cock', 'dirty', 'pedo', 'wanker']\n",
            "Final pre-processed text: jew vatican using nigger dirty work jew used rome kill jesus nothing new sun fuck vatican catholic church dead jesus cross cocksucker tammuz pope kiss baby cock dirty pedo wanker\n",
            "Text after removing HTML tags: i mean i am all for promoting and educating on abortions specifically in immigrant heavy communities but hysterectomy that going a little too far\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i mean i am all for promoting and educating on abortions specifically in immigrant heavy communities but hysterectomy that going a little too far\n",
            "Text after tokenization: ['i', 'mean', 'i', 'am', 'all', 'for', 'promoting', 'and', 'educating', 'on', 'abortions', 'specifically', 'in', 'immigrant', 'heavy', 'communities', 'but', 'hysterectomy', 'that', 'going', 'a', 'little', 'too', 'far']\n",
            "Text after removing stop words: ['mean', 'promoting', 'educating', 'abortions', 'specifically', 'immigrant', 'heavy', 'communities', 'hysterectomy', 'going', 'little', 'far']\n",
            "Text after Lemmatization: ['mean', 'promoting', 'educating', 'abortion', 'specifically', 'immigrant', 'heavy', 'community', 'hysterectomy', 'going', 'little', 'far']\n",
            "Final pre-processed text: mean promoting educating abortion specifically immigrant heavy community hysterectomy going little far\n",
            "Text after removing HTML tags: interestingly the daily mirror say tommy robinson was filmed punching migrant in italy anybody that saw the full video will see how misleading and devious this is the mirror also broke the telford grooming gang story but failed to mention muslim men something does not seem right about this\n",
            "Text after removing non-alphabetic characters and converting to lowercase: interestingly the daily mirror say tommy robinson was filmed punching migrant in italy anybody that saw the full video will see how misleading and devious this is the mirror also broke the telford grooming gang story but failed to mention muslim men something does not seem right about this\n",
            "Text after tokenization: ['interestingly', 'the', 'daily', 'mirror', 'say', 'tommy', 'robinson', 'was', 'filmed', 'punching', 'migrant', 'in', 'italy', 'anybody', 'that', 'saw', 'the', 'full', 'video', 'will', 'see', 'how', 'misleading', 'and', 'devious', 'this', 'is', 'the', 'mirror', 'also', 'broke', 'the', 'telford', 'grooming', 'gang', 'story', 'but', 'failed', 'to', 'mention', 'muslim', 'men', 'something', 'does', 'not', 'seem', 'right', 'about', 'this']\n",
            "Text after removing stop words: ['interestingly', 'daily', 'mirror', 'say', 'tommy', 'robinson', 'filmed', 'punching', 'migrant', 'italy', 'anybody', 'saw', 'full', 'video', 'see', 'misleading', 'devious', 'mirror', 'also', 'broke', 'telford', 'grooming', 'gang', 'story', 'failed', 'mention', 'muslim', 'men', 'something', 'seem', 'right']\n",
            "Text after Lemmatization: ['interestingly', 'daily', 'mirror', 'say', 'tommy', 'robinson', 'filmed', 'punching', 'migrant', 'italy', 'anybody', 'saw', 'full', 'video', 'see', 'misleading', 'devious', 'mirror', 'also', 'broke', 'telford', 'grooming', 'gang', 'story', 'failed', 'mention', 'muslim', 'men', 'something', 'seem', 'right']\n",
            "Final pre-processed text: interestingly daily mirror say tommy robinson filmed punching migrant italy anybody saw full video see misleading devious mirror also broke telford grooming gang story failed mention muslim men something seem right\n",
            "Text after removing HTML tags: be boomer grow up when access to information is heavily controlled along comes internet age have the world at your finger tips believe things even more retarded than when info was controlled\n",
            "Text after removing non-alphabetic characters and converting to lowercase: be boomer grow up when access to information is heavily controlled along comes internet age have the world at your finger tips believe things even more retarded than when info was controlled\n",
            "Text after tokenization: ['be', 'boomer', 'grow', 'up', 'when', 'access', 'to', 'information', 'is', 'heavily', 'controlled', 'along', 'comes', 'internet', 'age', 'have', 'the', 'world', 'at', 'your', 'finger', 'tips', 'believe', 'things', 'even', 'more', 'retarded', 'than', 'when', 'info', 'was', 'controlled']\n",
            "Text after removing stop words: ['boomer', 'grow', 'access', 'information', 'heavily', 'controlled', 'along', 'comes', 'internet', 'age', 'world', 'finger', 'tips', 'believe', 'things', 'even', 'retarded', 'info', 'controlled']\n",
            "Text after Lemmatization: ['boomer', 'grow', 'access', 'information', 'heavily', 'controlled', 'along', 'come', 'internet', 'age', 'world', 'finger', 'tip', 'believe', 'thing', 'even', 'retarded', 'info', 'controlled']\n",
            "Final pre-processed text: boomer grow access information heavily controlled along come internet age world finger tip believe thing even retarded info controlled\n",
            "Text after removing HTML tags: did she say here or some where else that white men are vile\n",
            "Text after removing non-alphabetic characters and converting to lowercase: did she say here or some where else that white men are vile\n",
            "Text after tokenization: ['did', 'she', 'say', 'here', 'or', 'some', 'where', 'else', 'that', 'white', 'men', 'are', 'vile']\n",
            "Text after removing stop words: ['say', 'else', 'white', 'men', 'vile']\n",
            "Text after Lemmatization: ['say', 'else', 'white', 'men', 'vile']\n",
            "Final pre-processed text: say else white men vile\n",
            "Text after removing HTML tags: did not the bundswehr have some unit that got caught doing some kind of nazi ritual i guess that why the globalists want to push trannies fags blacks and dykes in lest the soldiers realize what they are fighting for is fraudulent  ‍\n",
            "Text after removing non-alphabetic characters and converting to lowercase: did not the bundswehr have some unit that got caught doing some kind of nazi ritual i guess that why the globalists want to push trannies fags blacks and dykes in lest the soldiers realize what they are fighting for is fraudulent   \n",
            "Text after tokenization: ['did', 'not', 'the', 'bundswehr', 'have', 'some', 'unit', 'that', 'got', 'caught', 'doing', 'some', 'kind', 'of', 'nazi', 'ritual', 'i', 'guess', 'that', 'why', 'the', 'globalists', 'want', 'to', 'push', 'trannies', 'fags', 'blacks', 'and', 'dykes', 'in', 'lest', 'the', 'soldiers', 'realize', 'what', 'they', 'are', 'fighting', 'for', 'is', 'fraudulent']\n",
            "Text after removing stop words: ['bundswehr', 'unit', 'got', 'caught', 'kind', 'nazi', 'ritual', 'guess', 'globalists', 'want', 'push', 'trannies', 'fags', 'blacks', 'dykes', 'lest', 'soldiers', 'realize', 'fighting', 'fraudulent']\n",
            "Text after Lemmatization: ['bundswehr', 'unit', 'got', 'caught', 'kind', 'nazi', 'ritual', 'guess', 'globalists', 'want', 'push', 'trannies', 'fag', 'black', 'dyke', 'lest', 'soldier', 'realize', 'fighting', 'fraudulent']\n",
            "Final pre-processed text: bundswehr unit got caught kind nazi ritual guess globalists want push trannies fag black dyke lest soldier realize fighting fraudulent\n",
            "Text after removing HTML tags: she a criminal piece of shit this bitch should hang when her fat ass falls it will pop her head off like saddam hussein\n",
            "Text after removing non-alphabetic characters and converting to lowercase: she a criminal piece of shit this bitch should hang when her fat ass falls it will pop her head off like saddam hussein\n",
            "Text after tokenization: ['she', 'a', 'criminal', 'piece', 'of', 'shit', 'this', 'bitch', 'should', 'hang', 'when', 'her', 'fat', 'ass', 'falls', 'it', 'will', 'pop', 'her', 'head', 'off', 'like', 'saddam', 'hussein']\n",
            "Text after removing stop words: ['criminal', 'piece', 'shit', 'bitch', 'hang', 'fat', 'ass', 'falls', 'pop', 'head', 'like', 'saddam', 'hussein']\n",
            "Text after Lemmatization: ['criminal', 'piece', 'shit', 'bitch', 'hang', 'fat', 'as', 'fall', 'pop', 'head', 'like', 'saddam', 'hussein']\n",
            "Final pre-processed text: criminal piece shit bitch hang fat as fall pop head like saddam hussein\n",
            "Text after removing HTML tags: i see a lot of brown people and stupid women being the ones in these marches young white men know what is up do not let the msm gaslight you to think otherwise\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i see a lot of brown people and stupid women being the ones in these marches young white men know what is up do not let the msm gaslight you to think otherwise\n",
            "Text after tokenization: ['i', 'see', 'a', 'lot', 'of', 'brown', 'people', 'and', 'stupid', 'women', 'being', 'the', 'ones', 'in', 'these', 'marches', 'young', 'white', 'men', 'know', 'what', 'is', 'up', 'do', 'not', 'let', 'the', 'msm', 'gaslight', 'you', 'to', 'think', 'otherwise']\n",
            "Text after removing stop words: ['see', 'lot', 'brown', 'people', 'stupid', 'women', 'ones', 'marches', 'young', 'white', 'men', 'know', 'let', 'msm', 'gaslight', 'think', 'otherwise']\n",
            "Text after Lemmatization: ['see', 'lot', 'brown', 'people', 'stupid', 'woman', 'one', 'march', 'young', 'white', 'men', 'know', 'let', 'msm', 'gaslight', 'think', 'otherwise']\n",
            "Final pre-processed text: see lot brown people stupid woman one march young white men know let msm gaslight think otherwise\n",
            "Text after removing HTML tags: the delighted kikes broke his nose via sand nigger proxy\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the delighted kikes broke his nose via sand nigger proxy\n",
            "Text after tokenization: ['the', 'delighted', 'kikes', 'broke', 'his', 'nose', 'via', 'sand', 'nigger', 'proxy']\n",
            "Text after removing stop words: ['delighted', 'kikes', 'broke', 'nose', 'via', 'sand', 'nigger', 'proxy']\n",
            "Text after Lemmatization: ['delighted', 'kike', 'broke', 'nose', 'via', 'sand', 'nigger', 'proxy']\n",
            "Final pre-processed text: delighted kike broke nose via sand nigger proxy\n",
            "Text after removing HTML tags: israel need some diversity then jews will be good at sports finally it will be their strength altright\n",
            "Text after removing non-alphabetic characters and converting to lowercase: israel need some diversity then jews will be good at sports finally it will be their strength altright\n",
            "Text after tokenization: ['israel', 'need', 'some', 'diversity', 'then', 'jews', 'will', 'be', 'good', 'at', 'sports', 'finally', 'it', 'will', 'be', 'their', 'strength', 'altright']\n",
            "Text after removing stop words: ['israel', 'need', 'diversity', 'jews', 'good', 'sports', 'finally', 'strength', 'altright']\n",
            "Text after Lemmatization: ['israel', 'need', 'diversity', 'jew', 'good', 'sport', 'finally', 'strength', 'altright']\n",
            "Final pre-processed text: israel need diversity jew good sport finally strength altright\n",
            "Text after removing HTML tags: i am pro homosexual ethnostate\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am pro homosexual ethnostate\n",
            "Text after tokenization: ['i', 'am', 'pro', 'homosexual', 'ethnostate']\n",
            "Text after removing stop words: ['pro', 'homosexual', 'ethnostate']\n",
            "Text after Lemmatization: ['pro', 'homosexual', 'ethnostate']\n",
            "Final pre-processed text: pro homosexual ethnostate\n",
            "Text after removing HTML tags: glad to hear do not be a sheltered cuck for any side lefties are gay as fuck but gayness is contagious to the right as well but i think you already grasp that\n",
            "Text after removing non-alphabetic characters and converting to lowercase: glad to hear do not be a sheltered cuck for any side lefties are gay as fuck but gayness is contagious to the right as well but i think you already grasp that\n",
            "Text after tokenization: ['glad', 'to', 'hear', 'do', 'not', 'be', 'a', 'sheltered', 'cuck', 'for', 'any', 'side', 'lefties', 'are', 'gay', 'as', 'fuck', 'but', 'gayness', 'is', 'contagious', 'to', 'the', 'right', 'as', 'well', 'but', 'i', 'think', 'you', 'already', 'grasp', 'that']\n",
            "Text after removing stop words: ['glad', 'hear', 'sheltered', 'cuck', 'side', 'lefties', 'gay', 'fuck', 'gayness', 'contagious', 'right', 'well', 'think', 'already', 'grasp']\n",
            "Text after Lemmatization: ['glad', 'hear', 'sheltered', 'cuck', 'side', 'lefty', 'gay', 'fuck', 'gayness', 'contagious', 'right', 'well', 'think', 'already', 'grasp']\n",
            "Final pre-processed text: glad hear sheltered cuck side lefty gay fuck gayness contagious right well think already grasp\n",
            "Text after removing HTML tags: exactly i chuckle when women say she made it on her own no she did not she was the first lady and became a senator just becasue she was married to bill and she was a senator of my state for  years becasue my state is fucking retarded\n",
            "Text after removing non-alphabetic characters and converting to lowercase: exactly i chuckle when women say she made it on her own no she did not she was the first lady and became a senator just becasue she was married to bill and she was a senator of my state for  years becasue my state is fucking retarded\n",
            "Text after tokenization: ['exactly', 'i', 'chuckle', 'when', 'women', 'say', 'she', 'made', 'it', 'on', 'her', 'own', 'no', 'she', 'did', 'not', 'she', 'was', 'the', 'first', 'lady', 'and', 'became', 'a', 'senator', 'just', 'becasue', 'she', 'was', 'married', 'to', 'bill', 'and', 'she', 'was', 'a', 'senator', 'of', 'my', 'state', 'for', 'years', 'becasue', 'my', 'state', 'is', 'fucking', 'retarded']\n",
            "Text after removing stop words: ['exactly', 'chuckle', 'women', 'say', 'made', 'first', 'lady', 'became', 'senator', 'becasue', 'married', 'bill', 'senator', 'state', 'years', 'becasue', 'state', 'fucking', 'retarded']\n",
            "Text after Lemmatization: ['exactly', 'chuckle', 'woman', 'say', 'made', 'first', 'lady', 'became', 'senator', 'becasue', 'married', 'bill', 'senator', 'state', 'year', 'becasue', 'state', 'fucking', 'retarded']\n",
            "Final pre-processed text: exactly chuckle woman say made first lady became senator becasue married bill senator state year becasue state fucking retarded\n",
            "Text after removing HTML tags: look faggot i am not gonna entertain your homosexual fantasies of larping as a bitchy woman on the internet so just fuck off and try to find your penis among your overbloated fat belly\n",
            "Text after removing non-alphabetic characters and converting to lowercase: look faggot i am not gonna entertain your homosexual fantasies of larping as a bitchy woman on the internet so just fuck off and try to find your penis among your overbloated fat belly\n",
            "Text after tokenization: ['look', 'faggot', 'i', 'am', 'not', 'gon', 'na', 'entertain', 'your', 'homosexual', 'fantasies', 'of', 'larping', 'as', 'a', 'bitchy', 'woman', 'on', 'the', 'internet', 'so', 'just', 'fuck', 'off', 'and', 'try', 'to', 'find', 'your', 'penis', 'among', 'your', 'overbloated', 'fat', 'belly']\n",
            "Text after removing stop words: ['look', 'faggot', 'gon', 'na', 'entertain', 'homosexual', 'fantasies', 'larping', 'bitchy', 'woman', 'internet', 'fuck', 'try', 'find', 'penis', 'among', 'overbloated', 'fat', 'belly']\n",
            "Text after Lemmatization: ['look', 'faggot', 'gon', 'na', 'entertain', 'homosexual', 'fantasy', 'larping', 'bitchy', 'woman', 'internet', 'fuck', 'try', 'find', 'penis', 'among', 'overbloated', 'fat', 'belly']\n",
            "Final pre-processed text: look faggot gon na entertain homosexual fantasy larping bitchy woman internet fuck try find penis among overbloated fat belly\n",
            "Text after removing HTML tags: i wake up everyday feeling blessed that i am a citizen of the united states in india there are more cellphones than there are working toilets god bless the great men and women who made our country what it is and who fight to preserve our exceptionalism on the world stage\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i wake up everyday feeling blessed that i am a citizen of the united states in india there are more cellphones than there are working toilets god bless the great men and women who made our country what it is and who fight to preserve our exceptionalism on the world stage\n",
            "Text after tokenization: ['i', 'wake', 'up', 'everyday', 'feeling', 'blessed', 'that', 'i', 'am', 'a', 'citizen', 'of', 'the', 'united', 'states', 'in', 'india', 'there', 'are', 'more', 'cellphones', 'than', 'there', 'are', 'working', 'toilets', 'god', 'bless', 'the', 'great', 'men', 'and', 'women', 'who', 'made', 'our', 'country', 'what', 'it', 'is', 'and', 'who', 'fight', 'to', 'preserve', 'our', 'exceptionalism', 'on', 'the', 'world', 'stage']\n",
            "Text after removing stop words: ['wake', 'everyday', 'feeling', 'blessed', 'citizen', 'united', 'states', 'india', 'cellphones', 'working', 'toilets', 'god', 'bless', 'great', 'men', 'women', 'made', 'country', 'fight', 'preserve', 'exceptionalism', 'world', 'stage']\n",
            "Text after Lemmatization: ['wake', 'everyday', 'feeling', 'blessed', 'citizen', 'united', 'state', 'india', 'cellphone', 'working', 'toilet', 'god', 'bless', 'great', 'men', 'woman', 'made', 'country', 'fight', 'preserve', 'exceptionalism', 'world', 'stage']\n",
            "Final pre-processed text: wake everyday feeling blessed citizen united state india cellphone working toilet god bless great men woman made country fight preserve exceptionalism world stage\n",
            "Text after removing HTML tags: i expat i think i might have criticised some muslims and a bit of islam possibly some jews slightly and i am afraid to go and visit my elderly relatives i am pro israel though perhaps that like a sort of passport now what happening to us\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i expat i think i might have criticised some muslims and a bit of islam possibly some jews slightly and i am afraid to go and visit my elderly relatives i am pro israel though perhaps that like a sort of passport now what happening to us\n",
            "Text after tokenization: ['i', 'expat', 'i', 'think', 'i', 'might', 'have', 'criticised', 'some', 'muslims', 'and', 'a', 'bit', 'of', 'islam', 'possibly', 'some', 'jews', 'slightly', 'and', 'i', 'am', 'afraid', 'to', 'go', 'and', 'visit', 'my', 'elderly', 'relatives', 'i', 'am', 'pro', 'israel', 'though', 'perhaps', 'that', 'like', 'a', 'sort', 'of', 'passport', 'now', 'what', 'happening', 'to', 'us']\n",
            "Text after removing stop words: ['expat', 'think', 'might', 'criticised', 'muslims', 'bit', 'islam', 'possibly', 'jews', 'slightly', 'afraid', 'go', 'visit', 'elderly', 'relatives', 'pro', 'israel', 'though', 'perhaps', 'like', 'sort', 'passport', 'happening', 'us']\n",
            "Text after Lemmatization: ['expat', 'think', 'might', 'criticised', 'muslim', 'bit', 'islam', 'possibly', 'jew', 'slightly', 'afraid', 'go', 'visit', 'elderly', 'relative', 'pro', 'israel', 'though', 'perhaps', 'like', 'sort', 'passport', 'happening', 'u']\n",
            "Final pre-processed text: expat think might criticised muslim bit islam possibly jew slightly afraid go visit elderly relative pro israel though perhaps like sort passport happening u\n",
            "Text after removing HTML tags: i doubt if muslims pledge or salute our flag when sworn in for an office in nyc muslim used the koran i think most of us are under stress with maryland texas bombing not a good day time for a break always a pleasure a\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i doubt if muslims pledge or salute our flag when sworn in for an office in nyc muslim used the koran i think most of us are under stress with maryland texas bombing not a good day time for a break always a pleasure a\n",
            "Text after tokenization: ['i', 'doubt', 'if', 'muslims', 'pledge', 'or', 'salute', 'our', 'flag', 'when', 'sworn', 'in', 'for', 'an', 'office', 'in', 'nyc', 'muslim', 'used', 'the', 'koran', 'i', 'think', 'most', 'of', 'us', 'are', 'under', 'stress', 'with', 'maryland', 'texas', 'bombing', 'not', 'a', 'good', 'day', 'time', 'for', 'a', 'break', 'always', 'a', 'pleasure', 'a']\n",
            "Text after removing stop words: ['doubt', 'muslims', 'pledge', 'salute', 'flag', 'sworn', 'office', 'nyc', 'muslim', 'used', 'koran', 'think', 'us', 'stress', 'maryland', 'texas', 'bombing', 'good', 'day', 'time', 'break', 'always', 'pleasure']\n",
            "Text after Lemmatization: ['doubt', 'muslim', 'pledge', 'salute', 'flag', 'sworn', 'office', 'nyc', 'muslim', 'used', 'koran', 'think', 'u', 'stress', 'maryland', 'texas', 'bombing', 'good', 'day', 'time', 'break', 'always', 'pleasure']\n",
            "Final pre-processed text: doubt muslim pledge salute flag sworn office nyc muslim used koran think u stress maryland texas bombing good day time break always pleasure\n",
            "Text after removing HTML tags:  niggers trying to rape a camel\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  niggers trying to rape a camel\n",
            "Text after tokenization: ['niggers', 'trying', 'to', 'rape', 'a', 'camel']\n",
            "Text after removing stop words: ['niggers', 'trying', 'rape', 'camel']\n",
            "Text after Lemmatization: ['nigger', 'trying', 'rape', 'camel']\n",
            "Final pre-processed text: nigger trying rape camel\n",
            "Text after removing HTML tags: texas is a shithole because it full of niggers i do not know why anybody would live there texans are not tough they are a bunch of pussies they let their whole state be overrun by niggers and mexicans\n",
            "Text after removing non-alphabetic characters and converting to lowercase: texas is a shithole because it full of niggers i do not know why anybody would live there texans are not tough they are a bunch of pussies they let their whole state be overrun by niggers and mexicans\n",
            "Text after tokenization: ['texas', 'is', 'a', 'shithole', 'because', 'it', 'full', 'of', 'niggers', 'i', 'do', 'not', 'know', 'why', 'anybody', 'would', 'live', 'there', 'texans', 'are', 'not', 'tough', 'they', 'are', 'a', 'bunch', 'of', 'pussies', 'they', 'let', 'their', 'whole', 'state', 'be', 'overrun', 'by', 'niggers', 'and', 'mexicans']\n",
            "Text after removing stop words: ['texas', 'shithole', 'full', 'niggers', 'know', 'anybody', 'would', 'live', 'texans', 'tough', 'bunch', 'pussies', 'let', 'whole', 'state', 'overrun', 'niggers', 'mexicans']\n",
            "Text after Lemmatization: ['texas', 'shithole', 'full', 'nigger', 'know', 'anybody', 'would', 'live', 'texan', 'tough', 'bunch', 'pussy', 'let', 'whole', 'state', 'overrun', 'nigger', 'mexican']\n",
            "Final pre-processed text: texas shithole full nigger know anybody would live texan tough bunch pussy let whole state overrun nigger mexican\n",
            "Text after removing HTML tags: the same  ‍ account that just told someone to commit an act of violence on someone else an hour ago\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the same    account that just told someone to commit an act of violence on someone else an hour ago\n",
            "Text after tokenization: ['the', 'same', 'account', 'that', 'just', 'told', 'someone', 'to', 'commit', 'an', 'act', 'of', 'violence', 'on', 'someone', 'else', 'an', 'hour', 'ago']\n",
            "Text after removing stop words: ['account', 'told', 'someone', 'commit', 'act', 'violence', 'someone', 'else', 'hour', 'ago']\n",
            "Text after Lemmatization: ['account', 'told', 'someone', 'commit', 'act', 'violence', 'someone', 'else', 'hour', 'ago']\n",
            "Final pre-processed text: account told someone commit act violence someone else hour ago\n",
            "Text after removing HTML tags: being stuck in the gab ghetto makes me want to an hero\n",
            "Text after removing non-alphabetic characters and converting to lowercase: being stuck in the gab ghetto makes me want to an hero\n",
            "Text after tokenization: ['being', 'stuck', 'in', 'the', 'gab', 'ghetto', 'makes', 'me', 'want', 'to', 'an', 'hero']\n",
            "Text after removing stop words: ['stuck', 'gab', 'ghetto', 'makes', 'want', 'hero']\n",
            "Text after Lemmatization: ['stuck', 'gab', 'ghetto', 'make', 'want', 'hero']\n",
            "Final pre-processed text: stuck gab ghetto make want hero\n",
            "Text after removing HTML tags: really saddam kept that shit hole from completely imploding bolton is a goon moron and the rest of the bush administration had no clue what was going on over there on the ground bolton enabled neocons with his retarded cowboy attitude even if you actually believe he wasn t one himself\n",
            "Text after removing non-alphabetic characters and converting to lowercase: really saddam kept that shit hole from completely imploding bolton is a goon moron and the rest of the bush administration had no clue what was going on over there on the ground bolton enabled neocons with his retarded cowboy attitude even if you actually believe he wasn t one himself\n",
            "Text after tokenization: ['really', 'saddam', 'kept', 'that', 'shit', 'hole', 'from', 'completely', 'imploding', 'bolton', 'is', 'a', 'goon', 'moron', 'and', 'the', 'rest', 'of', 'the', 'bush', 'administration', 'had', 'no', 'clue', 'what', 'was', 'going', 'on', 'over', 'there', 'on', 'the', 'ground', 'bolton', 'enabled', 'neocons', 'with', 'his', 'retarded', 'cowboy', 'attitude', 'even', 'if', 'you', 'actually', 'believe', 'he', 'wasn', 't', 'one', 'himself']\n",
            "Text after removing stop words: ['really', 'saddam', 'kept', 'shit', 'hole', 'completely', 'imploding', 'bolton', 'goon', 'moron', 'rest', 'bush', 'administration', 'clue', 'going', 'ground', 'bolton', 'enabled', 'neocons', 'retarded', 'cowboy', 'attitude', 'even', 'actually', 'believe', 'one']\n",
            "Text after Lemmatization: ['really', 'saddam', 'kept', 'shit', 'hole', 'completely', 'imploding', 'bolton', 'goon', 'moron', 'rest', 'bush', 'administration', 'clue', 'going', 'ground', 'bolton', 'enabled', 'neocon', 'retarded', 'cowboy', 'attitude', 'even', 'actually', 'believe', 'one']\n",
            "Final pre-processed text: really saddam kept shit hole completely imploding bolton goon moron rest bush administration clue going ground bolton enabled neocon retarded cowboy attitude even actually believe one\n",
            "Text after removing HTML tags:  while i have seen some of this footage before it is a reminder that a good  of these so called refugees appear to be men of fighting age with no intention of assimilation not the families children the media like to find show\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  while i have seen some of this footage before it is a reminder that a good  of these so called refugees appear to be men of fighting age with no intention of assimilation not the families children the media like to find show\n",
            "Text after tokenization: ['while', 'i', 'have', 'seen', 'some', 'of', 'this', 'footage', 'before', 'it', 'is', 'a', 'reminder', 'that', 'a', 'good', 'of', 'these', 'so', 'called', 'refugees', 'appear', 'to', 'be', 'men', 'of', 'fighting', 'age', 'with', 'no', 'intention', 'of', 'assimilation', 'not', 'the', 'families', 'children', 'the', 'media', 'like', 'to', 'find', 'show']\n",
            "Text after removing stop words: ['seen', 'footage', 'reminder', 'good', 'called', 'refugees', 'appear', 'men', 'fighting', 'age', 'intention', 'assimilation', 'families', 'children', 'media', 'like', 'find', 'show']\n",
            "Text after Lemmatization: ['seen', 'footage', 'reminder', 'good', 'called', 'refugee', 'appear', 'men', 'fighting', 'age', 'intention', 'assimilation', 'family', 'child', 'medium', 'like', 'find', 'show']\n",
            "Final pre-processed text: seen footage reminder good called refugee appear men fighting age intention assimilation family child medium like find show\n",
            "Text after removing HTML tags: sickening time to intervene these are refugees we need to help\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sickening time to intervene these are refugees we need to help\n",
            "Text after tokenization: ['sickening', 'time', 'to', 'intervene', 'these', 'are', 'refugees', 'we', 'need', 'to', 'help']\n",
            "Text after removing stop words: ['sickening', 'time', 'intervene', 'refugees', 'need', 'help']\n",
            "Text after Lemmatization: ['sickening', 'time', 'intervene', 'refugee', 'need', 'help']\n",
            "Final pre-processed text: sickening time intervene refugee need help\n",
            "Text after removing HTML tags: merkel needs to be pack raped by a gang of muzzos then ass fucked by african niggers i hope she gets aids and dies in pain\n",
            "Text after removing non-alphabetic characters and converting to lowercase: merkel needs to be pack raped by a gang of muzzos then ass fucked by african niggers i hope she gets aids and dies in pain\n",
            "Text after tokenization: ['merkel', 'needs', 'to', 'be', 'pack', 'raped', 'by', 'a', 'gang', 'of', 'muzzos', 'then', 'ass', 'fucked', 'by', 'african', 'niggers', 'i', 'hope', 'she', 'gets', 'aids', 'and', 'dies', 'in', 'pain']\n",
            "Text after removing stop words: ['merkel', 'needs', 'pack', 'raped', 'gang', 'muzzos', 'ass', 'fucked', 'african', 'niggers', 'hope', 'gets', 'aids', 'dies', 'pain']\n",
            "Text after Lemmatization: ['merkel', 'need', 'pack', 'raped', 'gang', 'muzzos', 'as', 'fucked', 'african', 'nigger', 'hope', 'get', 'aid', 'dy', 'pain']\n",
            "Final pre-processed text: merkel need pack raped gang muzzos as fucked african nigger hope get aid dy pain\n",
            "Text after removing HTML tags: faggots who put such an extreme emphasis on bourgouise education are like retarded children or maybe more like a normie republican who do not understand nature vs nurture wtf is wrong w you\n",
            "Text after removing non-alphabetic characters and converting to lowercase: faggots who put such an extreme emphasis on bourgouise education are like retarded children or maybe more like a normie republican who do not understand nature vs nurture wtf is wrong w you\n",
            "Text after tokenization: ['faggots', 'who', 'put', 'such', 'an', 'extreme', 'emphasis', 'on', 'bourgouise', 'education', 'are', 'like', 'retarded', 'children', 'or', 'maybe', 'more', 'like', 'a', 'normie', 'republican', 'who', 'do', 'not', 'understand', 'nature', 'vs', 'nurture', 'wtf', 'is', 'wrong', 'w', 'you']\n",
            "Text after removing stop words: ['faggots', 'put', 'extreme', 'emphasis', 'bourgouise', 'education', 'like', 'retarded', 'children', 'maybe', 'like', 'normie', 'republican', 'understand', 'nature', 'vs', 'nurture', 'wtf', 'wrong', 'w']\n",
            "Text after Lemmatization: ['faggot', 'put', 'extreme', 'emphasis', 'bourgouise', 'education', 'like', 'retarded', 'child', 'maybe', 'like', 'normie', 'republican', 'understand', 'nature', 'v', 'nurture', 'wtf', 'wrong', 'w']\n",
            "Final pre-processed text: faggot put extreme emphasis bourgouise education like retarded child maybe like normie republican understand nature v nurture wtf wrong w\n",
            "Text after removing HTML tags: guise i just want to remind all you haters that before ebul lbj tricked all de black peepul onto his democrat ghetto plantations every single heart doctor hn america was a kindly black man who wore sweaters and dats a fact\n",
            "Text after removing non-alphabetic characters and converting to lowercase: guise i just want to remind all you haters that before ebul lbj tricked all de black peepul onto his democrat ghetto plantations every single heart doctor hn america was a kindly black man who wore sweaters and dats a fact\n",
            "Text after tokenization: ['guise', 'i', 'just', 'want', 'to', 'remind', 'all', 'you', 'haters', 'that', 'before', 'ebul', 'lbj', 'tricked', 'all', 'de', 'black', 'peepul', 'onto', 'his', 'democrat', 'ghetto', 'plantations', 'every', 'single', 'heart', 'doctor', 'hn', 'america', 'was', 'a', 'kindly', 'black', 'man', 'who', 'wore', 'sweaters', 'and', 'dats', 'a', 'fact']\n",
            "Text after removing stop words: ['guise', 'want', 'remind', 'haters', 'ebul', 'lbj', 'tricked', 'de', 'black', 'peepul', 'onto', 'democrat', 'ghetto', 'plantations', 'every', 'single', 'heart', 'doctor', 'hn', 'america', 'kindly', 'black', 'man', 'wore', 'sweaters', 'dats', 'fact']\n",
            "Text after Lemmatization: ['guise', 'want', 'remind', 'hater', 'ebul', 'lbj', 'tricked', 'de', 'black', 'peepul', 'onto', 'democrat', 'ghetto', 'plantation', 'every', 'single', 'heart', 'doctor', 'hn', 'america', 'kindly', 'black', 'man', 'wore', 'sweater', 'dat', 'fact']\n",
            "Final pre-processed text: guise want remind hater ebul lbj tricked de black peepul onto democrat ghetto plantation every single heart doctor hn america kindly black man wore sweater dat fact\n",
            "Text after removing HTML tags: i watched his video seemed to be predicated on north western europeans as normatively white but in my view white refers to all those caucasians who interacted within the western civilizational system thus greek are white despite having dna closer to levantine semites than germans or slavs\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i watched his video seemed to be predicated on north western europeans as normatively white but in my view white refers to all those caucasians who interacted within the western civilizational system thus greek are white despite having dna closer to levantine semites than germans or slavs\n",
            "Text after tokenization: ['i', 'watched', 'his', 'video', 'seemed', 'to', 'be', 'predicated', 'on', 'north', 'western', 'europeans', 'as', 'normatively', 'white', 'but', 'in', 'my', 'view', 'white', 'refers', 'to', 'all', 'those', 'caucasians', 'who', 'interacted', 'within', 'the', 'western', 'civilizational', 'system', 'thus', 'greek', 'are', 'white', 'despite', 'having', 'dna', 'closer', 'to', 'levantine', 'semites', 'than', 'germans', 'or', 'slavs']\n",
            "Text after removing stop words: ['watched', 'video', 'seemed', 'predicated', 'north', 'western', 'europeans', 'normatively', 'white', 'view', 'white', 'refers', 'caucasians', 'interacted', 'within', 'western', 'civilizational', 'system', 'thus', 'greek', 'white', 'despite', 'dna', 'closer', 'levantine', 'semites', 'germans', 'slavs']\n",
            "Text after Lemmatization: ['watched', 'video', 'seemed', 'predicated', 'north', 'western', 'european', 'normatively', 'white', 'view', 'white', 'refers', 'caucasian', 'interacted', 'within', 'western', 'civilizational', 'system', 'thus', 'greek', 'white', 'despite', 'dna', 'closer', 'levantine', 'semite', 'german', 'slav']\n",
            "Final pre-processed text: watched video seemed predicated north western european normatively white view white refers caucasian interacted within western civilizational system thus greek white despite dna closer levantine semite german slav\n",
            "Text after removing HTML tags: because jews are not white dickhead\n",
            "Text after removing non-alphabetic characters and converting to lowercase: because jews are not white dickhead\n",
            "Text after tokenization: ['because', 'jews', 'are', 'not', 'white', 'dickhead']\n",
            "Text after removing stop words: ['jews', 'white', 'dickhead']\n",
            "Text after Lemmatization: ['jew', 'white', 'dickhead']\n",
            "Final pre-processed text: jew white dickhead\n",
            "Text after removing HTML tags: 😂 😂 😂 😂 😂 defending sand niggers 😂 😂 😂 😂 😂 you should visit iran and see how they treat whites 😉 seeing islam implode takes priority over re muh joz\n",
            "Text after removing non-alphabetic characters and converting to lowercase:           defending sand niggers           you should visit iran and see how they treat whites   seeing islam implode takes priority over re muh joz\n",
            "Text after tokenization: ['defending', 'sand', 'niggers', 'you', 'should', 'visit', 'iran', 'and', 'see', 'how', 'they', 'treat', 'whites', 'seeing', 'islam', 'implode', 'takes', 'priority', 'over', 're', 'muh', 'joz']\n",
            "Text after removing stop words: ['defending', 'sand', 'niggers', 'visit', 'iran', 'see', 'treat', 'whites', 'seeing', 'islam', 'implode', 'takes', 'priority', 'muh', 'joz']\n",
            "Text after Lemmatization: ['defending', 'sand', 'nigger', 'visit', 'iran', 'see', 'treat', 'white', 'seeing', 'islam', 'implode', 'take', 'priority', 'muh', 'joz']\n",
            "Final pre-processed text: defending sand nigger visit iran see treat white seeing islam implode take priority muh joz\n",
            "Text after removing HTML tags: muslims and jews are allies\n",
            "Text after removing non-alphabetic characters and converting to lowercase: muslims and jews are allies\n",
            "Text after tokenization: ['muslims', 'and', 'jews', 'are', 'allies']\n",
            "Text after removing stop words: ['muslims', 'jews', 'allies']\n",
            "Text after Lemmatization: ['muslim', 'jew', 'ally']\n",
            "Final pre-processed text: muslim jew ally\n",
            "Text after removing HTML tags: at this point it looking bad for alex jones stop beating up on the retarded child alex\n",
            "Text after removing non-alphabetic characters and converting to lowercase: at this point it looking bad for alex jones stop beating up on the retarded child alex\n",
            "Text after tokenization: ['at', 'this', 'point', 'it', 'looking', 'bad', 'for', 'alex', 'jones', 'stop', 'beating', 'up', 'on', 'the', 'retarded', 'child', 'alex']\n",
            "Text after removing stop words: ['point', 'looking', 'bad', 'alex', 'jones', 'stop', 'beating', 'retarded', 'child', 'alex']\n",
            "Text after Lemmatization: ['point', 'looking', 'bad', 'alex', 'jones', 'stop', 'beating', 'retarded', 'child', 'alex']\n",
            "Final pre-processed text: point looking bad alex jones stop beating retarded child alex\n",
            "Text after removing HTML tags: a muslim in sweden asked for isis donations for  years on facebook and his account was not deleted\n",
            "Text after removing non-alphabetic characters and converting to lowercase: a muslim in sweden asked for isis donations for  years on facebook and his account was not deleted\n",
            "Text after tokenization: ['a', 'muslim', 'in', 'sweden', 'asked', 'for', 'isis', 'donations', 'for', 'years', 'on', 'facebook', 'and', 'his', 'account', 'was', 'not', 'deleted']\n",
            "Text after removing stop words: ['muslim', 'sweden', 'asked', 'isis', 'donations', 'years', 'facebook', 'account', 'deleted']\n",
            "Text after Lemmatization: ['muslim', 'sweden', 'asked', 'isi', 'donation', 'year', 'facebook', 'account', 'deleted']\n",
            "Final pre-processed text: muslim sweden asked isi donation year facebook account deleted\n",
            "Text after removing HTML tags: how about you let people with balls do what needs to be done you are a subversive kike who does nothing but punch right and counter signal\n",
            "Text after removing non-alphabetic characters and converting to lowercase: how about you let people with balls do what needs to be done you are a subversive kike who does nothing but punch right and counter signal\n",
            "Text after tokenization: ['how', 'about', 'you', 'let', 'people', 'with', 'balls', 'do', 'what', 'needs', 'to', 'be', 'done', 'you', 'are', 'a', 'subversive', 'kike', 'who', 'does', 'nothing', 'but', 'punch', 'right', 'and', 'counter', 'signal']\n",
            "Text after removing stop words: ['let', 'people', 'balls', 'needs', 'done', 'subversive', 'kike', 'nothing', 'punch', 'right', 'counter', 'signal']\n",
            "Text after Lemmatization: ['let', 'people', 'ball', 'need', 'done', 'subversive', 'kike', 'nothing', 'punch', 'right', 'counter', 'signal']\n",
            "Final pre-processed text: let people ball need done subversive kike nothing punch right counter signal\n",
            "Text after removing HTML tags: raise it and selective service to  i say lets see them faggots fight a zog war when you cant enlist till  nobody would be dumb enough to go\n",
            "Text after removing non-alphabetic characters and converting to lowercase: raise it and selective service to  i say lets see them faggots fight a zog war when you cant enlist till  nobody would be dumb enough to go\n",
            "Text after tokenization: ['raise', 'it', 'and', 'selective', 'service', 'to', 'i', 'say', 'lets', 'see', 'them', 'faggots', 'fight', 'a', 'zog', 'war', 'when', 'you', 'cant', 'enlist', 'till', 'nobody', 'would', 'be', 'dumb', 'enough', 'to', 'go']\n",
            "Text after removing stop words: ['raise', 'selective', 'service', 'say', 'lets', 'see', 'faggots', 'fight', 'zog', 'war', 'cant', 'enlist', 'till', 'nobody', 'would', 'dumb', 'enough', 'go']\n",
            "Text after Lemmatization: ['raise', 'selective', 'service', 'say', 'let', 'see', 'faggot', 'fight', 'zog', 'war', 'cant', 'enlist', 'till', 'nobody', 'would', 'dumb', 'enough', 'go']\n",
            "Final pre-processed text: raise selective service say let see faggot fight zog war cant enlist till nobody would dumb enough go\n",
            "Text after removing HTML tags: total fake worthless muslim black bastard\n",
            "Text after removing non-alphabetic characters and converting to lowercase: total fake worthless muslim black bastard\n",
            "Text after tokenization: ['total', 'fake', 'worthless', 'muslim', 'black', 'bastard']\n",
            "Text after removing stop words: ['total', 'fake', 'worthless', 'muslim', 'black', 'bastard']\n",
            "Text after Lemmatization: ['total', 'fake', 'worthless', 'muslim', 'black', 'bastard']\n",
            "Final pre-processed text: total fake worthless muslim black bastard\n",
            "Text after removing HTML tags: stand up and be counted feel free to message me with any questions the ikk kk is currently taking applications we are looking for honest christian men and women who are ready to make a change ik org\n",
            "Text after removing non-alphabetic characters and converting to lowercase: stand up and be counted feel free to message me with any questions the ikk kk is currently taking applications we are looking for honest christian men and women who are ready to make a change ik org\n",
            "Text after tokenization: ['stand', 'up', 'and', 'be', 'counted', 'feel', 'free', 'to', 'message', 'me', 'with', 'any', 'questions', 'the', 'ikk', 'kk', 'is', 'currently', 'taking', 'applications', 'we', 'are', 'looking', 'for', 'honest', 'christian', 'men', 'and', 'women', 'who', 'are', 'ready', 'to', 'make', 'a', 'change', 'ik', 'org']\n",
            "Text after removing stop words: ['stand', 'counted', 'feel', 'free', 'message', 'questions', 'ikk', 'kk', 'currently', 'taking', 'applications', 'looking', 'honest', 'christian', 'men', 'women', 'ready', 'make', 'change', 'ik', 'org']\n",
            "Text after Lemmatization: ['stand', 'counted', 'feel', 'free', 'message', 'question', 'ikk', 'kk', 'currently', 'taking', 'application', 'looking', 'honest', 'christian', 'men', 'woman', 'ready', 'make', 'change', 'ik', 'org']\n",
            "Final pre-processed text: stand counted feel free message question ikk kk currently taking application looking honest christian men woman ready make change ik org\n",
            "Text after removing HTML tags: anon attempts to ride the bus in the ghetto while being white niggerstories part \n",
            "Text after removing non-alphabetic characters and converting to lowercase: anon attempts to ride the bus in the ghetto while being white niggerstories part \n",
            "Text after tokenization: ['anon', 'attempts', 'to', 'ride', 'the', 'bus', 'in', 'the', 'ghetto', 'while', 'being', 'white', 'niggerstories', 'part']\n",
            "Text after removing stop words: ['anon', 'attempts', 'ride', 'bus', 'ghetto', 'white', 'niggerstories', 'part']\n",
            "Text after Lemmatization: ['anon', 'attempt', 'ride', 'bus', 'ghetto', 'white', 'niggerstories', 'part']\n",
            "Final pre-processed text: anon attempt ride bus ghetto white niggerstories part\n",
            "Text after removing HTML tags: rt illimitableman free speech is a release valve for tension it prevents violence by allowing people to air their grievances and in turn negotiate with the other side obstructions to free speech are an obstruction to the catharsis and negotiation process this is not good for a societ\n",
            "Text after removing non-alphabetic characters and converting to lowercase: rt illimitableman free speech is a release valve for tension it prevents violence by allowing people to air their grievances and in turn negotiate with the other side obstructions to free speech are an obstruction to the catharsis and negotiation process this is not good for a societ\n",
            "Text after tokenization: ['rt', 'illimitableman', 'free', 'speech', 'is', 'a', 'release', 'valve', 'for', 'tension', 'it', 'prevents', 'violence', 'by', 'allowing', 'people', 'to', 'air', 'their', 'grievances', 'and', 'in', 'turn', 'negotiate', 'with', 'the', 'other', 'side', 'obstructions', 'to', 'free', 'speech', 'are', 'an', 'obstruction', 'to', 'the', 'catharsis', 'and', 'negotiation', 'process', 'this', 'is', 'not', 'good', 'for', 'a', 'societ']\n",
            "Text after removing stop words: ['rt', 'illimitableman', 'free', 'speech', 'release', 'valve', 'tension', 'prevents', 'violence', 'allowing', 'people', 'air', 'grievances', 'turn', 'negotiate', 'side', 'obstructions', 'free', 'speech', 'obstruction', 'catharsis', 'negotiation', 'process', 'good', 'societ']\n",
            "Text after Lemmatization: ['rt', 'illimitableman', 'free', 'speech', 'release', 'valve', 'tension', 'prevents', 'violence', 'allowing', 'people', 'air', 'grievance', 'turn', 'negotiate', 'side', 'obstruction', 'free', 'speech', 'obstruction', 'catharsis', 'negotiation', 'process', 'good', 'societ']\n",
            "Final pre-processed text: rt illimitableman free speech release valve tension prevents violence allowing people air grievance turn negotiate side obstruction free speech obstruction catharsis negotiation process good societ\n",
            "Text after removing HTML tags: leftie americans say arabs are white leftie brits say arabs are asian so the left really does believe people are transracial or is the left complicit with violently criminal arabs\n",
            "Text after removing non-alphabetic characters and converting to lowercase: leftie americans say arabs are white leftie brits say arabs are asian so the left really does believe people are transracial or is the left complicit with violently criminal arabs\n",
            "Text after tokenization: ['leftie', 'americans', 'say', 'arabs', 'are', 'white', 'leftie', 'brits', 'say', 'arabs', 'are', 'asian', 'so', 'the', 'left', 'really', 'does', 'believe', 'people', 'are', 'transracial', 'or', 'is', 'the', 'left', 'complicit', 'with', 'violently', 'criminal', 'arabs']\n",
            "Text after removing stop words: ['leftie', 'americans', 'say', 'arabs', 'white', 'leftie', 'brits', 'say', 'arabs', 'asian', 'left', 'really', 'believe', 'people', 'transracial', 'left', 'complicit', 'violently', 'criminal', 'arabs']\n",
            "Text after Lemmatization: ['leftie', 'american', 'say', 'arab', 'white', 'leftie', 'brit', 'say', 'arab', 'asian', 'left', 'really', 'believe', 'people', 'transracial', 'left', 'complicit', 'violently', 'criminal', 'arab']\n",
            "Final pre-processed text: leftie american say arab white leftie brit say arab asian left really believe people transracial left complicit violently criminal arab\n",
            "Text after removing HTML tags: so just asians now right\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so just asians now right\n",
            "Text after tokenization: ['so', 'just', 'asians', 'now', 'right']\n",
            "Text after removing stop words: ['asians', 'right']\n",
            "Text after Lemmatization: ['asian', 'right']\n",
            "Final pre-processed text: asian right\n",
            "Text after removing HTML tags: it does not have to be perfect and eventually it gets easier keep scrapping white people try to have perfection and erase every bit of bias and what could be called racism the commies and blax just say shit they don t care it is a war not a math problem\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it does not have to be perfect and eventually it gets easier keep scrapping white people try to have perfection and erase every bit of bias and what could be called racism the commies and blax just say shit they don t care it is a war not a math problem\n",
            "Text after tokenization: ['it', 'does', 'not', 'have', 'to', 'be', 'perfect', 'and', 'eventually', 'it', 'gets', 'easier', 'keep', 'scrapping', 'white', 'people', 'try', 'to', 'have', 'perfection', 'and', 'erase', 'every', 'bit', 'of', 'bias', 'and', 'what', 'could', 'be', 'called', 'racism', 'the', 'commies', 'and', 'blax', 'just', 'say', 'shit', 'they', 'don', 't', 'care', 'it', 'is', 'a', 'war', 'not', 'a', 'math', 'problem']\n",
            "Text after removing stop words: ['perfect', 'eventually', 'gets', 'easier', 'keep', 'scrapping', 'white', 'people', 'try', 'perfection', 'erase', 'every', 'bit', 'bias', 'could', 'called', 'racism', 'commies', 'blax', 'say', 'shit', 'care', 'war', 'math', 'problem']\n",
            "Text after Lemmatization: ['perfect', 'eventually', 'get', 'easier', 'keep', 'scrapping', 'white', 'people', 'try', 'perfection', 'erase', 'every', 'bit', 'bias', 'could', 'called', 'racism', 'commie', 'blax', 'say', 'shit', 'care', 'war', 'math', 'problem']\n",
            "Final pre-processed text: perfect eventually get easier keep scrapping white people try perfection erase every bit bias could called racism commie blax say shit care war math problem\n",
            "Text after removing HTML tags: i do not understand why she apologized faggot is a wonderful word\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i do not understand why she apologized faggot is a wonderful word\n",
            "Text after tokenization: ['i', 'do', 'not', 'understand', 'why', 'she', 'apologized', 'faggot', 'is', 'a', 'wonderful', 'word']\n",
            "Text after removing stop words: ['understand', 'apologized', 'faggot', 'wonderful', 'word']\n",
            "Text after Lemmatization: ['understand', 'apologized', 'faggot', 'wonderful', 'word']\n",
            "Final pre-processed text: understand apologized faggot wonderful word\n",
            "Text after removing HTML tags: stop calling them illegal immigrants you bunch of fucking politically correct pussies these people are vile parasitic scum very likely all linked to the mexican cartel they are all drug mules and gangbangers every single last one of them even the children they should all be exterminated immediately\n",
            "Text after removing non-alphabetic characters and converting to lowercase: stop calling them illegal immigrants you bunch of fucking politically correct pussies these people are vile parasitic scum very likely all linked to the mexican cartel they are all drug mules and gangbangers every single last one of them even the children they should all be exterminated immediately\n",
            "Text after tokenization: ['stop', 'calling', 'them', 'illegal', 'immigrants', 'you', 'bunch', 'of', 'fucking', 'politically', 'correct', 'pussies', 'these', 'people', 'are', 'vile', 'parasitic', 'scum', 'very', 'likely', 'all', 'linked', 'to', 'the', 'mexican', 'cartel', 'they', 'are', 'all', 'drug', 'mules', 'and', 'gangbangers', 'every', 'single', 'last', 'one', 'of', 'them', 'even', 'the', 'children', 'they', 'should', 'all', 'be', 'exterminated', 'immediately']\n",
            "Text after removing stop words: ['stop', 'calling', 'illegal', 'immigrants', 'bunch', 'fucking', 'politically', 'correct', 'pussies', 'people', 'vile', 'parasitic', 'scum', 'likely', 'linked', 'mexican', 'cartel', 'drug', 'mules', 'gangbangers', 'every', 'single', 'last', 'one', 'even', 'children', 'exterminated', 'immediately']\n",
            "Text after Lemmatization: ['stop', 'calling', 'illegal', 'immigrant', 'bunch', 'fucking', 'politically', 'correct', 'pussy', 'people', 'vile', 'parasitic', 'scum', 'likely', 'linked', 'mexican', 'cartel', 'drug', 'mule', 'gangbangers', 'every', 'single', 'last', 'one', 'even', 'child', 'exterminated', 'immediately']\n",
            "Final pre-processed text: stop calling illegal immigrant bunch fucking politically correct pussy people vile parasitic scum likely linked mexican cartel drug mule gangbangers every single last one even child exterminated immediately\n",
            "Text after removing HTML tags: definitely a little kike fucker behind this\n",
            "Text after removing non-alphabetic characters and converting to lowercase: definitely a little kike fucker behind this\n",
            "Text after tokenization: ['definitely', 'a', 'little', 'kike', 'fucker', 'behind', 'this']\n",
            "Text after removing stop words: ['definitely', 'little', 'kike', 'fucker', 'behind']\n",
            "Text after Lemmatization: ['definitely', 'little', 'kike', 'fucker', 'behind']\n",
            "Final pre-processed text: definitely little kike fucker behind\n",
            "Text after removing HTML tags: bingo hates americans hates our values hates our country and you civnat faggots pay his tuition\n",
            "Text after removing non-alphabetic characters and converting to lowercase: bingo hates americans hates our values hates our country and you civnat faggots pay his tuition\n",
            "Text after tokenization: ['bingo', 'hates', 'americans', 'hates', 'our', 'values', 'hates', 'our', 'country', 'and', 'you', 'civnat', 'faggots', 'pay', 'his', 'tuition']\n",
            "Text after removing stop words: ['bingo', 'hates', 'americans', 'hates', 'values', 'hates', 'country', 'civnat', 'faggots', 'pay', 'tuition']\n",
            "Text after Lemmatization: ['bingo', 'hate', 'american', 'hate', 'value', 'hate', 'country', 'civnat', 'faggot', 'pay', 'tuition']\n",
            "Final pre-processed text: bingo hate american hate value hate country civnat faggot pay tuition\n",
            "Text after removing HTML tags: trump is outraged over mexico allowing immigrants into our country har har har cause it their job to protect our border hey neighbor i am mad you did not protect my house from these criminals i doubt trump will do anything about this only brave white men in texas will\n",
            "Text after removing non-alphabetic characters and converting to lowercase: trump is outraged over mexico allowing immigrants into our country har har har cause it their job to protect our border hey neighbor i am mad you did not protect my house from these criminals i doubt trump will do anything about this only brave white men in texas will\n",
            "Text after tokenization: ['trump', 'is', 'outraged', 'over', 'mexico', 'allowing', 'immigrants', 'into', 'our', 'country', 'har', 'har', 'har', 'cause', 'it', 'their', 'job', 'to', 'protect', 'our', 'border', 'hey', 'neighbor', 'i', 'am', 'mad', 'you', 'did', 'not', 'protect', 'my', 'house', 'from', 'these', 'criminals', 'i', 'doubt', 'trump', 'will', 'do', 'anything', 'about', 'this', 'only', 'brave', 'white', 'men', 'in', 'texas', 'will']\n",
            "Text after removing stop words: ['trump', 'outraged', 'mexico', 'allowing', 'immigrants', 'country', 'har', 'har', 'har', 'cause', 'job', 'protect', 'border', 'hey', 'neighbor', 'mad', 'protect', 'house', 'criminals', 'doubt', 'trump', 'anything', 'brave', 'white', 'men', 'texas']\n",
            "Text after Lemmatization: ['trump', 'outraged', 'mexico', 'allowing', 'immigrant', 'country', 'har', 'har', 'har', 'cause', 'job', 'protect', 'border', 'hey', 'neighbor', 'mad', 'protect', 'house', 'criminal', 'doubt', 'trump', 'anything', 'brave', 'white', 'men', 'texas']\n",
            "Final pre-processed text: trump outraged mexico allowing immigrant country har har har cause job protect border hey neighbor mad protect house criminal doubt trump anything brave white men texas\n",
            "Text after removing HTML tags: slut a person especially a woman considered sexually promiscuous promiscuous having casual sexual relations frequently with different partners indiscriminate in the choice of sexual partners  ‍ theoretically if she seriously rarely thrice fucks only fucks niggers then i suppose she is not a slut but she a mud shark\n",
            "Text after removing non-alphabetic characters and converting to lowercase: slut a person especially a woman considered sexually promiscuous promiscuous having casual sexual relations frequently with different partners indiscriminate in the choice of sexual partners    theoretically if she seriously rarely thrice fucks only fucks niggers then i suppose she is not a slut but she a mud shark\n",
            "Text after tokenization: ['slut', 'a', 'person', 'especially', 'a', 'woman', 'considered', 'sexually', 'promiscuous', 'promiscuous', 'having', 'casual', 'sexual', 'relations', 'frequently', 'with', 'different', 'partners', 'indiscriminate', 'in', 'the', 'choice', 'of', 'sexual', 'partners', 'theoretically', 'if', 'she', 'seriously', 'rarely', 'thrice', 'fucks', 'only', 'fucks', 'niggers', 'then', 'i', 'suppose', 'she', 'is', 'not', 'a', 'slut', 'but', 'she', 'a', 'mud', 'shark']\n",
            "Text after removing stop words: ['slut', 'person', 'especially', 'woman', 'considered', 'sexually', 'promiscuous', 'promiscuous', 'casual', 'sexual', 'relations', 'frequently', 'different', 'partners', 'indiscriminate', 'choice', 'sexual', 'partners', 'theoretically', 'seriously', 'rarely', 'thrice', 'fucks', 'fucks', 'niggers', 'suppose', 'slut', 'mud', 'shark']\n",
            "Text after Lemmatization: ['slut', 'person', 'especially', 'woman', 'considered', 'sexually', 'promiscuous', 'promiscuous', 'casual', 'sexual', 'relation', 'frequently', 'different', 'partner', 'indiscriminate', 'choice', 'sexual', 'partner', 'theoretically', 'seriously', 'rarely', 'thrice', 'fuck', 'fuck', 'nigger', 'suppose', 'slut', 'mud', 'shark']\n",
            "Final pre-processed text: slut person especially woman considered sexually promiscuous promiscuous casual sexual relation frequently different partner indiscriminate choice sexual partner theoretically seriously rarely thrice fuck fuck nigger suppose slut mud shark\n",
            "Text after removing HTML tags: the isis flag says all jews  and rabbis in israel believe that god created isis using angels to punish the nations that deserve to be punished these are isis flags in dearborn michigan usa you do the math\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the isis flag says all jews  and rabbis in israel believe that god created isis using angels to punish the nations that deserve to be punished these are isis flags in dearborn michigan usa you do the math\n",
            "Text after tokenization: ['the', 'isis', 'flag', 'says', 'all', 'jews', 'and', 'rabbis', 'in', 'israel', 'believe', 'that', 'god', 'created', 'isis', 'using', 'angels', 'to', 'punish', 'the', 'nations', 'that', 'deserve', 'to', 'be', 'punished', 'these', 'are', 'isis', 'flags', 'in', 'dearborn', 'michigan', 'usa', 'you', 'do', 'the', 'math']\n",
            "Text after removing stop words: ['isis', 'flag', 'says', 'jews', 'rabbis', 'israel', 'believe', 'god', 'created', 'isis', 'using', 'angels', 'punish', 'nations', 'deserve', 'punished', 'isis', 'flags', 'dearborn', 'michigan', 'usa', 'math']\n",
            "Text after Lemmatization: ['isi', 'flag', 'say', 'jew', 'rabbi', 'israel', 'believe', 'god', 'created', 'isi', 'using', 'angel', 'punish', 'nation', 'deserve', 'punished', 'isi', 'flag', 'dearborn', 'michigan', 'usa', 'math']\n",
            "Final pre-processed text: isi flag say jew rabbi israel believe god created isi using angel punish nation deserve punished isi flag dearborn michigan usa math\n",
            "Text after removing HTML tags: he funny as shit i do not know of anyone else who captures the experience of being a modern loser and pervert so well his article on how a gay guy stole his underwear at the gym is hilarious\n",
            "Text after removing non-alphabetic characters and converting to lowercase: he funny as shit i do not know of anyone else who captures the experience of being a modern loser and pervert so well his article on how a gay guy stole his underwear at the gym is hilarious\n",
            "Text after tokenization: ['he', 'funny', 'as', 'shit', 'i', 'do', 'not', 'know', 'of', 'anyone', 'else', 'who', 'captures', 'the', 'experience', 'of', 'being', 'a', 'modern', 'loser', 'and', 'pervert', 'so', 'well', 'his', 'article', 'on', 'how', 'a', 'gay', 'guy', 'stole', 'his', 'underwear', 'at', 'the', 'gym', 'is', 'hilarious']\n",
            "Text after removing stop words: ['funny', 'shit', 'know', 'anyone', 'else', 'captures', 'experience', 'modern', 'loser', 'pervert', 'well', 'article', 'gay', 'guy', 'stole', 'underwear', 'gym', 'hilarious']\n",
            "Text after Lemmatization: ['funny', 'shit', 'know', 'anyone', 'else', 'capture', 'experience', 'modern', 'loser', 'pervert', 'well', 'article', 'gay', 'guy', 'stole', 'underwear', 'gym', 'hilarious']\n",
            "Final pre-processed text: funny shit know anyone else capture experience modern loser pervert well article gay guy stole underwear gym hilarious\n",
            "Text after removing HTML tags: but black women are humans lmao\n",
            "Text after removing non-alphabetic characters and converting to lowercase: but black women are humans lmao\n",
            "Text after tokenization: ['but', 'black', 'women', 'are', 'humans', 'lmao']\n",
            "Text after removing stop words: ['black', 'women', 'humans', 'lmao']\n",
            "Text after Lemmatization: ['black', 'woman', 'human', 'lmao']\n",
            "Final pre-processed text: black woman human lmao\n",
            "Text after removing HTML tags: james is ghetto worthless trash\n",
            "Text after removing non-alphabetic characters and converting to lowercase: james is ghetto worthless trash\n",
            "Text after tokenization: ['james', 'is', 'ghetto', 'worthless', 'trash']\n",
            "Text after removing stop words: ['james', 'ghetto', 'worthless', 'trash']\n",
            "Text after Lemmatization: ['james', 'ghetto', 'worthless', 'trash']\n",
            "Final pre-processed text: james ghetto worthless trash\n",
            "Text after removing HTML tags: so let me see if iget this str8 wn1  was an eloaborate ruse conducted by jews in the mid late  that brought wlp na crashing down is that youre saying\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so let me see if iget this str  wn   was an eloaborate ruse conducted by jews in the mid late  that brought wlp na crashing down is that youre saying\n",
            "Text after tokenization: ['so', 'let', 'me', 'see', 'if', 'iget', 'this', 'str', 'wn', 'was', 'an', 'eloaborate', 'ruse', 'conducted', 'by', 'jews', 'in', 'the', 'mid', 'late', 'that', 'brought', 'wlp', 'na', 'crashing', 'down', 'is', 'that', 'youre', 'saying']\n",
            "Text after removing stop words: ['let', 'see', 'iget', 'str', 'wn', 'eloaborate', 'ruse', 'conducted', 'jews', 'mid', 'late', 'brought', 'wlp', 'na', 'crashing', 'youre', 'saying']\n",
            "Text after Lemmatization: ['let', 'see', 'iget', 'str', 'wn', 'eloaborate', 'ruse', 'conducted', 'jew', 'mid', 'late', 'brought', 'wlp', 'na', 'crashing', 'youre', 'saying']\n",
            "Final pre-processed text: let see iget str wn eloaborate ruse conducted jew mid late brought wlp na crashing youre saying\n",
            "Text after removing HTML tags: a brit a swede and an american are in a room together do they all have the same culture or do they each have their own identity ricky is talking about uniting white subcultures not pretending we are all pack animals confirmation bias can be brutal sometimes 👍\n",
            "Text after removing non-alphabetic characters and converting to lowercase: a brit a swede and an american are in a room together do they all have the same culture or do they each have their own identity ricky is talking about uniting white subcultures not pretending we are all pack animals confirmation bias can be brutal sometimes  \n",
            "Text after tokenization: ['a', 'brit', 'a', 'swede', 'and', 'an', 'american', 'are', 'in', 'a', 'room', 'together', 'do', 'they', 'all', 'have', 'the', 'same', 'culture', 'or', 'do', 'they', 'each', 'have', 'their', 'own', 'identity', 'ricky', 'is', 'talking', 'about', 'uniting', 'white', 'subcultures', 'not', 'pretending', 'we', 'are', 'all', 'pack', 'animals', 'confirmation', 'bias', 'can', 'be', 'brutal', 'sometimes']\n",
            "Text after removing stop words: ['brit', 'swede', 'american', 'room', 'together', 'culture', 'identity', 'ricky', 'talking', 'uniting', 'white', 'subcultures', 'pretending', 'pack', 'animals', 'confirmation', 'bias', 'brutal', 'sometimes']\n",
            "Text after Lemmatization: ['brit', 'swede', 'american', 'room', 'together', 'culture', 'identity', 'ricky', 'talking', 'uniting', 'white', 'subculture', 'pretending', 'pack', 'animal', 'confirmation', 'bias', 'brutal', 'sometimes']\n",
            "Final pre-processed text: brit swede american room together culture identity ricky talking uniting white subculture pretending pack animal confirmation bias brutal sometimes\n",
            "Text after removing HTML tags: the cold adapted eurasians of the northern hemisphere will make way for the homo superior as the thin white duke would put it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the cold adapted eurasians of the northern hemisphere will make way for the homo superior as the thin white duke would put it\n",
            "Text after tokenization: ['the', 'cold', 'adapted', 'eurasians', 'of', 'the', 'northern', 'hemisphere', 'will', 'make', 'way', 'for', 'the', 'homo', 'superior', 'as', 'the', 'thin', 'white', 'duke', 'would', 'put', 'it']\n",
            "Text after removing stop words: ['cold', 'adapted', 'eurasians', 'northern', 'hemisphere', 'make', 'way', 'homo', 'superior', 'thin', 'white', 'duke', 'would', 'put']\n",
            "Text after Lemmatization: ['cold', 'adapted', 'eurasian', 'northern', 'hemisphere', 'make', 'way', 'homo', 'superior', 'thin', 'white', 'duke', 'would', 'put']\n",
            "Final pre-processed text: cold adapted eurasian northern hemisphere make way homo superior thin white duke would put\n",
            "Text after removing HTML tags: this is what you nazi faggots do all day\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this is what you nazi faggots do all day\n",
            "Text after tokenization: ['this', 'is', 'what', 'you', 'nazi', 'faggots', 'do', 'all', 'day']\n",
            "Text after removing stop words: ['nazi', 'faggots', 'day']\n",
            "Text after Lemmatization: ['nazi', 'faggot', 'day']\n",
            "Final pre-processed text: nazi faggot day\n",
            "Text after removing HTML tags: this is why women having careers is not our people this is why any non white immigration is not our people this is why doing drugs is not our people this is why going down to home depot for illegal labor is not our people you cannot violate this principle and be our people\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this is why women having careers is not our people this is why any non white immigration is not our people this is why doing drugs is not our people this is why going down to home depot for illegal labor is not our people you cannot violate this principle and be our people\n",
            "Text after tokenization: ['this', 'is', 'why', 'women', 'having', 'careers', 'is', 'not', 'our', 'people', 'this', 'is', 'why', 'any', 'non', 'white', 'immigration', 'is', 'not', 'our', 'people', 'this', 'is', 'why', 'doing', 'drugs', 'is', 'not', 'our', 'people', 'this', 'is', 'why', 'going', 'down', 'to', 'home', 'depot', 'for', 'illegal', 'labor', 'is', 'not', 'our', 'people', 'you', 'can', 'not', 'violate', 'this', 'principle', 'and', 'be', 'our', 'people']\n",
            "Text after removing stop words: ['women', 'careers', 'people', 'non', 'white', 'immigration', 'people', 'drugs', 'people', 'going', 'home', 'depot', 'illegal', 'labor', 'people', 'violate', 'principle', 'people']\n",
            "Text after Lemmatization: ['woman', 'career', 'people', 'non', 'white', 'immigration', 'people', 'drug', 'people', 'going', 'home', 'depot', 'illegal', 'labor', 'people', 'violate', 'principle', 'people']\n",
            "Final pre-processed text: woman career people non white immigration people drug people going home depot illegal labor people violate principle people\n",
            "Text after removing HTML tags: so if we meet  moslem who does not try to kill us that makes islam an acceptable totalitarian political ideology how would the   people murdered by moslems feel about this\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so if we meet  moslem who does not try to kill us that makes islam an acceptable totalitarian political ideology how would the   people murdered by moslems feel about this\n",
            "Text after tokenization: ['so', 'if', 'we', 'meet', 'moslem', 'who', 'does', 'not', 'try', 'to', 'kill', 'us', 'that', 'makes', 'islam', 'an', 'acceptable', 'totalitarian', 'political', 'ideology', 'how', 'would', 'the', 'people', 'murdered', 'by', 'moslems', 'feel', 'about', 'this']\n",
            "Text after removing stop words: ['meet', 'moslem', 'try', 'kill', 'us', 'makes', 'islam', 'acceptable', 'totalitarian', 'political', 'ideology', 'would', 'people', 'murdered', 'moslems', 'feel']\n",
            "Text after Lemmatization: ['meet', 'moslem', 'try', 'kill', 'u', 'make', 'islam', 'acceptable', 'totalitarian', 'political', 'ideology', 'would', 'people', 'murdered', 'moslem', 'feel']\n",
            "Final pre-processed text: meet moslem try kill u make islam acceptable totalitarian political ideology would people murdered moslem feel\n",
            "Text after removing HTML tags: nasty women who hate to see a pretty lady on the tv or in the papers this does not offend me one bit very insecure people in this world sadly\n",
            "Text after removing non-alphabetic characters and converting to lowercase: nasty women who hate to see a pretty lady on the tv or in the papers this does not offend me one bit very insecure people in this world sadly\n",
            "Text after tokenization: ['nasty', 'women', 'who', 'hate', 'to', 'see', 'a', 'pretty', 'lady', 'on', 'the', 'tv', 'or', 'in', 'the', 'papers', 'this', 'does', 'not', 'offend', 'me', 'one', 'bit', 'very', 'insecure', 'people', 'in', 'this', 'world', 'sadly']\n",
            "Text after removing stop words: ['nasty', 'women', 'hate', 'see', 'pretty', 'lady', 'tv', 'papers', 'offend', 'one', 'bit', 'insecure', 'people', 'world', 'sadly']\n",
            "Text after Lemmatization: ['nasty', 'woman', 'hate', 'see', 'pretty', 'lady', 'tv', 'paper', 'offend', 'one', 'bit', 'insecure', 'people', 'world', 'sadly']\n",
            "Final pre-processed text: nasty woman hate see pretty lady tv paper offend one bit insecure people world sadly\n",
            "Text after removing HTML tags: america n all of western civilization needs to take lessons from poland hungary n the czech republic n get every refugee n jehadists islamic muslim out of our countries\n",
            "Text after removing non-alphabetic characters and converting to lowercase: america n all of western civilization needs to take lessons from poland hungary n the czech republic n get every refugee n jehadists islamic muslim out of our countries\n",
            "Text after tokenization: ['america', 'n', 'all', 'of', 'western', 'civilization', 'needs', 'to', 'take', 'lessons', 'from', 'poland', 'hungary', 'n', 'the', 'czech', 'republic', 'n', 'get', 'every', 'refugee', 'n', 'jehadists', 'islamic', 'muslim', 'out', 'of', 'our', 'countries']\n",
            "Text after removing stop words: ['america', 'n', 'western', 'civilization', 'needs', 'take', 'lessons', 'poland', 'hungary', 'n', 'czech', 'republic', 'n', 'get', 'every', 'refugee', 'n', 'jehadists', 'islamic', 'muslim', 'countries']\n",
            "Text after Lemmatization: ['america', 'n', 'western', 'civilization', 'need', 'take', 'lesson', 'poland', 'hungary', 'n', 'czech', 'republic', 'n', 'get', 'every', 'refugee', 'n', 'jehadists', 'islamic', 'muslim', 'country']\n",
            "Final pre-processed text: america n western civilization need take lesson poland hungary n czech republic n get every refugee n jehadists islamic muslim country\n",
            "Text after removing HTML tags: invisible white guy fed up of getting only  of my pay check to pay for left wing fantasies\n",
            "Text after removing non-alphabetic characters and converting to lowercase: invisible white guy fed up of getting only  of my pay check to pay for left wing fantasies\n",
            "Text after tokenization: ['invisible', 'white', 'guy', 'fed', 'up', 'of', 'getting', 'only', 'of', 'my', 'pay', 'check', 'to', 'pay', 'for', 'left', 'wing', 'fantasies']\n",
            "Text after removing stop words: ['invisible', 'white', 'guy', 'fed', 'getting', 'pay', 'check', 'pay', 'left', 'wing', 'fantasies']\n",
            "Text after Lemmatization: ['invisible', 'white', 'guy', 'fed', 'getting', 'pay', 'check', 'pay', 'left', 'wing', 'fantasy']\n",
            "Final pre-processed text: invisible white guy fed getting pay check pay left wing fantasy\n",
            "Text after removing HTML tags: lmaoo yeah real funny dumbass like i said usually when blacks place a real interest in a sport they tend to excel in it because blacks are more athletic than whites anything measurable yet\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lmaoo yeah real funny dumbass like i said usually when blacks place a real interest in a sport they tend to excel in it because blacks are more athletic than whites anything measurable yet\n",
            "Text after tokenization: ['lmaoo', 'yeah', 'real', 'funny', 'dumbass', 'like', 'i', 'said', 'usually', 'when', 'blacks', 'place', 'a', 'real', 'interest', 'in', 'a', 'sport', 'they', 'tend', 'to', 'excel', 'in', 'it', 'because', 'blacks', 'are', 'more', 'athletic', 'than', 'whites', 'anything', 'measurable', 'yet']\n",
            "Text after removing stop words: ['lmaoo', 'yeah', 'real', 'funny', 'dumbass', 'like', 'said', 'usually', 'blacks', 'place', 'real', 'interest', 'sport', 'tend', 'excel', 'blacks', 'athletic', 'whites', 'anything', 'measurable', 'yet']\n",
            "Text after Lemmatization: ['lmaoo', 'yeah', 'real', 'funny', 'dumbass', 'like', 'said', 'usually', 'black', 'place', 'real', 'interest', 'sport', 'tend', 'excel', 'black', 'athletic', 'white', 'anything', 'measurable', 'yet']\n",
            "Final pre-processed text: lmaoo yeah real funny dumbass like said usually black place real interest sport tend excel black athletic white anything measurable yet\n",
            "Text after removing HTML tags: this country was built on immigration but those immigrants had to go through the process no one should be exempt from said process my uncle worked his ass off to become a legal citizen and just letting anyone in shits all over his hard work and everyone else end rant\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this country was built on immigration but those immigrants had to go through the process no one should be exempt from said process my uncle worked his ass off to become a legal citizen and just letting anyone in shits all over his hard work and everyone else end rant\n",
            "Text after tokenization: ['this', 'country', 'was', 'built', 'on', 'immigration', 'but', 'those', 'immigrants', 'had', 'to', 'go', 'through', 'the', 'process', 'no', 'one', 'should', 'be', 'exempt', 'from', 'said', 'process', 'my', 'uncle', 'worked', 'his', 'ass', 'off', 'to', 'become', 'a', 'legal', 'citizen', 'and', 'just', 'letting', 'anyone', 'in', 'shits', 'all', 'over', 'his', 'hard', 'work', 'and', 'everyone', 'else', 'end', 'rant']\n",
            "Text after removing stop words: ['country', 'built', 'immigration', 'immigrants', 'go', 'process', 'one', 'exempt', 'said', 'process', 'uncle', 'worked', 'ass', 'become', 'legal', 'citizen', 'letting', 'anyone', 'shits', 'hard', 'work', 'everyone', 'else', 'end', 'rant']\n",
            "Text after Lemmatization: ['country', 'built', 'immigration', 'immigrant', 'go', 'process', 'one', 'exempt', 'said', 'process', 'uncle', 'worked', 'as', 'become', 'legal', 'citizen', 'letting', 'anyone', 'shit', 'hard', 'work', 'everyone', 'else', 'end', 'rant']\n",
            "Final pre-processed text: country built immigration immigrant go process one exempt said process uncle worked as become legal citizen letting anyone shit hard work everyone else end rant\n",
            "Text after removing HTML tags: pro refugee ✅ anti gun violence ✅ anti trump ✅ anti war in syria ❌ how is this possible\n",
            "Text after removing non-alphabetic characters and converting to lowercase: pro refugee   anti gun violence   anti trump   anti war in syria   how is this possible\n",
            "Text after tokenization: ['pro', 'refugee', 'anti', 'gun', 'violence', 'anti', 'trump', 'anti', 'war', 'in', 'syria', 'how', 'is', 'this', 'possible']\n",
            "Text after removing stop words: ['pro', 'refugee', 'anti', 'gun', 'violence', 'anti', 'trump', 'anti', 'war', 'syria', 'possible']\n",
            "Text after Lemmatization: ['pro', 'refugee', 'anti', 'gun', 'violence', 'anti', 'trump', 'anti', 'war', 'syria', 'possible']\n",
            "Final pre-processed text: pro refugee anti gun violence anti trump anti war syria possible\n",
            "Text after removing HTML tags: lol you must be butthurt to come on here and make an account just to hurl your kike bullshit go bite a baby dick heeb\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lol you must be butthurt to come on here and make an account just to hurl your kike bullshit go bite a baby dick heeb\n",
            "Text after tokenization: ['lol', 'you', 'must', 'be', 'butthurt', 'to', 'come', 'on', 'here', 'and', 'make', 'an', 'account', 'just', 'to', 'hurl', 'your', 'kike', 'bullshit', 'go', 'bite', 'a', 'baby', 'dick', 'heeb']\n",
            "Text after removing stop words: ['lol', 'must', 'butthurt', 'come', 'make', 'account', 'hurl', 'kike', 'bullshit', 'go', 'bite', 'baby', 'dick', 'heeb']\n",
            "Text after Lemmatization: ['lol', 'must', 'butthurt', 'come', 'make', 'account', 'hurl', 'kike', 'bullshit', 'go', 'bite', 'baby', 'dick', 'heeb']\n",
            "Final pre-processed text: lol must butthurt come make account hurl kike bullshit go bite baby dick heeb\n",
            "Text after removing HTML tags: do you lads like yachts are you aware that caligula had the first yachts built out of wood of course not because he was deranged and had sex with a horse maybe just maybe that is a lie no goy  gorrillion jews died\n",
            "Text after removing non-alphabetic characters and converting to lowercase: do you lads like yachts are you aware that caligula had the first yachts built out of wood of course not because he was deranged and had sex with a horse maybe just maybe that is a lie no goy  gorrillion jews died\n",
            "Text after tokenization: ['do', 'you', 'lads', 'like', 'yachts', 'are', 'you', 'aware', 'that', 'caligula', 'had', 'the', 'first', 'yachts', 'built', 'out', 'of', 'wood', 'of', 'course', 'not', 'because', 'he', 'was', 'deranged', 'and', 'had', 'sex', 'with', 'a', 'horse', 'maybe', 'just', 'maybe', 'that', 'is', 'a', 'lie', 'no', 'goy', 'gorrillion', 'jews', 'died']\n",
            "Text after removing stop words: ['lads', 'like', 'yachts', 'aware', 'caligula', 'first', 'yachts', 'built', 'wood', 'course', 'deranged', 'sex', 'horse', 'maybe', 'maybe', 'lie', 'goy', 'gorrillion', 'jews', 'died']\n",
            "Text after Lemmatization: ['lad', 'like', 'yacht', 'aware', 'caligula', 'first', 'yacht', 'built', 'wood', 'course', 'deranged', 'sex', 'horse', 'maybe', 'maybe', 'lie', 'goy', 'gorrillion', 'jew', 'died']\n",
            "Final pre-processed text: lad like yacht aware caligula first yacht built wood course deranged sex horse maybe maybe lie goy gorrillion jew died\n",
            "Text after removing HTML tags: so the un got trump to be their jewdog puppet spend millions of to bomb a christian leader and christian ppl to satisfy the jews and mussies we the ppl will never win\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so the un got trump to be their jewdog puppet spend millions of to bomb a christian leader and christian ppl to satisfy the jews and mussies we the ppl will never win\n",
            "Text after tokenization: ['so', 'the', 'un', 'got', 'trump', 'to', 'be', 'their', 'jewdog', 'puppet', 'spend', 'millions', 'of', 'to', 'bomb', 'a', 'christian', 'leader', 'and', 'christian', 'ppl', 'to', 'satisfy', 'the', 'jews', 'and', 'mussies', 'we', 'the', 'ppl', 'will', 'never', 'win']\n",
            "Text after removing stop words: ['un', 'got', 'trump', 'jewdog', 'puppet', 'spend', 'millions', 'bomb', 'christian', 'leader', 'christian', 'ppl', 'satisfy', 'jews', 'mussies', 'ppl', 'never', 'win']\n",
            "Text after Lemmatization: ['un', 'got', 'trump', 'jewdog', 'puppet', 'spend', 'million', 'bomb', 'christian', 'leader', 'christian', 'ppl', 'satisfy', 'jew', 'mussies', 'ppl', 'never', 'win']\n",
            "Final pre-processed text: un got trump jewdog puppet spend million bomb christian leader christian ppl satisfy jew mussies ppl never win\n",
            "Text after removing HTML tags: anyone consider maybe the missiles were oppressed us refugees looking for asylum in syria\n",
            "Text after removing non-alphabetic characters and converting to lowercase: anyone consider maybe the missiles were oppressed us refugees looking for asylum in syria\n",
            "Text after tokenization: ['anyone', 'consider', 'maybe', 'the', 'missiles', 'were', 'oppressed', 'us', 'refugees', 'looking', 'for', 'asylum', 'in', 'syria']\n",
            "Text after removing stop words: ['anyone', 'consider', 'maybe', 'missiles', 'oppressed', 'us', 'refugees', 'looking', 'asylum', 'syria']\n",
            "Text after Lemmatization: ['anyone', 'consider', 'maybe', 'missile', 'oppressed', 'u', 'refugee', 'looking', 'asylum', 'syria']\n",
            "Final pre-processed text: anyone consider maybe missile oppressed u refugee looking asylum syria\n",
            "Text after removing HTML tags: are those the guys who spam you with gay porn and abuse images shame you will not take action when they target your users tho  is taking the matthew prince approach to moderating\n",
            "Text after removing non-alphabetic characters and converting to lowercase: are those the guys who spam you with gay porn and abuse images shame you will not take action when they target your users tho  is taking the matthew prince approach to moderating\n",
            "Text after tokenization: ['are', 'those', 'the', 'guys', 'who', 'spam', 'you', 'with', 'gay', 'porn', 'and', 'abuse', 'images', 'shame', 'you', 'will', 'not', 'take', 'action', 'when', 'they', 'target', 'your', 'users', 'tho', 'is', 'taking', 'the', 'matthew', 'prince', 'approach', 'to', 'moderating']\n",
            "Text after removing stop words: ['guys', 'spam', 'gay', 'porn', 'abuse', 'images', 'shame', 'take', 'action', 'target', 'users', 'tho', 'taking', 'matthew', 'prince', 'approach', 'moderating']\n",
            "Text after Lemmatization: ['guy', 'spam', 'gay', 'porn', 'abuse', 'image', 'shame', 'take', 'action', 'target', 'user', 'tho', 'taking', 'matthew', 'prince', 'approach', 'moderating']\n",
            "Final pre-processed text: guy spam gay porn abuse image shame take action target user tho taking matthew prince approach moderating\n",
            "Text after removing HTML tags: rt tookenglandback politics in brexit britain is now firmly divided between patriots and liberal left wing traitors patriots will make our country great again for natives liberal and left wing traitors will continue to put the needs of immigrants asylum seekers and refugees first\n",
            "Text after removing non-alphabetic characters and converting to lowercase: rt tookenglandback politics in brexit britain is now firmly divided between patriots and liberal left wing traitors patriots will make our country great again for natives liberal and left wing traitors will continue to put the needs of immigrants asylum seekers and refugees first\n",
            "Text after tokenization: ['rt', 'tookenglandback', 'politics', 'in', 'brexit', 'britain', 'is', 'now', 'firmly', 'divided', 'between', 'patriots', 'and', 'liberal', 'left', 'wing', 'traitors', 'patriots', 'will', 'make', 'our', 'country', 'great', 'again', 'for', 'natives', 'liberal', 'and', 'left', 'wing', 'traitors', 'will', 'continue', 'to', 'put', 'the', 'needs', 'of', 'immigrants', 'asylum', 'seekers', 'and', 'refugees', 'first']\n",
            "Text after removing stop words: ['rt', 'tookenglandback', 'politics', 'brexit', 'britain', 'firmly', 'divided', 'patriots', 'liberal', 'left', 'wing', 'traitors', 'patriots', 'make', 'country', 'great', 'natives', 'liberal', 'left', 'wing', 'traitors', 'continue', 'put', 'needs', 'immigrants', 'asylum', 'seekers', 'refugees', 'first']\n",
            "Text after Lemmatization: ['rt', 'tookenglandback', 'politics', 'brexit', 'britain', 'firmly', 'divided', 'patriot', 'liberal', 'left', 'wing', 'traitor', 'patriot', 'make', 'country', 'great', 'native', 'liberal', 'left', 'wing', 'traitor', 'continue', 'put', 'need', 'immigrant', 'asylum', 'seeker', 'refugee', 'first']\n",
            "Final pre-processed text: rt tookenglandback politics brexit britain firmly divided patriot liberal left wing traitor patriot make country great native liberal left wing traitor continue put need immigrant asylum seeker refugee first\n",
            "Text after removing HTML tags: hang all sand niggers from a tree\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hang all sand niggers from a tree\n",
            "Text after tokenization: ['hang', 'all', 'sand', 'niggers', 'from', 'a', 'tree']\n",
            "Text after removing stop words: ['hang', 'sand', 'niggers', 'tree']\n",
            "Text after Lemmatization: ['hang', 'sand', 'nigger', 'tree']\n",
            "Final pre-processed text: hang sand nigger tree\n",
            "Text after removing HTML tags: attention women of all genders\n",
            "Text after removing non-alphabetic characters and converting to lowercase: attention women of all genders\n",
            "Text after tokenization: ['attention', 'women', 'of', 'all', 'genders']\n",
            "Text after removing stop words: ['attention', 'women', 'genders']\n",
            "Text after Lemmatization: ['attention', 'woman', 'gender']\n",
            "Final pre-processed text: attention woman gender\n",
            "Text after removing HTML tags: i do not know why they make the carjacker a white guy tho it not realistic irl it would be a nigger or spic\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i do not know why they make the carjacker a white guy tho it not realistic irl it would be a nigger or spic\n",
            "Text after tokenization: ['i', 'do', 'not', 'know', 'why', 'they', 'make', 'the', 'carjacker', 'a', 'white', 'guy', 'tho', 'it', 'not', 'realistic', 'irl', 'it', 'would', 'be', 'a', 'nigger', 'or', 'spic']\n",
            "Text after removing stop words: ['know', 'make', 'carjacker', 'white', 'guy', 'tho', 'realistic', 'irl', 'would', 'nigger', 'spic']\n",
            "Text after Lemmatization: ['know', 'make', 'carjacker', 'white', 'guy', 'tho', 'realistic', 'irl', 'would', 'nigger', 'spic']\n",
            "Final pre-processed text: know make carjacker white guy tho realistic irl would nigger spic\n",
            "Text after removing HTML tags: how many refugees should be admitted to the united states if your answer was greater than zero we have got a problem immigration\n",
            "Text after removing non-alphabetic characters and converting to lowercase: how many refugees should be admitted to the united states if your answer was greater than zero we have got a problem immigration\n",
            "Text after tokenization: ['how', 'many', 'refugees', 'should', 'be', 'admitted', 'to', 'the', 'united', 'states', 'if', 'your', 'answer', 'was', 'greater', 'than', 'zero', 'we', 'have', 'got', 'a', 'problem', 'immigration']\n",
            "Text after removing stop words: ['many', 'refugees', 'admitted', 'united', 'states', 'answer', 'greater', 'zero', 'got', 'problem', 'immigration']\n",
            "Text after Lemmatization: ['many', 'refugee', 'admitted', 'united', 'state', 'answer', 'greater', 'zero', 'got', 'problem', 'immigration']\n",
            "Final pre-processed text: many refugee admitted united state answer greater zero got problem immigration\n",
            "Text after removing HTML tags: where are the german men to protect this child and beat the living shit out of the muslim retards who are bullying him\n",
            "Text after removing non-alphabetic characters and converting to lowercase: where are the german men to protect this child and beat the living shit out of the muslim retards who are bullying him\n",
            "Text after tokenization: ['where', 'are', 'the', 'german', 'men', 'to', 'protect', 'this', 'child', 'and', 'beat', 'the', 'living', 'shit', 'out', 'of', 'the', 'muslim', 'retards', 'who', 'are', 'bullying', 'him']\n",
            "Text after removing stop words: ['german', 'men', 'protect', 'child', 'beat', 'living', 'shit', 'muslim', 'retards', 'bullying']\n",
            "Text after Lemmatization: ['german', 'men', 'protect', 'child', 'beat', 'living', 'shit', 'muslim', 'retard', 'bullying']\n",
            "Final pre-processed text: german men protect child beat living shit muslim retard bullying\n",
            "Text after removing HTML tags: how much longer will the west continue to put up with all of this shit from retarded third worlders and liberal cucks before we revert to a frontier mentality and just start the real violence\n",
            "Text after removing non-alphabetic characters and converting to lowercase: how much longer will the west continue to put up with all of this shit from retarded third worlders and liberal cucks before we revert to a frontier mentality and just start the real violence\n",
            "Text after tokenization: ['how', 'much', 'longer', 'will', 'the', 'west', 'continue', 'to', 'put', 'up', 'with', 'all', 'of', 'this', 'shit', 'from', 'retarded', 'third', 'worlders', 'and', 'liberal', 'cucks', 'before', 'we', 'revert', 'to', 'a', 'frontier', 'mentality', 'and', 'just', 'start', 'the', 'real', 'violence']\n",
            "Text after removing stop words: ['much', 'longer', 'west', 'continue', 'put', 'shit', 'retarded', 'third', 'worlders', 'liberal', 'cucks', 'revert', 'frontier', 'mentality', 'start', 'real', 'violence']\n",
            "Text after Lemmatization: ['much', 'longer', 'west', 'continue', 'put', 'shit', 'retarded', 'third', 'worlders', 'liberal', 'cucks', 'revert', 'frontier', 'mentality', 'start', 'real', 'violence']\n",
            "Final pre-processed text: much longer west continue put shit retarded third worlders liberal cucks revert frontier mentality start real violence\n",
            "Text after removing HTML tags: the reason the germans ended up in world war2 and your people got ostracized was the jewish elite were destroying the german economy google weinmar rebublic do your research you are a bottom feeder jew the elite jews dont care about you or anyone else\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the reason the germans ended up in world war  and your people got ostracized was the jewish elite were destroying the german economy google weinmar rebublic do your research you are a bottom feeder jew the elite jews dont care about you or anyone else\n",
            "Text after tokenization: ['the', 'reason', 'the', 'germans', 'ended', 'up', 'in', 'world', 'war', 'and', 'your', 'people', 'got', 'ostracized', 'was', 'the', 'jewish', 'elite', 'were', 'destroying', 'the', 'german', 'economy', 'google', 'weinmar', 'rebublic', 'do', 'your', 'research', 'you', 'are', 'a', 'bottom', 'feeder', 'jew', 'the', 'elite', 'jews', 'dont', 'care', 'about', 'you', 'or', 'anyone', 'else']\n",
            "Text after removing stop words: ['reason', 'germans', 'ended', 'world', 'war', 'people', 'got', 'ostracized', 'jewish', 'elite', 'destroying', 'german', 'economy', 'google', 'weinmar', 'rebublic', 'research', 'bottom', 'feeder', 'jew', 'elite', 'jews', 'dont', 'care', 'anyone', 'else']\n",
            "Text after Lemmatization: ['reason', 'german', 'ended', 'world', 'war', 'people', 'got', 'ostracized', 'jewish', 'elite', 'destroying', 'german', 'economy', 'google', 'weinmar', 'rebublic', 'research', 'bottom', 'feeder', 'jew', 'elite', 'jew', 'dont', 'care', 'anyone', 'else']\n",
            "Final pre-processed text: reason german ended world war people got ostracized jewish elite destroying german economy google weinmar rebublic research bottom feeder jew elite jew dont care anyone else\n",
            "Text after removing HTML tags: magazine you dumb goat fucking sand nigger\n",
            "Text after removing non-alphabetic characters and converting to lowercase: magazine you dumb goat fucking sand nigger\n",
            "Text after tokenization: ['magazine', 'you', 'dumb', 'goat', 'fucking', 'sand', 'nigger']\n",
            "Text after removing stop words: ['magazine', 'dumb', 'goat', 'fucking', 'sand', 'nigger']\n",
            "Text after Lemmatization: ['magazine', 'dumb', 'goat', 'fucking', 'sand', 'nigger']\n",
            "Final pre-processed text: magazine dumb goat fucking sand nigger\n",
            "Text after removing HTML tags: just a cohencidence that the only film of this event was taken by a hooknosed kike who hit the shekel jackpot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: just a cohencidence that the only film of this event was taken by a hooknosed kike who hit the shekel jackpot\n",
            "Text after tokenization: ['just', 'a', 'cohencidence', 'that', 'the', 'only', 'film', 'of', 'this', 'event', 'was', 'taken', 'by', 'a', 'hooknosed', 'kike', 'who', 'hit', 'the', 'shekel', 'jackpot']\n",
            "Text after removing stop words: ['cohencidence', 'film', 'event', 'taken', 'hooknosed', 'kike', 'hit', 'shekel', 'jackpot']\n",
            "Text after Lemmatization: ['cohencidence', 'film', 'event', 'taken', 'hooknosed', 'kike', 'hit', 'shekel', 'jackpot']\n",
            "Final pre-processed text: cohencidence film event taken hooknosed kike hit shekel jackpot\n",
            "Text after removing HTML tags: look you were useful in that you gave me the opportunity to express my own views and a backdrop against which to set it but i am done and you are boring so goodbye oh and jews did not put blacks in south africa reality did that long before we got there\n",
            "Text after removing non-alphabetic characters and converting to lowercase: look you were useful in that you gave me the opportunity to express my own views and a backdrop against which to set it but i am done and you are boring so goodbye oh and jews did not put blacks in south africa reality did that long before we got there\n",
            "Text after tokenization: ['look', 'you', 'were', 'useful', 'in', 'that', 'you', 'gave', 'me', 'the', 'opportunity', 'to', 'express', 'my', 'own', 'views', 'and', 'a', 'backdrop', 'against', 'which', 'to', 'set', 'it', 'but', 'i', 'am', 'done', 'and', 'you', 'are', 'boring', 'so', 'goodbye', 'oh', 'and', 'jews', 'did', 'not', 'put', 'blacks', 'in', 'south', 'africa', 'reality', 'did', 'that', 'long', 'before', 'we', 'got', 'there']\n",
            "Text after removing stop words: ['look', 'useful', 'gave', 'opportunity', 'express', 'views', 'backdrop', 'set', 'done', 'boring', 'goodbye', 'oh', 'jews', 'put', 'blacks', 'south', 'africa', 'reality', 'long', 'got']\n",
            "Text after Lemmatization: ['look', 'useful', 'gave', 'opportunity', 'express', 'view', 'backdrop', 'set', 'done', 'boring', 'goodbye', 'oh', 'jew', 'put', 'black', 'south', 'africa', 'reality', 'long', 'got']\n",
            "Final pre-processed text: look useful gave opportunity express view backdrop set done boring goodbye oh jew put black south africa reality long got\n",
            "Text after removing HTML tags: so which are you a faggot or jew or both\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so which are you a faggot or jew or both\n",
            "Text after tokenization: ['so', 'which', 'are', 'you', 'a', 'faggot', 'or', 'jew', 'or', 'both']\n",
            "Text after removing stop words: ['faggot', 'jew']\n",
            "Text after Lemmatization: ['faggot', 'jew']\n",
            "Final pre-processed text: faggot jew\n",
            "Text after removing HTML tags: your children are literally being raped by invaders encouraged by your leaders go fuck yourself you delusional faggot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: your children are literally being raped by invaders encouraged by your leaders go fuck yourself you delusional faggot\n",
            "Text after tokenization: ['your', 'children', 'are', 'literally', 'being', 'raped', 'by', 'invaders', 'encouraged', 'by', 'your', 'leaders', 'go', 'fuck', 'yourself', 'you', 'delusional', 'faggot']\n",
            "Text after removing stop words: ['children', 'literally', 'raped', 'invaders', 'encouraged', 'leaders', 'go', 'fuck', 'delusional', 'faggot']\n",
            "Text after Lemmatization: ['child', 'literally', 'raped', 'invader', 'encouraged', 'leader', 'go', 'fuck', 'delusional', 'faggot']\n",
            "Final pre-processed text: child literally raped invader encouraged leader go fuck delusional faggot\n",
            "Text after removing HTML tags: nope because other low iq illiterates to block vote cheating their way into power together with the matrix dwelling white lib the tribal vote has power but there is hope and a change in tbe air so we shall see\n",
            "Text after removing non-alphabetic characters and converting to lowercase: nope because other low iq illiterates to block vote cheating their way into power together with the matrix dwelling white lib the tribal vote has power but there is hope and a change in tbe air so we shall see\n",
            "Text after tokenization: ['nope', 'because', 'other', 'low', 'iq', 'illiterates', 'to', 'block', 'vote', 'cheating', 'their', 'way', 'into', 'power', 'together', 'with', 'the', 'matrix', 'dwelling', 'white', 'lib', 'the', 'tribal', 'vote', 'has', 'power', 'but', 'there', 'is', 'hope', 'and', 'a', 'change', 'in', 'tbe', 'air', 'so', 'we', 'shall', 'see']\n",
            "Text after removing stop words: ['nope', 'low', 'iq', 'illiterates', 'block', 'vote', 'cheating', 'way', 'power', 'together', 'matrix', 'dwelling', 'white', 'lib', 'tribal', 'vote', 'power', 'hope', 'change', 'tbe', 'air', 'shall', 'see']\n",
            "Text after Lemmatization: ['nope', 'low', 'iq', 'illiterate', 'block', 'vote', 'cheating', 'way', 'power', 'together', 'matrix', 'dwelling', 'white', 'lib', 'tribal', 'vote', 'power', 'hope', 'change', 'tbe', 'air', 'shall', 'see']\n",
            "Final pre-processed text: nope low iq illiterate block vote cheating way power together matrix dwelling white lib tribal vote power hope change tbe air shall see\n",
            "Text after removing HTML tags: trump is turning america the beautiful into america the booty ful in which he can just grab dozens of helpless women by the groin and get away with it taking the highest office thereafter while many americans defend this behaviour\n",
            "Text after removing non-alphabetic characters and converting to lowercase: trump is turning america the beautiful into america the booty ful in which he can just grab dozens of helpless women by the groin and get away with it taking the highest office thereafter while many americans defend this behaviour\n",
            "Text after tokenization: ['trump', 'is', 'turning', 'america', 'the', 'beautiful', 'into', 'america', 'the', 'booty', 'ful', 'in', 'which', 'he', 'can', 'just', 'grab', 'dozens', 'of', 'helpless', 'women', 'by', 'the', 'groin', 'and', 'get', 'away', 'with', 'it', 'taking', 'the', 'highest', 'office', 'thereafter', 'while', 'many', 'americans', 'defend', 'this', 'behaviour']\n",
            "Text after removing stop words: ['trump', 'turning', 'america', 'beautiful', 'america', 'booty', 'ful', 'grab', 'dozens', 'helpless', 'women', 'groin', 'get', 'away', 'taking', 'highest', 'office', 'thereafter', 'many', 'americans', 'defend', 'behaviour']\n",
            "Text after Lemmatization: ['trump', 'turning', 'america', 'beautiful', 'america', 'booty', 'ful', 'grab', 'dozen', 'helpless', 'woman', 'groin', 'get', 'away', 'taking', 'highest', 'office', 'thereafter', 'many', 'american', 'defend', 'behaviour']\n",
            "Final pre-processed text: trump turning america beautiful america booty ful grab dozen helpless woman groin get away taking highest office thereafter many american defend behaviour\n",
            "Text after removing HTML tags: warski on the list nah i do not think he an enemy i just think he needs to be red pilled a lot more maybe if he came and hung out in the ghetto for about an hour he would wake up\n",
            "Text after removing non-alphabetic characters and converting to lowercase: warski on the list nah i do not think he an enemy i just think he needs to be red pilled a lot more maybe if he came and hung out in the ghetto for about an hour he would wake up\n",
            "Text after tokenization: ['warski', 'on', 'the', 'list', 'nah', 'i', 'do', 'not', 'think', 'he', 'an', 'enemy', 'i', 'just', 'think', 'he', 'needs', 'to', 'be', 'red', 'pilled', 'a', 'lot', 'more', 'maybe', 'if', 'he', 'came', 'and', 'hung', 'out', 'in', 'the', 'ghetto', 'for', 'about', 'an', 'hour', 'he', 'would', 'wake', 'up']\n",
            "Text after removing stop words: ['warski', 'list', 'nah', 'think', 'enemy', 'think', 'needs', 'red', 'pilled', 'lot', 'maybe', 'came', 'hung', 'ghetto', 'hour', 'would', 'wake']\n",
            "Text after Lemmatization: ['warski', 'list', 'nah', 'think', 'enemy', 'think', 'need', 'red', 'pilled', 'lot', 'maybe', 'came', 'hung', 'ghetto', 'hour', 'would', 'wake']\n",
            "Final pre-processed text: warski list nah think enemy think need red pilled lot maybe came hung ghetto hour would wake\n",
            "Text after removing HTML tags: show class you stupid tornado bait trailer trash descendant of ghetto lice bait saved from his people crimes by said trailer trash\n",
            "Text after removing non-alphabetic characters and converting to lowercase: show class you stupid tornado bait trailer trash descendant of ghetto lice bait saved from his people crimes by said trailer trash\n",
            "Text after tokenization: ['show', 'class', 'you', 'stupid', 'tornado', 'bait', 'trailer', 'trash', 'descendant', 'of', 'ghetto', 'lice', 'bait', 'saved', 'from', 'his', 'people', 'crimes', 'by', 'said', 'trailer', 'trash']\n",
            "Text after removing stop words: ['show', 'class', 'stupid', 'tornado', 'bait', 'trailer', 'trash', 'descendant', 'ghetto', 'lice', 'bait', 'saved', 'people', 'crimes', 'said', 'trailer', 'trash']\n",
            "Text after Lemmatization: ['show', 'class', 'stupid', 'tornado', 'bait', 'trailer', 'trash', 'descendant', 'ghetto', 'louse', 'bait', 'saved', 'people', 'crime', 'said', 'trailer', 'trash']\n",
            "Final pre-processed text: show class stupid tornado bait trailer trash descendant ghetto louse bait saved people crime said trailer trash\n",
            "Text after removing HTML tags: the ghetto lottery an old standby we v seen it again and again\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the ghetto lottery an old standby we v seen it again and again\n",
            "Text after tokenization: ['the', 'ghetto', 'lottery', 'an', 'old', 'standby', 'we', 'v', 'seen', 'it', 'again', 'and', 'again']\n",
            "Text after removing stop words: ['ghetto', 'lottery', 'old', 'standby', 'v', 'seen']\n",
            "Text after Lemmatization: ['ghetto', 'lottery', 'old', 'standby', 'v', 'seen']\n",
            "Final pre-processed text: ghetto lottery old standby v seen\n",
            "Text after removing HTML tags: look at the price of shit in tis bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase: look at the price of shit in tis bitch\n",
            "Text after tokenization: ['look', 'at', 'the', 'price', 'of', 'shit', 'in', 'tis', 'bitch']\n",
            "Text after removing stop words: ['look', 'price', 'shit', 'tis', 'bitch']\n",
            "Text after Lemmatization: ['look', 'price', 'shit', 'ti', 'bitch']\n",
            "Final pre-processed text: look price shit ti bitch\n",
            "Text after removing HTML tags: call others ignorant and stupid say 4 th grade level things about a topic you have not researched what was it like learning about your white guilt for 1 0 0 k a year did it make you feel like a good person\n",
            "Text after removing non-alphabetic characters and converting to lowercase: call others ignorant and stupid say   th grade level things about a topic you have not researched what was it like learning about your white guilt for       k a year did it make you feel like a good person\n",
            "Text after tokenization: ['call', 'others', 'ignorant', 'and', 'stupid', 'say', 'th', 'grade', 'level', 'things', 'about', 'a', 'topic', 'you', 'have', 'not', 'researched', 'what', 'was', 'it', 'like', 'learning', 'about', 'your', 'white', 'guilt', 'for', 'k', 'a', 'year', 'did', 'it', 'make', 'you', 'feel', 'like', 'a', 'good', 'person']\n",
            "Text after removing stop words: ['call', 'others', 'ignorant', 'stupid', 'say', 'th', 'grade', 'level', 'things', 'topic', 'researched', 'like', 'learning', 'white', 'guilt', 'k', 'year', 'make', 'feel', 'like', 'good', 'person']\n",
            "Text after Lemmatization: ['call', 'others', 'ignorant', 'stupid', 'say', 'th', 'grade', 'level', 'thing', 'topic', 'researched', 'like', 'learning', 'white', 'guilt', 'k', 'year', 'make', 'feel', 'like', 'good', 'person']\n",
            "Final pre-processed text: call others ignorant stupid say th grade level thing topic researched like learning white guilt k year make feel like good person\n",
            "Text after removing HTML tags: yep i am all for more black conservatives being spawned but that not our job low hanging fruit is a bazillion retarded whites\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yep i am all for more black conservatives being spawned but that not our job low hanging fruit is a bazillion retarded whites\n",
            "Text after tokenization: ['yep', 'i', 'am', 'all', 'for', 'more', 'black', 'conservatives', 'being', 'spawned', 'but', 'that', 'not', 'our', 'job', 'low', 'hanging', 'fruit', 'is', 'a', 'bazillion', 'retarded', 'whites']\n",
            "Text after removing stop words: ['yep', 'black', 'conservatives', 'spawned', 'job', 'low', 'hanging', 'fruit', 'bazillion', 'retarded', 'whites']\n",
            "Text after Lemmatization: ['yep', 'black', 'conservative', 'spawned', 'job', 'low', 'hanging', 'fruit', 'bazillion', 'retarded', 'white']\n",
            "Final pre-processed text: yep black conservative spawned job low hanging fruit bazillion retarded white\n",
            "Text after removing HTML tags: it is not the violence that is feared it is the amount one will go to achieve his rights\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it is not the violence that is feared it is the amount one will go to achieve his rights\n",
            "Text after tokenization: ['it', 'is', 'not', 'the', 'violence', 'that', 'is', 'feared', 'it', 'is', 'the', 'amount', 'one', 'will', 'go', 'to', 'achieve', 'his', 'rights']\n",
            "Text after removing stop words: ['violence', 'feared', 'amount', 'one', 'go', 'achieve', 'rights']\n",
            "Text after Lemmatization: ['violence', 'feared', 'amount', 'one', 'go', 'achieve', 'right']\n",
            "Final pre-processed text: violence feared amount one go achieve right\n",
            "Text after removing HTML tags: ♪ i am dreaming of a so cal christmas just like the ones i used to know where the sunshine glistens ♪ and no one listens ♪ cause there no sleigh bells and no snow ♪ buncha icky white stuff all over the ground this morning\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   i am dreaming of a so cal christmas just like the ones i used to know where the sunshine glistens   and no one listens   cause there no sleigh bells and no snow   buncha icky white stuff all over the ground this morning\n",
            "Text after tokenization: ['i', 'am', 'dreaming', 'of', 'a', 'so', 'cal', 'christmas', 'just', 'like', 'the', 'ones', 'i', 'used', 'to', 'know', 'where', 'the', 'sunshine', 'glistens', 'and', 'no', 'one', 'listens', 'cause', 'there', 'no', 'sleigh', 'bells', 'and', 'no', 'snow', 'buncha', 'icky', 'white', 'stuff', 'all', 'over', 'the', 'ground', 'this', 'morning']\n",
            "Text after removing stop words: ['dreaming', 'cal', 'christmas', 'like', 'ones', 'used', 'know', 'sunshine', 'glistens', 'one', 'listens', 'cause', 'sleigh', 'bells', 'snow', 'buncha', 'icky', 'white', 'stuff', 'ground', 'morning']\n",
            "Text after Lemmatization: ['dreaming', 'cal', 'christmas', 'like', 'one', 'used', 'know', 'sunshine', 'glisten', 'one', 'listens', 'cause', 'sleigh', 'bell', 'snow', 'buncha', 'icky', 'white', 'stuff', 'ground', 'morning']\n",
            "Final pre-processed text: dreaming cal christmas like one used know sunshine glisten one listens cause sleigh bell snow buncha icky white stuff ground morning\n",
            "Text after removing HTML tags: and the pussy faggot mutes thanks for the scalp bitch\n",
            "Text after removing non-alphabetic characters and converting to lowercase: and the pussy faggot mutes thanks for the scalp bitch\n",
            "Text after tokenization: ['and', 'the', 'pussy', 'faggot', 'mutes', 'thanks', 'for', 'the', 'scalp', 'bitch']\n",
            "Text after removing stop words: ['pussy', 'faggot', 'mutes', 'thanks', 'scalp', 'bitch']\n",
            "Text after Lemmatization: ['pussy', 'faggot', 'mute', 'thanks', 'scalp', 'bitch']\n",
            "Final pre-processed text: pussy faggot mute thanks scalp bitch\n",
            "Text after removing HTML tags: there are just women and men male and female there is no gender spectrum the ones that claim they are such and such are just in need of help i e psychiatric the only ones i accept other than male and female are gay and even then some dabble in this\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there are just women and men male and female there is no gender spectrum the ones that claim they are such and such are just in need of help i e psychiatric the only ones i accept other than male and female are gay and even then some dabble in this\n",
            "Text after tokenization: ['there', 'are', 'just', 'women', 'and', 'men', 'male', 'and', 'female', 'there', 'is', 'no', 'gender', 'spectrum', 'the', 'ones', 'that', 'claim', 'they', 'are', 'such', 'and', 'such', 'are', 'just', 'in', 'need', 'of', 'help', 'i', 'e', 'psychiatric', 'the', 'only', 'ones', 'i', 'accept', 'other', 'than', 'male', 'and', 'female', 'are', 'gay', 'and', 'even', 'then', 'some', 'dabble', 'in', 'this']\n",
            "Text after removing stop words: ['women', 'men', 'male', 'female', 'gender', 'spectrum', 'ones', 'claim', 'need', 'help', 'e', 'psychiatric', 'ones', 'accept', 'male', 'female', 'gay', 'even', 'dabble']\n",
            "Text after Lemmatization: ['woman', 'men', 'male', 'female', 'gender', 'spectrum', 'one', 'claim', 'need', 'help', 'e', 'psychiatric', 'one', 'accept', 'male', 'female', 'gay', 'even', 'dabble']\n",
            "Final pre-processed text: woman men male female gender spectrum one claim need help e psychiatric one accept male female gay even dabble\n",
            "Text after removing HTML tags: whitecountries no longer exist a white country does not allow itself to be invaded whitepeopleawake must consolidate in order to survive whitegenocide rebuild and advance without the threat of der jude and it antiwhite allies you are in the middle of a racewar run by culturalmarxists that will not stopwhitegenocide\n",
            "Text after removing non-alphabetic characters and converting to lowercase: whitecountries no longer exist a white country does not allow itself to be invaded whitepeopleawake must consolidate in order to survive whitegenocide rebuild and advance without the threat of der jude and it antiwhite allies you are in the middle of a racewar run by culturalmarxists that will not stopwhitegenocide\n",
            "Text after tokenization: ['whitecountries', 'no', 'longer', 'exist', 'a', 'white', 'country', 'does', 'not', 'allow', 'itself', 'to', 'be', 'invaded', 'whitepeopleawake', 'must', 'consolidate', 'in', 'order', 'to', 'survive', 'whitegenocide', 'rebuild', 'and', 'advance', 'without', 'the', 'threat', 'of', 'der', 'jude', 'and', 'it', 'antiwhite', 'allies', 'you', 'are', 'in', 'the', 'middle', 'of', 'a', 'racewar', 'run', 'by', 'culturalmarxists', 'that', 'will', 'not', 'stopwhitegenocide']\n",
            "Text after removing stop words: ['whitecountries', 'longer', 'exist', 'white', 'country', 'allow', 'invaded', 'whitepeopleawake', 'must', 'consolidate', 'order', 'survive', 'whitegenocide', 'rebuild', 'advance', 'without', 'threat', 'der', 'jude', 'antiwhite', 'allies', 'middle', 'racewar', 'run', 'culturalmarxists', 'stopwhitegenocide']\n",
            "Text after Lemmatization: ['whitecountries', 'longer', 'exist', 'white', 'country', 'allow', 'invaded', 'whitepeopleawake', 'must', 'consolidate', 'order', 'survive', 'whitegenocide', 'rebuild', 'advance', 'without', 'threat', 'der', 'jude', 'antiwhite', 'ally', 'middle', 'racewar', 'run', 'culturalmarxists', 'stopwhitegenocide']\n",
            "Final pre-processed text: whitecountries longer exist white country allow invaded whitepeopleawake must consolidate order survive whitegenocide rebuild advance without threat der jude antiwhite ally middle racewar run culturalmarxists stopwhitegenocide\n",
            "Text after removing HTML tags: will the cowardly little nazi faggot that downvoted u.s. marines kindly identify yourself so we can all mock you thank you\n",
            "Text after removing non-alphabetic characters and converting to lowercase: will the cowardly little nazi faggot that downvoted u s  marines kindly identify yourself so we can all mock you thank you\n",
            "Text after tokenization: ['will', 'the', 'cowardly', 'little', 'nazi', 'faggot', 'that', 'downvoted', 'u', 's', 'marines', 'kindly', 'identify', 'yourself', 'so', 'we', 'can', 'all', 'mock', 'you', 'thank', 'you']\n",
            "Text after removing stop words: ['cowardly', 'little', 'nazi', 'faggot', 'downvoted', 'u', 'marines', 'kindly', 'identify', 'mock', 'thank']\n",
            "Text after Lemmatization: ['cowardly', 'little', 'nazi', 'faggot', 'downvoted', 'u', 'marine', 'kindly', 'identify', 'mock', 'thank']\n",
            "Final pre-processed text: cowardly little nazi faggot downvoted u marine kindly identify mock thank\n",
            "Text after removing HTML tags: i am posting everything via gab so it officially not on twitter  ‍ therefore you have no reason to shut down and censor my account again for exposing truth and historical facts about the violent ideology of islam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am posting everything via gab so it officially not on twitter    therefore you have no reason to shut down and censor my account again for exposing truth and historical facts about the violent ideology of islam\n",
            "Text after tokenization: ['i', 'am', 'posting', 'everything', 'via', 'gab', 'so', 'it', 'officially', 'not', 'on', 'twitter', 'therefore', 'you', 'have', 'no', 'reason', 'to', 'shut', 'down', 'and', 'censor', 'my', 'account', 'again', 'for', 'exposing', 'truth', 'and', 'historical', 'facts', 'about', 'the', 'violent', 'ideology', 'of', 'islam']\n",
            "Text after removing stop words: ['posting', 'everything', 'via', 'gab', 'officially', 'twitter', 'therefore', 'reason', 'shut', 'censor', 'account', 'exposing', 'truth', 'historical', 'facts', 'violent', 'ideology', 'islam']\n",
            "Text after Lemmatization: ['posting', 'everything', 'via', 'gab', 'officially', 'twitter', 'therefore', 'reason', 'shut', 'censor', 'account', 'exposing', 'truth', 'historical', 'fact', 'violent', 'ideology', 'islam']\n",
            "Final pre-processed text: posting everything via gab officially twitter therefore reason shut censor account exposing truth historical fact violent ideology islam\n",
            "Text after removing HTML tags: what the hell look at the nazi faggot go at it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what the hell look at the nazi faggot go at it\n",
            "Text after tokenization: ['what', 'the', 'hell', 'look', 'at', 'the', 'nazi', 'faggot', 'go', 'at', 'it']\n",
            "Text after removing stop words: ['hell', 'look', 'nazi', 'faggot', 'go']\n",
            "Text after Lemmatization: ['hell', 'look', 'nazi', 'faggot', 'go']\n",
            "Final pre-processed text: hell look nazi faggot go\n",
            "Text after removing HTML tags: here comes silverstein to white knight for faggotry while taking potshots at  ‍ you are a pathetic faggot consider an hero\n",
            "Text after removing non-alphabetic characters and converting to lowercase: here comes silverstein to white knight for faggotry while taking potshots at    you are a pathetic faggot consider an hero\n",
            "Text after tokenization: ['here', 'comes', 'silverstein', 'to', 'white', 'knight', 'for', 'faggotry', 'while', 'taking', 'potshots', 'at', 'you', 'are', 'a', 'pathetic', 'faggot', 'consider', 'an', 'hero']\n",
            "Text after removing stop words: ['comes', 'silverstein', 'white', 'knight', 'faggotry', 'taking', 'potshots', 'pathetic', 'faggot', 'consider', 'hero']\n",
            "Text after Lemmatization: ['come', 'silverstein', 'white', 'knight', 'faggotry', 'taking', 'potshot', 'pathetic', 'faggot', 'consider', 'hero']\n",
            "Final pre-processed text: come silverstein white knight faggotry taking potshot pathetic faggot consider hero\n",
            "Text after removing HTML tags: keep shipping in the shitskins i say replace the brownies with white blonde africans who will farm for us make cali white and right again we tried this 3 rd world crap it ain t workin\n",
            "Text after removing non-alphabetic characters and converting to lowercase: keep shipping in the shitskins i say replace the brownies with white blonde africans who will farm for us make cali white and right again we tried this   rd world crap it ain t workin\n",
            "Text after tokenization: ['keep', 'shipping', 'in', 'the', 'shitskins', 'i', 'say', 'replace', 'the', 'brownies', 'with', 'white', 'blonde', 'africans', 'who', 'will', 'farm', 'for', 'us', 'make', 'cali', 'white', 'and', 'right', 'again', 'we', 'tried', 'this', 'rd', 'world', 'crap', 'it', 'ain', 't', 'workin']\n",
            "Text after removing stop words: ['keep', 'shipping', 'shitskins', 'say', 'replace', 'brownies', 'white', 'blonde', 'africans', 'farm', 'us', 'make', 'cali', 'white', 'right', 'tried', 'rd', 'world', 'crap', 'workin']\n",
            "Text after Lemmatization: ['keep', 'shipping', 'shitskins', 'say', 'replace', 'brownie', 'white', 'blonde', 'african', 'farm', 'u', 'make', 'cali', 'white', 'right', 'tried', 'rd', 'world', 'crap', 'workin']\n",
            "Final pre-processed text: keep shipping shitskins say replace brownie white blonde african farm u make cali white right tried rd world crap workin\n",
            "Text after removing HTML tags: islamists plainly state their goals why are so many in the west unwilling to acknowledge the goals of islam and the consequences of accepting this ideology into western societies it just cohencidenal i am sure\n",
            "Text after removing non-alphabetic characters and converting to lowercase: islamists plainly state their goals why are so many in the west unwilling to acknowledge the goals of islam and the consequences of accepting this ideology into western societies it just cohencidenal i am sure\n",
            "Text after tokenization: ['islamists', 'plainly', 'state', 'their', 'goals', 'why', 'are', 'so', 'many', 'in', 'the', 'west', 'unwilling', 'to', 'acknowledge', 'the', 'goals', 'of', 'islam', 'and', 'the', 'consequences', 'of', 'accepting', 'this', 'ideology', 'into', 'western', 'societies', 'it', 'just', 'cohencidenal', 'i', 'am', 'sure']\n",
            "Text after removing stop words: ['islamists', 'plainly', 'state', 'goals', 'many', 'west', 'unwilling', 'acknowledge', 'goals', 'islam', 'consequences', 'accepting', 'ideology', 'western', 'societies', 'cohencidenal', 'sure']\n",
            "Text after Lemmatization: ['islamist', 'plainly', 'state', 'goal', 'many', 'west', 'unwilling', 'acknowledge', 'goal', 'islam', 'consequence', 'accepting', 'ideology', 'western', 'society', 'cohencidenal', 'sure']\n",
            "Final pre-processed text: islamist plainly state goal many west unwilling acknowledge goal islam consequence accepting ideology western society cohencidenal sure\n",
            "Text after removing HTML tags: the posh slut at the bbc bolshevik broadcasting corporation was literally flicking her bean send the fake ugees to her town 👍 illiberals love getting raped\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the posh slut at the bbc bolshevik broadcasting corporation was literally flicking her bean send the fake ugees to her town   illiberals love getting raped\n",
            "Text after tokenization: ['the', 'posh', 'slut', 'at', 'the', 'bbc', 'bolshevik', 'broadcasting', 'corporation', 'was', 'literally', 'flicking', 'her', 'bean', 'send', 'the', 'fake', 'ugees', 'to', 'her', 'town', 'illiberals', 'love', 'getting', 'raped']\n",
            "Text after removing stop words: ['posh', 'slut', 'bbc', 'bolshevik', 'broadcasting', 'corporation', 'literally', 'flicking', 'bean', 'send', 'fake', 'ugees', 'town', 'illiberals', 'love', 'getting', 'raped']\n",
            "Text after Lemmatization: ['posh', 'slut', 'bbc', 'bolshevik', 'broadcasting', 'corporation', 'literally', 'flicking', 'bean', 'send', 'fake', 'ugees', 'town', 'illiberals', 'love', 'getting', 'raped']\n",
            "Final pre-processed text: posh slut bbc bolshevik broadcasting corporation literally flicking bean send fake ugees town illiberals love getting raped\n",
            "Text after removing HTML tags: the traitors that stand with israel and the jews are forcing the anglo saxons to kneel before allah\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the traitors that stand with israel and the jews are forcing the anglo saxons to kneel before allah\n",
            "Text after tokenization: ['the', 'traitors', 'that', 'stand', 'with', 'israel', 'and', 'the', 'jews', 'are', 'forcing', 'the', 'anglo', 'saxons', 'to', 'kneel', 'before', 'allah']\n",
            "Text after removing stop words: ['traitors', 'stand', 'israel', 'jews', 'forcing', 'anglo', 'saxons', 'kneel', 'allah']\n",
            "Text after Lemmatization: ['traitor', 'stand', 'israel', 'jew', 'forcing', 'anglo', 'saxon', 'kneel', 'allah']\n",
            "Final pre-processed text: traitor stand israel jew forcing anglo saxon kneel allah\n",
            "Text after removing HTML tags: it just going to make you alt kike faggots get even more milquetoast and moderate in the vain hope that if you fine tune your optics just right then the jews will leave you alone\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it just going to make you alt kike faggots get even more milquetoast and moderate in the vain hope that if you fine tune your optics just right then the jews will leave you alone\n",
            "Text after tokenization: ['it', 'just', 'going', 'to', 'make', 'you', 'alt', 'kike', 'faggots', 'get', 'even', 'more', 'milquetoast', 'and', 'moderate', 'in', 'the', 'vain', 'hope', 'that', 'if', 'you', 'fine', 'tune', 'your', 'optics', 'just', 'right', 'then', 'the', 'jews', 'will', 'leave', 'you', 'alone']\n",
            "Text after removing stop words: ['going', 'make', 'alt', 'kike', 'faggots', 'get', 'even', 'milquetoast', 'moderate', 'vain', 'hope', 'fine', 'tune', 'optics', 'right', 'jews', 'leave', 'alone']\n",
            "Text after Lemmatization: ['going', 'make', 'alt', 'kike', 'faggot', 'get', 'even', 'milquetoast', 'moderate', 'vain', 'hope', 'fine', 'tune', 'optic', 'right', 'jew', 'leave', 'alone']\n",
            "Final pre-processed text: going make alt kike faggot get even milquetoast moderate vain hope fine tune optic right jew leave alone\n",
            "Text after removing HTML tags: i actually just posted about her earlier women who are resourceful can whip a mans ass or  romans\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i actually just posted about her earlier women who are resourceful can whip a mans ass or  romans\n",
            "Text after tokenization: ['i', 'actually', 'just', 'posted', 'about', 'her', 'earlier', 'women', 'who', 'are', 'resourceful', 'can', 'whip', 'a', 'mans', 'ass', 'or', 'romans']\n",
            "Text after removing stop words: ['actually', 'posted', 'earlier', 'women', 'resourceful', 'whip', 'mans', 'ass', 'romans']\n",
            "Text after Lemmatization: ['actually', 'posted', 'earlier', 'woman', 'resourceful', 'whip', 'man', 'as', 'roman']\n",
            "Final pre-processed text: actually posted earlier woman resourceful whip man as roman\n",
            "Text after removing HTML tags: hey  will you need jews to figure out how to implement your retarded mars colonization plan\n",
            "Text after removing non-alphabetic characters and converting to lowercase: hey  will you need jews to figure out how to implement your retarded mars colonization plan\n",
            "Text after tokenization: ['hey', 'will', 'you', 'need', 'jews', 'to', 'figure', 'out', 'how', 'to', 'implement', 'your', 'retarded', 'mars', 'colonization', 'plan']\n",
            "Text after removing stop words: ['hey', 'need', 'jews', 'figure', 'implement', 'retarded', 'mars', 'colonization', 'plan']\n",
            "Text after Lemmatization: ['hey', 'need', 'jew', 'figure', 'implement', 'retarded', 'mar', 'colonization', 'plan']\n",
            "Final pre-processed text: hey need jew figure implement retarded mar colonization plan\n",
            "Text after removing HTML tags: tfw you are actually retarded enough to believe the q larp or that anyone of any significance in government will actually be prosecuted by president zion sleepy jeff or their gop friends across the aisle\n",
            "Text after removing non-alphabetic characters and converting to lowercase: tfw you are actually retarded enough to believe the q larp or that anyone of any significance in government will actually be prosecuted by president zion sleepy jeff or their gop friends across the aisle\n",
            "Text after tokenization: ['tfw', 'you', 'are', 'actually', 'retarded', 'enough', 'to', 'believe', 'the', 'q', 'larp', 'or', 'that', 'anyone', 'of', 'any', 'significance', 'in', 'government', 'will', 'actually', 'be', 'prosecuted', 'by', 'president', 'zion', 'sleepy', 'jeff', 'or', 'their', 'gop', 'friends', 'across', 'the', 'aisle']\n",
            "Text after removing stop words: ['tfw', 'actually', 'retarded', 'enough', 'believe', 'q', 'larp', 'anyone', 'significance', 'government', 'actually', 'prosecuted', 'president', 'zion', 'sleepy', 'jeff', 'gop', 'friends', 'across', 'aisle']\n",
            "Text after Lemmatization: ['tfw', 'actually', 'retarded', 'enough', 'believe', 'q', 'larp', 'anyone', 'significance', 'government', 'actually', 'prosecuted', 'president', 'zion', 'sleepy', 'jeff', 'gop', 'friend', 'across', 'aisle']\n",
            "Final pre-processed text: tfw actually retarded enough believe q larp anyone significance government actually prosecuted president zion sleepy jeff gop friend across aisle\n",
            "Text after removing HTML tags: jewish journolister here are the edgelords of the intellectual dark web lists a bunch of jews who retweet bill kristol tweets oy vey they are so dangerous right goy\n",
            "Text after removing non-alphabetic characters and converting to lowercase: jewish journolister here are the edgelords of the intellectual dark web lists a bunch of jews who retweet bill kristol tweets oy vey they are so dangerous right goy\n",
            "Text after tokenization: ['jewish', 'journolister', 'here', 'are', 'the', 'edgelords', 'of', 'the', 'intellectual', 'dark', 'web', 'lists', 'a', 'bunch', 'of', 'jews', 'who', 'retweet', 'bill', 'kristol', 'tweets', 'oy', 'vey', 'they', 'are', 'so', 'dangerous', 'right', 'goy']\n",
            "Text after removing stop words: ['jewish', 'journolister', 'edgelords', 'intellectual', 'dark', 'web', 'lists', 'bunch', 'jews', 'retweet', 'bill', 'kristol', 'tweets', 'oy', 'vey', 'dangerous', 'right', 'goy']\n",
            "Text after Lemmatization: ['jewish', 'journolister', 'edgelords', 'intellectual', 'dark', 'web', 'list', 'bunch', 'jew', 'retweet', 'bill', 'kristol', 'tweet', 'oy', 'vey', 'dangerous', 'right', 'goy']\n",
            "Final pre-processed text: jewish journolister edgelords intellectual dark web list bunch jew retweet bill kristol tweet oy vey dangerous right goy\n",
            "Text after removing HTML tags: obama is a corrupt lying muslim pos who goal is to destroy traditional us culture values for islam trump killing his anti us deal w iran upset the little punk  tcot pjnet maga lnyhbt wakeupamerica\n",
            "Text after removing non-alphabetic characters and converting to lowercase: obama is a corrupt lying muslim pos who goal is to destroy traditional us culture values for islam trump killing his anti us deal w iran upset the little punk  tcot pjnet maga lnyhbt wakeupamerica\n",
            "Text after tokenization: ['obama', 'is', 'a', 'corrupt', 'lying', 'muslim', 'pos', 'who', 'goal', 'is', 'to', 'destroy', 'traditional', 'us', 'culture', 'values', 'for', 'islam', 'trump', 'killing', 'his', 'anti', 'us', 'deal', 'w', 'iran', 'upset', 'the', 'little', 'punk', 'tcot', 'pjnet', 'maga', 'lnyhbt', 'wakeupamerica']\n",
            "Text after removing stop words: ['obama', 'corrupt', 'lying', 'muslim', 'pos', 'goal', 'destroy', 'traditional', 'us', 'culture', 'values', 'islam', 'trump', 'killing', 'anti', 'us', 'deal', 'w', 'iran', 'upset', 'little', 'punk', 'tcot', 'pjnet', 'maga', 'lnyhbt', 'wakeupamerica']\n",
            "Text after Lemmatization: ['obama', 'corrupt', 'lying', 'muslim', 'po', 'goal', 'destroy', 'traditional', 'u', 'culture', 'value', 'islam', 'trump', 'killing', 'anti', 'u', 'deal', 'w', 'iran', 'upset', 'little', 'punk', 'tcot', 'pjnet', 'maga', 'lnyhbt', 'wakeupamerica']\n",
            "Final pre-processed text: obama corrupt lying muslim po goal destroy traditional u culture value islam trump killing anti u deal w iran upset little punk tcot pjnet maga lnyhbt wakeupamerica\n",
            "Text after removing HTML tags: there not asians they are pakis and arabs\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there not asians they are pakis and arabs\n",
            "Text after tokenization: ['there', 'not', 'asians', 'they', 'are', 'pakis', 'and', 'arabs']\n",
            "Text after removing stop words: ['asians', 'pakis', 'arabs']\n",
            "Text after Lemmatization: ['asian', 'pakis', 'arab']\n",
            "Final pre-processed text: asian pakis arab\n",
            "Text after removing HTML tags: all it does is boomerpost while pretending to be a slav and screeches anytime anyone says anything about jews like anyone other than a boomer would be retarded enough to believe slavs do not know their own 2 0 th century history at the hands of the chosenites\n",
            "Text after removing non-alphabetic characters and converting to lowercase: all it does is boomerpost while pretending to be a slav and screeches anytime anyone says anything about jews like anyone other than a boomer would be retarded enough to believe slavs do not know their own     th century history at the hands of the chosenites\n",
            "Text after tokenization: ['all', 'it', 'does', 'is', 'boomerpost', 'while', 'pretending', 'to', 'be', 'a', 'slav', 'and', 'screeches', 'anytime', 'anyone', 'says', 'anything', 'about', 'jews', 'like', 'anyone', 'other', 'than', 'a', 'boomer', 'would', 'be', 'retarded', 'enough', 'to', 'believe', 'slavs', 'do', 'not', 'know', 'their', 'own', 'th', 'century', 'history', 'at', 'the', 'hands', 'of', 'the', 'chosenites']\n",
            "Text after removing stop words: ['boomerpost', 'pretending', 'slav', 'screeches', 'anytime', 'anyone', 'says', 'anything', 'jews', 'like', 'anyone', 'boomer', 'would', 'retarded', 'enough', 'believe', 'slavs', 'know', 'th', 'century', 'history', 'hands', 'chosenites']\n",
            "Text after Lemmatization: ['boomerpost', 'pretending', 'slav', 'screech', 'anytime', 'anyone', 'say', 'anything', 'jew', 'like', 'anyone', 'boomer', 'would', 'retarded', 'enough', 'believe', 'slav', 'know', 'th', 'century', 'history', 'hand', 'chosenites']\n",
            "Final pre-processed text: boomerpost pretending slav screech anytime anyone say anything jew like anyone boomer would retarded enough believe slav know th century history hand chosenites\n",
            "Text after removing HTML tags: nr is not conservative they are just a bunch of retarded cucks no compromise with sexual deviants do not like it move to saudi arabia and find a compromise there\n",
            "Text after removing non-alphabetic characters and converting to lowercase: nr is not conservative they are just a bunch of retarded cucks no compromise with sexual deviants do not like it move to saudi arabia and find a compromise there\n",
            "Text after tokenization: ['nr', 'is', 'not', 'conservative', 'they', 'are', 'just', 'a', 'bunch', 'of', 'retarded', 'cucks', 'no', 'compromise', 'with', 'sexual', 'deviants', 'do', 'not', 'like', 'it', 'move', 'to', 'saudi', 'arabia', 'and', 'find', 'a', 'compromise', 'there']\n",
            "Text after removing stop words: ['nr', 'conservative', 'bunch', 'retarded', 'cucks', 'compromise', 'sexual', 'deviants', 'like', 'move', 'saudi', 'arabia', 'find', 'compromise']\n",
            "Text after Lemmatization: ['nr', 'conservative', 'bunch', 'retarded', 'cucks', 'compromise', 'sexual', 'deviant', 'like', 'move', 'saudi', 'arabia', 'find', 'compromise']\n",
            "Final pre-processed text: nr conservative bunch retarded cucks compromise sexual deviant like move saudi arabia find compromise\n",
            "Text after removing HTML tags: you are retarded sir no sand niggers muslim no fighting period dude 😉\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you are retarded sir no sand niggers muslim no fighting period dude  \n",
            "Text after tokenization: ['you', 'are', 'retarded', 'sir', 'no', 'sand', 'niggers', 'muslim', 'no', 'fighting', 'period', 'dude']\n",
            "Text after removing stop words: ['retarded', 'sir', 'sand', 'niggers', 'muslim', 'fighting', 'period', 'dude']\n",
            "Text after Lemmatization: ['retarded', 'sir', 'sand', 'nigger', 'muslim', 'fighting', 'period', 'dude']\n",
            "Final pre-processed text: retarded sir sand nigger muslim fighting period dude\n",
            "Text after removing HTML tags: boo hope poor little sick nazi faggot is offended 😁 😁 😁 🖕 🏽\n",
            "Text after removing non-alphabetic characters and converting to lowercase: boo hope poor little sick nazi faggot is offended          \n",
            "Text after tokenization: ['boo', 'hope', 'poor', 'little', 'sick', 'nazi', 'faggot', 'is', 'offended']\n",
            "Text after removing stop words: ['boo', 'hope', 'poor', 'little', 'sick', 'nazi', 'faggot', 'offended']\n",
            "Text after Lemmatization: ['boo', 'hope', 'poor', 'little', 'sick', 'nazi', 'faggot', 'offended']\n",
            "Final pre-processed text: boo hope poor little sick nazi faggot offended\n",
            "Text after removing HTML tags: and you are a typical kike using typical kike logic that shit do not fly no more hoss the goyim know\n",
            "Text after removing non-alphabetic characters and converting to lowercase: and you are a typical kike using typical kike logic that shit do not fly no more hoss the goyim know\n",
            "Text after tokenization: ['and', 'you', 'are', 'a', 'typical', 'kike', 'using', 'typical', 'kike', 'logic', 'that', 'shit', 'do', 'not', 'fly', 'no', 'more', 'hoss', 'the', 'goyim', 'know']\n",
            "Text after removing stop words: ['typical', 'kike', 'using', 'typical', 'kike', 'logic', 'shit', 'fly', 'hoss', 'goyim', 'know']\n",
            "Text after Lemmatization: ['typical', 'kike', 'using', 'typical', 'kike', 'logic', 'shit', 'fly', 'ho', 'goy', 'know']\n",
            "Final pre-processed text: typical kike using typical kike logic shit fly ho goy know\n",
            "Text after removing HTML tags: if this soy trend continues women are going to need ai penises capable of producing sperm with aryan dna as much as mgtow men are wishing for ai wombs\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if this soy trend continues women are going to need ai penises capable of producing sperm with aryan dna as much as mgtow men are wishing for ai wombs\n",
            "Text after tokenization: ['if', 'this', 'soy', 'trend', 'continues', 'women', 'are', 'going', 'to', 'need', 'ai', 'penises', 'capable', 'of', 'producing', 'sperm', 'with', 'aryan', 'dna', 'as', 'much', 'as', 'mgtow', 'men', 'are', 'wishing', 'for', 'ai', 'wombs']\n",
            "Text after removing stop words: ['soy', 'trend', 'continues', 'women', 'going', 'need', 'ai', 'penises', 'capable', 'producing', 'sperm', 'aryan', 'dna', 'much', 'mgtow', 'men', 'wishing', 'ai', 'wombs']\n",
            "Text after Lemmatization: ['soy', 'trend', 'continues', 'woman', 'going', 'need', 'ai', 'penis', 'capable', 'producing', 'sperm', 'aryan', 'dna', 'much', 'mgtow', 'men', 'wishing', 'ai', 'womb']\n",
            "Final pre-processed text: soy trend continues woman going need ai penis capable producing sperm aryan dna much mgtow men wishing ai womb\n",
            "Text after removing HTML tags: implies female sand niggers are hot ignores the bin bag they wear to hide their hideousness\n",
            "Text after removing non-alphabetic characters and converting to lowercase: implies female sand niggers are hot ignores the bin bag they wear to hide their hideousness\n",
            "Text after tokenization: ['implies', 'female', 'sand', 'niggers', 'are', 'hot', 'ignores', 'the', 'bin', 'bag', 'they', 'wear', 'to', 'hide', 'their', 'hideousness']\n",
            "Text after removing stop words: ['implies', 'female', 'sand', 'niggers', 'hot', 'ignores', 'bin', 'bag', 'wear', 'hide', 'hideousness']\n",
            "Text after Lemmatization: ['implies', 'female', 'sand', 'nigger', 'hot', 'ignores', 'bin', 'bag', 'wear', 'hide', 'hideousness']\n",
            "Final pre-processed text: implies female sand nigger hot ignores bin bag wear hide hideousness\n",
            "Text after removing HTML tags: it so they can not run away when you chase them with a baseball bat lmao they trip on their pants they wonder why they end up in the slammer they look like retarded jerk offs\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it so they can not run away when you chase them with a baseball bat lmao they trip on their pants they wonder why they end up in the slammer they look like retarded jerk offs\n",
            "Text after tokenization: ['it', 'so', 'they', 'can', 'not', 'run', 'away', 'when', 'you', 'chase', 'them', 'with', 'a', 'baseball', 'bat', 'lmao', 'they', 'trip', 'on', 'their', 'pants', 'they', 'wonder', 'why', 'they', 'end', 'up', 'in', 'the', 'slammer', 'they', 'look', 'like', 'retarded', 'jerk', 'offs']\n",
            "Text after removing stop words: ['run', 'away', 'chase', 'baseball', 'bat', 'lmao', 'trip', 'pants', 'wonder', 'end', 'slammer', 'look', 'like', 'retarded', 'jerk', 'offs']\n",
            "Text after Lemmatization: ['run', 'away', 'chase', 'baseball', 'bat', 'lmao', 'trip', 'pant', 'wonder', 'end', 'slammer', 'look', 'like', 'retarded', 'jerk', 'offs']\n",
            "Final pre-processed text: run away chase baseball bat lmao trip pant wonder end slammer look like retarded jerk offs\n",
            "Text after removing HTML tags: i bet he wears a belt niggers are like retarded retards hopefully this white one will get the actual niggers to boycott america and leave it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i bet he wears a belt niggers are like retarded retards hopefully this white one will get the actual niggers to boycott america and leave it\n",
            "Text after tokenization: ['i', 'bet', 'he', 'wears', 'a', 'belt', 'niggers', 'are', 'like', 'retarded', 'retards', 'hopefully', 'this', 'white', 'one', 'will', 'get', 'the', 'actual', 'niggers', 'to', 'boycott', 'america', 'and', 'leave', 'it']\n",
            "Text after removing stop words: ['bet', 'wears', 'belt', 'niggers', 'like', 'retarded', 'retards', 'hopefully', 'white', 'one', 'get', 'actual', 'niggers', 'boycott', 'america', 'leave']\n",
            "Text after Lemmatization: ['bet', 'wear', 'belt', 'nigger', 'like', 'retarded', 'retard', 'hopefully', 'white', 'one', 'get', 'actual', 'nigger', 'boycott', 'america', 'leave']\n",
            "Final pre-processed text: bet wear belt nigger like retarded retard hopefully white one get actual nigger boycott america leave\n",
            "Text after removing HTML tags: i support sellner but let me just say white taqiyya sounds like the gayest thing ever and it will never work why are we emulating a sandnigger cult\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i support sellner but let me just say white taqiyya sounds like the gayest thing ever and it will never work why are we emulating a sandnigger cult\n",
            "Text after tokenization: ['i', 'support', 'sellner', 'but', 'let', 'me', 'just', 'say', 'white', 'taqiyya', 'sounds', 'like', 'the', 'gayest', 'thing', 'ever', 'and', 'it', 'will', 'never', 'work', 'why', 'are', 'we', 'emulating', 'a', 'sandnigger', 'cult']\n",
            "Text after removing stop words: ['support', 'sellner', 'let', 'say', 'white', 'taqiyya', 'sounds', 'like', 'gayest', 'thing', 'ever', 'never', 'work', 'emulating', 'sandnigger', 'cult']\n",
            "Text after Lemmatization: ['support', 'sellner', 'let', 'say', 'white', 'taqiyya', 'sound', 'like', 'gayest', 'thing', 'ever', 'never', 'work', 'emulating', 'sandnigger', 'cult']\n",
            "Final pre-processed text: support sellner let say white taqiyya sound like gayest thing ever never work emulating sandnigger cult\n",
            "Text after removing HTML tags: they were niglet babies i am not saying it is not awful but it pretty much normal behavior for ghetto monkeys\n",
            "Text after removing non-alphabetic characters and converting to lowercase: they were niglet babies i am not saying it is not awful but it pretty much normal behavior for ghetto monkeys\n",
            "Text after tokenization: ['they', 'were', 'niglet', 'babies', 'i', 'am', 'not', 'saying', 'it', 'is', 'not', 'awful', 'but', 'it', 'pretty', 'much', 'normal', 'behavior', 'for', 'ghetto', 'monkeys']\n",
            "Text after removing stop words: ['niglet', 'babies', 'saying', 'awful', 'pretty', 'much', 'normal', 'behavior', 'ghetto', 'monkeys']\n",
            "Text after Lemmatization: ['niglet', 'baby', 'saying', 'awful', 'pretty', 'much', 'normal', 'behavior', 'ghetto', 'monkey']\n",
            "Final pre-processed text: niglet baby saying awful pretty much normal behavior ghetto monkey\n",
            "Text after removing HTML tags: do you accept barack obama as your white brother meghan markle as your white sister and yet your step brother or step sister is treated as your bio family right we accept them as part of black fam but most caucasians do not accept half whites as white fam sad\n",
            "Text after removing non-alphabetic characters and converting to lowercase: do you accept barack obama as your white brother meghan markle as your white sister and yet your step brother or step sister is treated as your bio family right we accept them as part of black fam but most caucasians do not accept half whites as white fam sad\n",
            "Text after tokenization: ['do', 'you', 'accept', 'barack', 'obama', 'as', 'your', 'white', 'brother', 'meghan', 'markle', 'as', 'your', 'white', 'sister', 'and', 'yet', 'your', 'step', 'brother', 'or', 'step', 'sister', 'is', 'treated', 'as', 'your', 'bio', 'family', 'right', 'we', 'accept', 'them', 'as', 'part', 'of', 'black', 'fam', 'but', 'most', 'caucasians', 'do', 'not', 'accept', 'half', 'whites', 'as', 'white', 'fam', 'sad']\n",
            "Text after removing stop words: ['accept', 'barack', 'obama', 'white', 'brother', 'meghan', 'markle', 'white', 'sister', 'yet', 'step', 'brother', 'step', 'sister', 'treated', 'bio', 'family', 'right', 'accept', 'part', 'black', 'fam', 'caucasians', 'accept', 'half', 'whites', 'white', 'fam', 'sad']\n",
            "Text after Lemmatization: ['accept', 'barack', 'obama', 'white', 'brother', 'meghan', 'markle', 'white', 'sister', 'yet', 'step', 'brother', 'step', 'sister', 'treated', 'bio', 'family', 'right', 'accept', 'part', 'black', 'fam', 'caucasian', 'accept', 'half', 'white', 'white', 'fam', 'sad']\n",
            "Final pre-processed text: accept barack obama white brother meghan markle white sister yet step brother step sister treated bio family right accept part black fam caucasian accept half white white fam sad\n",
            "Text after removing HTML tags: if the democrat party is such a good things why are they trying to find new supporter by changing law that allow refugees and illegal immigrants to voter and they are still losing\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if the democrat party is such a good things why are they trying to find new supporter by changing law that allow refugees and illegal immigrants to voter and they are still losing\n",
            "Text after tokenization: ['if', 'the', 'democrat', 'party', 'is', 'such', 'a', 'good', 'things', 'why', 'are', 'they', 'trying', 'to', 'find', 'new', 'supporter', 'by', 'changing', 'law', 'that', 'allow', 'refugees', 'and', 'illegal', 'immigrants', 'to', 'voter', 'and', 'they', 'are', 'still', 'losing']\n",
            "Text after removing stop words: ['democrat', 'party', 'good', 'things', 'trying', 'find', 'new', 'supporter', 'changing', 'law', 'allow', 'refugees', 'illegal', 'immigrants', 'voter', 'still', 'losing']\n",
            "Text after Lemmatization: ['democrat', 'party', 'good', 'thing', 'trying', 'find', 'new', 'supporter', 'changing', 'law', 'allow', 'refugee', 'illegal', 'immigrant', 'voter', 'still', 'losing']\n",
            "Final pre-processed text: democrat party good thing trying find new supporter changing law allow refugee illegal immigrant voter still losing\n",
            "Text after removing HTML tags: oy vey we have to help the minorities because they can not achieve anything especially if you know that native americans made peanut butter before jews brought blacks to the us einstein stole relativity from an italian\n",
            "Text after removing non-alphabetic characters and converting to lowercase: oy vey we have to help the minorities because they can not achieve anything especially if you know that native americans made peanut butter before jews brought blacks to the us einstein stole relativity from an italian\n",
            "Text after tokenization: ['oy', 'vey', 'we', 'have', 'to', 'help', 'the', 'minorities', 'because', 'they', 'can', 'not', 'achieve', 'anything', 'especially', 'if', 'you', 'know', 'that', 'native', 'americans', 'made', 'peanut', 'butter', 'before', 'jews', 'brought', 'blacks', 'to', 'the', 'us', 'einstein', 'stole', 'relativity', 'from', 'an', 'italian']\n",
            "Text after removing stop words: ['oy', 'vey', 'help', 'minorities', 'achieve', 'anything', 'especially', 'know', 'native', 'americans', 'made', 'peanut', 'butter', 'jews', 'brought', 'blacks', 'us', 'einstein', 'stole', 'relativity', 'italian']\n",
            "Text after Lemmatization: ['oy', 'vey', 'help', 'minority', 'achieve', 'anything', 'especially', 'know', 'native', 'american', 'made', 'peanut', 'butter', 'jew', 'brought', 'black', 'u', 'einstein', 'stole', 'relativity', 'italian']\n",
            "Final pre-processed text: oy vey help minority achieve anything especially know native american made peanut butter jew brought black u einstein stole relativity italian\n",
            "Text after removing HTML tags: he was a faggot just like you\n",
            "Text after removing non-alphabetic characters and converting to lowercase: he was a faggot just like you\n",
            "Text after tokenization: ['he', 'was', 'a', 'faggot', 'just', 'like', 'you']\n",
            "Text after removing stop words: ['faggot', 'like']\n",
            "Text after Lemmatization: ['faggot', 'like']\n",
            "Final pre-processed text: faggot like\n",
            "Text after removing HTML tags: the disturbing reason trump evangelical base is thrilled to see violence in gaza hotpagenews com r \n",
            "Text after removing non-alphabetic characters and converting to lowercase: the disturbing reason trump evangelical base is thrilled to see violence in gaza hotpagenews com r \n",
            "Text after tokenization: ['the', 'disturbing', 'reason', 'trump', 'evangelical', 'base', 'is', 'thrilled', 'to', 'see', 'violence', 'in', 'gaza', 'hotpagenews', 'com', 'r']\n",
            "Text after removing stop words: ['disturbing', 'reason', 'trump', 'evangelical', 'base', 'thrilled', 'see', 'violence', 'gaza', 'hotpagenews', 'com', 'r']\n",
            "Text after Lemmatization: ['disturbing', 'reason', 'trump', 'evangelical', 'base', 'thrilled', 'see', 'violence', 'gaza', 'hotpagenews', 'com', 'r']\n",
            "Final pre-processed text: disturbing reason trump evangelical base thrilled see violence gaza hotpagenews com r\n",
            "Text after removing HTML tags: defending muslims being in europe saying that islam isnt the greatest threat to the white race pick one kys for having the reading comprehension of a goat\n",
            "Text after removing non-alphabetic characters and converting to lowercase: defending muslims being in europe saying that islam isnt the greatest threat to the white race pick one kys for having the reading comprehension of a goat\n",
            "Text after tokenization: ['defending', 'muslims', 'being', 'in', 'europe', 'saying', 'that', 'islam', 'isnt', 'the', 'greatest', 'threat', 'to', 'the', 'white', 'race', 'pick', 'one', 'kys', 'for', 'having', 'the', 'reading', 'comprehension', 'of', 'a', 'goat']\n",
            "Text after removing stop words: ['defending', 'muslims', 'europe', 'saying', 'islam', 'isnt', 'greatest', 'threat', 'white', 'race', 'pick', 'one', 'kys', 'reading', 'comprehension', 'goat']\n",
            "Text after Lemmatization: ['defending', 'muslim', 'europe', 'saying', 'islam', 'isnt', 'greatest', 'threat', 'white', 'race', 'pick', 'one', 'ky', 'reading', 'comprehension', 'goat']\n",
            "Final pre-processed text: defending muslim europe saying islam isnt greatest threat white race pick one ky reading comprehension goat\n",
            "Text after removing HTML tags: something seems all wrong about this whole thing — should not there be some retard screaming about how there culture is not her goddamn wedding dress 😂 royalwedding meghanandharry\n",
            "Text after removing non-alphabetic characters and converting to lowercase: something seems all wrong about this whole thing   should not there be some retard screaming about how there culture is not her goddamn wedding dress   royalwedding meghanandharry\n",
            "Text after tokenization: ['something', 'seems', 'all', 'wrong', 'about', 'this', 'whole', 'thing', 'should', 'not', 'there', 'be', 'some', 'retard', 'screaming', 'about', 'how', 'there', 'culture', 'is', 'not', 'her', 'goddamn', 'wedding', 'dress', 'royalwedding', 'meghanandharry']\n",
            "Text after removing stop words: ['something', 'seems', 'wrong', 'whole', 'thing', 'retard', 'screaming', 'culture', 'goddamn', 'wedding', 'dress', 'royalwedding', 'meghanandharry']\n",
            "Text after Lemmatization: ['something', 'seems', 'wrong', 'whole', 'thing', 'retard', 'screaming', 'culture', 'goddamn', 'wedding', 'dress', 'royalwedding', 'meghanandharry']\n",
            "Final pre-processed text: something seems wrong whole thing retard screaming culture goddamn wedding dress royalwedding meghanandharry\n",
            "Text after removing HTML tags: when you cannot deny that the democrats are going above board for illegal immigrants and non humans do not think compassion think votes\n",
            "Text after removing non-alphabetic characters and converting to lowercase: when you cannot deny that the democrats are going above board for illegal immigrants and non humans do not think compassion think votes\n",
            "Text after tokenization: ['when', 'you', 'can', 'not', 'deny', 'that', 'the', 'democrats', 'are', 'going', 'above', 'board', 'for', 'illegal', 'immigrants', 'and', 'non', 'humans', 'do', 'not', 'think', 'compassion', 'think', 'votes']\n",
            "Text after removing stop words: ['deny', 'democrats', 'going', 'board', 'illegal', 'immigrants', 'non', 'humans', 'think', 'compassion', 'think', 'votes']\n",
            "Text after Lemmatization: ['deny', 'democrat', 'going', 'board', 'illegal', 'immigrant', 'non', 'human', 'think', 'compassion', 'think', 'vote']\n",
            "Final pre-processed text: deny democrat going board illegal immigrant non human think compassion think vote\n",
            "Text after removing HTML tags: another sand nigger bites the dust\n",
            "Text after removing non-alphabetic characters and converting to lowercase: another sand nigger bites the dust\n",
            "Text after tokenization: ['another', 'sand', 'nigger', 'bites', 'the', 'dust']\n",
            "Text after removing stop words: ['another', 'sand', 'nigger', 'bites', 'dust']\n",
            "Text after Lemmatization: ['another', 'sand', 'nigger', 'bite', 'dust']\n",
            "Final pre-processed text: another sand nigger bite dust\n",
            "Text after removing HTML tags: abortions for niggers jews arabs spics gooks should not be a right it should be a law white women should never have an abortion because it takes up valuable resources for people of colour if you are a white women and you have an abortion you should be labeled a racist  ‍\n",
            "Text after removing non-alphabetic characters and converting to lowercase: abortions for niggers jews arabs spics gooks should not be a right it should be a law white women should never have an abortion because it takes up valuable resources for people of colour if you are a white women and you have an abortion you should be labeled a racist   \n",
            "Text after tokenization: ['abortions', 'for', 'niggers', 'jews', 'arabs', 'spics', 'gooks', 'should', 'not', 'be', 'a', 'right', 'it', 'should', 'be', 'a', 'law', 'white', 'women', 'should', 'never', 'have', 'an', 'abortion', 'because', 'it', 'takes', 'up', 'valuable', 'resources', 'for', 'people', 'of', 'colour', 'if', 'you', 'are', 'a', 'white', 'women', 'and', 'you', 'have', 'an', 'abortion', 'you', 'should', 'be', 'labeled', 'a', 'racist']\n",
            "Text after removing stop words: ['abortions', 'niggers', 'jews', 'arabs', 'spics', 'gooks', 'right', 'law', 'white', 'women', 'never', 'abortion', 'takes', 'valuable', 'resources', 'people', 'colour', 'white', 'women', 'abortion', 'labeled', 'racist']\n",
            "Text after Lemmatization: ['abortion', 'nigger', 'jew', 'arab', 'spic', 'gook', 'right', 'law', 'white', 'woman', 'never', 'abortion', 'take', 'valuable', 'resource', 'people', 'colour', 'white', 'woman', 'abortion', 'labeled', 'racist']\n",
            "Final pre-processed text: abortion nigger jew arab spic gook right law white woman never abortion take valuable resource people colour white woman abortion labeled racist\n",
            "Text after removing HTML tags: dear team if you hear myself and bangoob and michael winner spouting some retarded complicated shit that seems heretical just trust us our shit is aimed at mind breaking normies if you have the intelligence you should join us\n",
            "Text after removing non-alphabetic characters and converting to lowercase: dear team if you hear myself and bangoob and michael winner spouting some retarded complicated shit that seems heretical just trust us our shit is aimed at mind breaking normies if you have the intelligence you should join us\n",
            "Text after tokenization: ['dear', 'team', 'if', 'you', 'hear', 'myself', 'and', 'bangoob', 'and', 'michael', 'winner', 'spouting', 'some', 'retarded', 'complicated', 'shit', 'that', 'seems', 'heretical', 'just', 'trust', 'us', 'our', 'shit', 'is', 'aimed', 'at', 'mind', 'breaking', 'normies', 'if', 'you', 'have', 'the', 'intelligence', 'you', 'should', 'join', 'us']\n",
            "Text after removing stop words: ['dear', 'team', 'hear', 'bangoob', 'michael', 'winner', 'spouting', 'retarded', 'complicated', 'shit', 'seems', 'heretical', 'trust', 'us', 'shit', 'aimed', 'mind', 'breaking', 'normies', 'intelligence', 'join', 'us']\n",
            "Text after Lemmatization: ['dear', 'team', 'hear', 'bangoob', 'michael', 'winner', 'spouting', 'retarded', 'complicated', 'shit', 'seems', 'heretical', 'trust', 'u', 'shit', 'aimed', 'mind', 'breaking', 'normies', 'intelligence', 'join', 'u']\n",
            "Final pre-processed text: dear team hear bangoob michael winner spouting retarded complicated shit seems heretical trust u shit aimed mind breaking normies intelligence join u\n",
            "Text after removing HTML tags:  ‍ i had to give you a o for this line and imagine a homosexual like lemon railing about someone undermining the values and traditions of our society how many layers of subversion are you on my dude ‍\n",
            "Text after removing non-alphabetic characters and converting to lowercase:    i had to give you a o for this line and imagine a homosexual like lemon railing about someone undermining the values and traditions of our society how many layers of subversion are you on my dude  \n",
            "Text after tokenization: ['i', 'had', 'to', 'give', 'you', 'a', 'o', 'for', 'this', 'line', 'and', 'imagine', 'a', 'homosexual', 'like', 'lemon', 'railing', 'about', 'someone', 'undermining', 'the', 'values', 'and', 'traditions', 'of', 'our', 'society', 'how', 'many', 'layers', 'of', 'subversion', 'are', 'you', 'on', 'my', 'dude']\n",
            "Text after removing stop words: ['give', 'line', 'imagine', 'homosexual', 'like', 'lemon', 'railing', 'someone', 'undermining', 'values', 'traditions', 'society', 'many', 'layers', 'subversion', 'dude']\n",
            "Text after Lemmatization: ['give', 'line', 'imagine', 'homosexual', 'like', 'lemon', 'railing', 'someone', 'undermining', 'value', 'tradition', 'society', 'many', 'layer', 'subversion', 'dude']\n",
            "Final pre-processed text: give line imagine homosexual like lemon railing someone undermining value tradition society many layer subversion dude\n",
            "Text after removing HTML tags: women have spontaneous multiple orgasms when they see a woman better looking than themselves insulted they love that more than damn near anything\n",
            "Text after removing non-alphabetic characters and converting to lowercase: women have spontaneous multiple orgasms when they see a woman better looking than themselves insulted they love that more than damn near anything\n",
            "Text after tokenization: ['women', 'have', 'spontaneous', 'multiple', 'orgasms', 'when', 'they', 'see', 'a', 'woman', 'better', 'looking', 'than', 'themselves', 'insulted', 'they', 'love', 'that', 'more', 'than', 'damn', 'near', 'anything']\n",
            "Text after removing stop words: ['women', 'spontaneous', 'multiple', 'orgasms', 'see', 'woman', 'better', 'looking', 'insulted', 'love', 'damn', 'near', 'anything']\n",
            "Text after Lemmatization: ['woman', 'spontaneous', 'multiple', 'orgasm', 'see', 'woman', 'better', 'looking', 'insulted', 'love', 'damn', 'near', 'anything']\n",
            "Final pre-processed text: woman spontaneous multiple orgasm see woman better looking insulted love damn near anything\n",
            "Text after removing HTML tags: i am not disagreeing with your version but do not you think that backing off of jewz mudslimes looks awfully suspicious in light of what going on today i do\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am not disagreeing with your version but do not you think that backing off of jewz mudslimes looks awfully suspicious in light of what going on today i do\n",
            "Text after tokenization: ['i', 'am', 'not', 'disagreeing', 'with', 'your', 'version', 'but', 'do', 'not', 'you', 'think', 'that', 'backing', 'off', 'of', 'jewz', 'mudslimes', 'looks', 'awfully', 'suspicious', 'in', 'light', 'of', 'what', 'going', 'on', 'today', 'i', 'do']\n",
            "Text after removing stop words: ['disagreeing', 'version', 'think', 'backing', 'jewz', 'mudslimes', 'looks', 'awfully', 'suspicious', 'light', 'going', 'today']\n",
            "Text after Lemmatization: ['disagreeing', 'version', 'think', 'backing', 'jewz', 'mudslimes', 'look', 'awfully', 'suspicious', 'light', 'going', 'today']\n",
            "Final pre-processed text: disagreeing version think backing jewz mudslimes look awfully suspicious light going today\n",
            "Text after removing HTML tags: traddad realizing that white women are corrupted is important knowing they are the only route to a white population is the next step figuring out how to un whore them and rebuild patriarchy is the result of these realizations if you are not considering this you are too nihilist for our movement\n",
            "Text after removing non-alphabetic characters and converting to lowercase: traddad realizing that white women are corrupted is important knowing they are the only route to a white population is the next step figuring out how to un whore them and rebuild patriarchy is the result of these realizations if you are not considering this you are too nihilist for our movement\n",
            "Text after tokenization: ['traddad', 'realizing', 'that', 'white', 'women', 'are', 'corrupted', 'is', 'important', 'knowing', 'they', 'are', 'the', 'only', 'route', 'to', 'a', 'white', 'population', 'is', 'the', 'next', 'step', 'figuring', 'out', 'how', 'to', 'un', 'whore', 'them', 'and', 'rebuild', 'patriarchy', 'is', 'the', 'result', 'of', 'these', 'realizations', 'if', 'you', 'are', 'not', 'considering', 'this', 'you', 'are', 'too', 'nihilist', 'for', 'our', 'movement']\n",
            "Text after removing stop words: ['traddad', 'realizing', 'white', 'women', 'corrupted', 'important', 'knowing', 'route', 'white', 'population', 'next', 'step', 'figuring', 'un', 'whore', 'rebuild', 'patriarchy', 'result', 'realizations', 'considering', 'nihilist', 'movement']\n",
            "Text after Lemmatization: ['traddad', 'realizing', 'white', 'woman', 'corrupted', 'important', 'knowing', 'route', 'white', 'population', 'next', 'step', 'figuring', 'un', 'whore', 'rebuild', 'patriarchy', 'result', 'realization', 'considering', 'nihilist', 'movement']\n",
            "Final pre-processed text: traddad realizing white woman corrupted important knowing route white population next step figuring un whore rebuild patriarchy result realization considering nihilist movement\n",
            "Text after removing HTML tags: did you know i argued with a nazi for a good long who argued that hot idf female soldiers are actually ukrainian models he provided absolutely no proof when pressured even reveled in the fact that he was full of shit\n",
            "Text after removing non-alphabetic characters and converting to lowercase: did you know i argued with a nazi for a good long who argued that hot idf female soldiers are actually ukrainian models he provided absolutely no proof when pressured even reveled in the fact that he was full of shit\n",
            "Text after tokenization: ['did', 'you', 'know', 'i', 'argued', 'with', 'a', 'nazi', 'for', 'a', 'good', 'long', 'who', 'argued', 'that', 'hot', 'idf', 'female', 'soldiers', 'are', 'actually', 'ukrainian', 'models', 'he', 'provided', 'absolutely', 'no', 'proof', 'when', 'pressured', 'even', 'reveled', 'in', 'the', 'fact', 'that', 'he', 'was', 'full', 'of', 'shit']\n",
            "Text after removing stop words: ['know', 'argued', 'nazi', 'good', 'long', 'argued', 'hot', 'idf', 'female', 'soldiers', 'actually', 'ukrainian', 'models', 'provided', 'absolutely', 'proof', 'pressured', 'even', 'reveled', 'fact', 'full', 'shit']\n",
            "Text after Lemmatization: ['know', 'argued', 'nazi', 'good', 'long', 'argued', 'hot', 'idf', 'female', 'soldier', 'actually', 'ukrainian', 'model', 'provided', 'absolutely', 'proof', 'pressured', 'even', 'reveled', 'fact', 'full', 'shit']\n",
            "Final pre-processed text: know argued nazi good long argued hot idf female soldier actually ukrainian model provided absolutely proof pressured even reveled fact full shit\n",
            "Text after removing HTML tags: what do you all suppose the likelihood is that the judge is a sandnigger grooming gang customer who rapes little girls or boys\n",
            "Text after removing non-alphabetic characters and converting to lowercase: what do you all suppose the likelihood is that the judge is a sandnigger grooming gang customer who rapes little girls or boys\n",
            "Text after tokenization: ['what', 'do', 'you', 'all', 'suppose', 'the', 'likelihood', 'is', 'that', 'the', 'judge', 'is', 'a', 'sandnigger', 'grooming', 'gang', 'customer', 'who', 'rapes', 'little', 'girls', 'or', 'boys']\n",
            "Text after removing stop words: ['suppose', 'likelihood', 'judge', 'sandnigger', 'grooming', 'gang', 'customer', 'rapes', 'little', 'girls', 'boys']\n",
            "Text after Lemmatization: ['suppose', 'likelihood', 'judge', 'sandnigger', 'grooming', 'gang', 'customer', 'rape', 'little', 'girl', 'boy']\n",
            "Final pre-processed text: suppose likelihood judge sandnigger grooming gang customer rape little girl boy\n",
            "Text after removing HTML tags: come together as humans means join the african and muslim rapists talk about a dog whistle\n",
            "Text after removing non-alphabetic characters and converting to lowercase: come together as humans means join the african and muslim rapists talk about a dog whistle\n",
            "Text after tokenization: ['come', 'together', 'as', 'humans', 'means', 'join', 'the', 'african', 'and', 'muslim', 'rapists', 'talk', 'about', 'a', 'dog', 'whistle']\n",
            "Text after removing stop words: ['come', 'together', 'humans', 'means', 'join', 'african', 'muslim', 'rapists', 'talk', 'dog', 'whistle']\n",
            "Text after Lemmatization: ['come', 'together', 'human', 'mean', 'join', 'african', 'muslim', 'rapist', 'talk', 'dog', 'whistle']\n",
            "Final pre-processed text: come together human mean join african muslim rapist talk dog whistle\n",
            "Text after removing HTML tags: starbucks everybody remember that in muslim countries they do not allow women kindly ask them to wait in the car and send a man\n",
            "Text after removing non-alphabetic characters and converting to lowercase: starbucks everybody remember that in muslim countries they do not allow women kindly ask them to wait in the car and send a man\n",
            "Text after tokenization: ['starbucks', 'everybody', 'remember', 'that', 'in', 'muslim', 'countries', 'they', 'do', 'not', 'allow', 'women', 'kindly', 'ask', 'them', 'to', 'wait', 'in', 'the', 'car', 'and', 'send', 'a', 'man']\n",
            "Text after removing stop words: ['starbucks', 'everybody', 'remember', 'muslim', 'countries', 'allow', 'women', 'kindly', 'ask', 'wait', 'car', 'send', 'man']\n",
            "Text after Lemmatization: ['starbucks', 'everybody', 'remember', 'muslim', 'country', 'allow', 'woman', 'kindly', 'ask', 'wait', 'car', 'send', 'man']\n",
            "Final pre-processed text: starbucks everybody remember muslim country allow woman kindly ask wait car send man\n",
            "Text after removing HTML tags: remember nelson mandela  is a white guy\n",
            "Text after removing non-alphabetic characters and converting to lowercase: remember nelson mandela  is a white guy\n",
            "Text after tokenization: ['remember', 'nelson', 'mandela', 'is', 'a', 'white', 'guy']\n",
            "Text after removing stop words: ['remember', 'nelson', 'mandela', 'white', 'guy']\n",
            "Text after Lemmatization: ['remember', 'nelson', 'mandela', 'white', 'guy']\n",
            "Final pre-processed text: remember nelson mandela white guy\n",
            "Text after removing HTML tags: i never trust anyone in a burqa not because they are muslim most muslims are decent people dont me but because anyone who covers their face has something to hide\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i never trust anyone in a burqa not because they are muslim most muslims are decent people dont me but because anyone who covers their face has something to hide\n",
            "Text after tokenization: ['i', 'never', 'trust', 'anyone', 'in', 'a', 'burqa', 'not', 'because', 'they', 'are', 'muslim', 'most', 'muslims', 'are', 'decent', 'people', 'dont', 'me', 'but', 'because', 'anyone', 'who', 'covers', 'their', 'face', 'has', 'something', 'to', 'hide']\n",
            "Text after removing stop words: ['never', 'trust', 'anyone', 'burqa', 'muslim', 'muslims', 'decent', 'people', 'dont', 'anyone', 'covers', 'face', 'something', 'hide']\n",
            "Text after Lemmatization: ['never', 'trust', 'anyone', 'burqa', 'muslim', 'muslim', 'decent', 'people', 'dont', 'anyone', 'cover', 'face', 'something', 'hide']\n",
            "Final pre-processed text: never trust anyone burqa muslim muslim decent people dont anyone cover face something hide\n",
            "Text after removing HTML tags: how on earth did the muslim paedophiles in rotherham operate with impunity for so long\n",
            "Text after removing non-alphabetic characters and converting to lowercase: how on earth did the muslim paedophiles in rotherham operate with impunity for so long\n",
            "Text after tokenization: ['how', 'on', 'earth', 'did', 'the', 'muslim', 'paedophiles', 'in', 'rotherham', 'operate', 'with', 'impunity', 'for', 'so', 'long']\n",
            "Text after removing stop words: ['earth', 'muslim', 'paedophiles', 'rotherham', 'operate', 'impunity', 'long']\n",
            "Text after Lemmatization: ['earth', 'muslim', 'paedophile', 'rotherham', 'operate', 'impunity', 'long']\n",
            "Final pre-processed text: earth muslim paedophile rotherham operate impunity long\n",
            "Text after removing HTML tags: israel home of the jewish people arabs jordan u may know that israel was promised to the jewish people well b4 ur saviour came or any arab knew where israel was clue it is in the chumash pots melamed and ps u can call me sir\n",
            "Text after removing non-alphabetic characters and converting to lowercase: israel home of the jewish people arabs jordan u may know that israel was promised to the jewish people well b  ur saviour came or any arab knew where israel was clue it is in the chumash pots melamed and ps u can call me sir\n",
            "Text after tokenization: ['israel', 'home', 'of', 'the', 'jewish', 'people', 'arabs', 'jordan', 'u', 'may', 'know', 'that', 'israel', 'was', 'promised', 'to', 'the', 'jewish', 'people', 'well', 'b', 'ur', 'saviour', 'came', 'or', 'any', 'arab', 'knew', 'where', 'israel', 'was', 'clue', 'it', 'is', 'in', 'the', 'chumash', 'pots', 'melamed', 'and', 'ps', 'u', 'can', 'call', 'me', 'sir']\n",
            "Text after removing stop words: ['israel', 'home', 'jewish', 'people', 'arabs', 'jordan', 'u', 'may', 'know', 'israel', 'promised', 'jewish', 'people', 'well', 'b', 'ur', 'saviour', 'came', 'arab', 'knew', 'israel', 'clue', 'chumash', 'pots', 'melamed', 'ps', 'u', 'call', 'sir']\n",
            "Text after Lemmatization: ['israel', 'home', 'jewish', 'people', 'arab', 'jordan', 'u', 'may', 'know', 'israel', 'promised', 'jewish', 'people', 'well', 'b', 'ur', 'saviour', 'came', 'arab', 'knew', 'israel', 'clue', 'chumash', 'pot', 'melamed', 'p', 'u', 'call', 'sir']\n",
            "Final pre-processed text: israel home jewish people arab jordan u may know israel promised jewish people well b ur saviour came arab knew israel clue chumash pot melamed p u call sir\n",
            "Text after removing HTML tags: because europe so wanted to love the arabs and fuck israel remember you all got your wish\n",
            "Text after removing non-alphabetic characters and converting to lowercase: because europe so wanted to love the arabs and fuck israel remember you all got your wish\n",
            "Text after tokenization: ['because', 'europe', 'so', 'wanted', 'to', 'love', 'the', 'arabs', 'and', 'fuck', 'israel', 'remember', 'you', 'all', 'got', 'your', 'wish']\n",
            "Text after removing stop words: ['europe', 'wanted', 'love', 'arabs', 'fuck', 'israel', 'remember', 'got', 'wish']\n",
            "Text after Lemmatization: ['europe', 'wanted', 'love', 'arab', 'fuck', 'israel', 'remember', 'got', 'wish']\n",
            "Final pre-processed text: europe wanted love arab fuck israel remember got wish\n",
            "Text after removing HTML tags: italy tired of afrikas and muslim migrations start sending back them to their own world 👍\n",
            "Text after removing non-alphabetic characters and converting to lowercase: italy tired of afrikas and muslim migrations start sending back them to their own world  \n",
            "Text after tokenization: ['italy', 'tired', 'of', 'afrikas', 'and', 'muslim', 'migrations', 'start', 'sending', 'back', 'them', 'to', 'their', 'own', 'world']\n",
            "Text after removing stop words: ['italy', 'tired', 'afrikas', 'muslim', 'migrations', 'start', 'sending', 'back', 'world']\n",
            "Text after Lemmatization: ['italy', 'tired', 'afrikas', 'muslim', 'migration', 'start', 'sending', 'back', 'world']\n",
            "Final pre-processed text: italy tired afrikas muslim migration start sending back world\n",
            "Text after removing HTML tags:  of  being one court ruling away from yet another national moral failure is no way to live also what has scotus given us over the years but sanctioned infanticide forced racial integration birthright citizenship for foreigners homosexual marriage the god given right to self determination being declared nonexistent\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  of  being one court ruling away from yet another national moral failure is no way to live also what has scotus given us over the years but sanctioned infanticide forced racial integration birthright citizenship for foreigners homosexual marriage the god given right to self determination being declared nonexistent\n",
            "Text after tokenization: ['of', 'being', 'one', 'court', 'ruling', 'away', 'from', 'yet', 'another', 'national', 'moral', 'failure', 'is', 'no', 'way', 'to', 'live', 'also', 'what', 'has', 'scotus', 'given', 'us', 'over', 'the', 'years', 'but', 'sanctioned', 'infanticide', 'forced', 'racial', 'integration', 'birthright', 'citizenship', 'for', 'foreigners', 'homosexual', 'marriage', 'the', 'god', 'given', 'right', 'to', 'self', 'determination', 'being', 'declared', 'nonexistent']\n",
            "Text after removing stop words: ['one', 'court', 'ruling', 'away', 'yet', 'another', 'national', 'moral', 'failure', 'way', 'live', 'also', 'scotus', 'given', 'us', 'years', 'sanctioned', 'infanticide', 'forced', 'racial', 'integration', 'birthright', 'citizenship', 'foreigners', 'homosexual', 'marriage', 'god', 'given', 'right', 'self', 'determination', 'declared', 'nonexistent']\n",
            "Text after Lemmatization: ['one', 'court', 'ruling', 'away', 'yet', 'another', 'national', 'moral', 'failure', 'way', 'live', 'also', 'scotus', 'given', 'u', 'year', 'sanctioned', 'infanticide', 'forced', 'racial', 'integration', 'birthright', 'citizenship', 'foreigner', 'homosexual', 'marriage', 'god', 'given', 'right', 'self', 'determination', 'declared', 'nonexistent']\n",
            "Final pre-processed text: one court ruling away yet another national moral failure way live also scotus given u year sanctioned infanticide forced racial integration birthright citizenship foreigner homosexual marriage god given right self determination declared nonexistent\n",
            "Text after removing HTML tags: just on the off chance you were not completely convinced that racism and bonus blacks were retarded fuckwits here comes this heart warming tale from black twitter\n",
            "Text after removing non-alphabetic characters and converting to lowercase: just on the off chance you were not completely convinced that racism and bonus blacks were retarded fuckwits here comes this heart warming tale from black twitter\n",
            "Text after tokenization: ['just', 'on', 'the', 'off', 'chance', 'you', 'were', 'not', 'completely', 'convinced', 'that', 'racism', 'and', 'bonus', 'blacks', 'were', 'retarded', 'fuckwits', 'here', 'comes', 'this', 'heart', 'warming', 'tale', 'from', 'black', 'twitter']\n",
            "Text after removing stop words: ['chance', 'completely', 'convinced', 'racism', 'bonus', 'blacks', 'retarded', 'fuckwits', 'comes', 'heart', 'warming', 'tale', 'black', 'twitter']\n",
            "Text after Lemmatization: ['chance', 'completely', 'convinced', 'racism', 'bonus', 'black', 'retarded', 'fuckwits', 'come', 'heart', 'warming', 'tale', 'black', 'twitter']\n",
            "Final pre-processed text: chance completely convinced racism bonus black retarded fuckwits come heart warming tale black twitter\n",
            "Text after removing HTML tags: i never realized how concerned for civil rights people chronically accused of domestic violence were tuesdaythoughts nfl eagles\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i never realized how concerned for civil rights people chronically accused of domestic violence were tuesdaythoughts nfl eagles\n",
            "Text after tokenization: ['i', 'never', 'realized', 'how', 'concerned', 'for', 'civil', 'rights', 'people', 'chronically', 'accused', 'of', 'domestic', 'violence', 'were', 'tuesdaythoughts', 'nfl', 'eagles']\n",
            "Text after removing stop words: ['never', 'realized', 'concerned', 'civil', 'rights', 'people', 'chronically', 'accused', 'domestic', 'violence', 'tuesdaythoughts', 'nfl', 'eagles']\n",
            "Text after Lemmatization: ['never', 'realized', 'concerned', 'civil', 'right', 'people', 'chronically', 'accused', 'domestic', 'violence', 'tuesdaythoughts', 'nfl', 'eagle']\n",
            "Final pre-processed text: never realized concerned civil right people chronically accused domestic violence tuesdaythoughts nfl eagle\n",
            "Text after removing HTML tags: lack of direction infighting we should shoot the niggers then the jews then moslems no we should shoot the jews then niggers than moslems no we should shoot the jews then wetbacks than moslems then nigs no we should shoot moslems then jews then nigs then wetbacks then moslem\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lack of direction infighting we should shoot the niggers then the jews then moslems no we should shoot the jews then niggers than moslems no we should shoot the jews then wetbacks than moslems then nigs no we should shoot moslems then jews then nigs then wetbacks then moslem\n",
            "Text after tokenization: ['lack', 'of', 'direction', 'infighting', 'we', 'should', 'shoot', 'the', 'niggers', 'then', 'the', 'jews', 'then', 'moslems', 'no', 'we', 'should', 'shoot', 'the', 'jews', 'then', 'niggers', 'than', 'moslems', 'no', 'we', 'should', 'shoot', 'the', 'jews', 'then', 'wetbacks', 'than', 'moslems', 'then', 'nigs', 'no', 'we', 'should', 'shoot', 'moslems', 'then', 'jews', 'then', 'nigs', 'then', 'wetbacks', 'then', 'moslem']\n",
            "Text after removing stop words: ['lack', 'direction', 'infighting', 'shoot', 'niggers', 'jews', 'moslems', 'shoot', 'jews', 'niggers', 'moslems', 'shoot', 'jews', 'wetbacks', 'moslems', 'nigs', 'shoot', 'moslems', 'jews', 'nigs', 'wetbacks', 'moslem']\n",
            "Text after Lemmatization: ['lack', 'direction', 'infighting', 'shoot', 'nigger', 'jew', 'moslem', 'shoot', 'jew', 'nigger', 'moslem', 'shoot', 'jew', 'wetback', 'moslem', 'nigs', 'shoot', 'moslem', 'jew', 'nigs', 'wetback', 'moslem']\n",
            "Final pre-processed text: lack direction infighting shoot nigger jew moslem shoot jew nigger moslem shoot jew wetback moslem nigs shoot moslem jew nigs wetback moslem\n",
            "Text after removing HTML tags: lol at edwin you are a faggot you lost bigly edwin duterte is a faggot and the philippines are a joke keep those call centers open for the white liberals though\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lol at edwin you are a faggot you lost bigly edwin duterte is a faggot and the philippines are a joke keep those call centers open for the white liberals though\n",
            "Text after tokenization: ['lol', 'at', 'edwin', 'you', 'are', 'a', 'faggot', 'you', 'lost', 'bigly', 'edwin', 'duterte', 'is', 'a', 'faggot', 'and', 'the', 'philippines', 'are', 'a', 'joke', 'keep', 'those', 'call', 'centers', 'open', 'for', 'the', 'white', 'liberals', 'though']\n",
            "Text after removing stop words: ['lol', 'edwin', 'faggot', 'lost', 'bigly', 'edwin', 'duterte', 'faggot', 'philippines', 'joke', 'keep', 'call', 'centers', 'open', 'white', 'liberals', 'though']\n",
            "Text after Lemmatization: ['lol', 'edwin', 'faggot', 'lost', 'bigly', 'edwin', 'duterte', 'faggot', 'philippine', 'joke', 'keep', 'call', 'center', 'open', 'white', 'liberal', 'though']\n",
            "Final pre-processed text: lol edwin faggot lost bigly edwin duterte faggot philippine joke keep call center open white liberal though\n",
            "Text after removing HTML tags: big smile on my face when i think about all the college snowflakes who were so close to banning free speech persecuting white males for being white and male popularizing feminism lesbianism and forwarding the end of times how suddenly shitty their lives must be speakfreely maga gab\n",
            "Text after removing non-alphabetic characters and converting to lowercase: big smile on my face when i think about all the college snowflakes who were so close to banning free speech persecuting white males for being white and male popularizing feminism lesbianism and forwarding the end of times how suddenly shitty their lives must be speakfreely maga gab\n",
            "Text after tokenization: ['big', 'smile', 'on', 'my', 'face', 'when', 'i', 'think', 'about', 'all', 'the', 'college', 'snowflakes', 'who', 'were', 'so', 'close', 'to', 'banning', 'free', 'speech', 'persecuting', 'white', 'males', 'for', 'being', 'white', 'and', 'male', 'popularizing', 'feminism', 'lesbianism', 'and', 'forwarding', 'the', 'end', 'of', 'times', 'how', 'suddenly', 'shitty', 'their', 'lives', 'must', 'be', 'speakfreely', 'maga', 'gab']\n",
            "Text after removing stop words: ['big', 'smile', 'face', 'think', 'college', 'snowflakes', 'close', 'banning', 'free', 'speech', 'persecuting', 'white', 'males', 'white', 'male', 'popularizing', 'feminism', 'lesbianism', 'forwarding', 'end', 'times', 'suddenly', 'shitty', 'lives', 'must', 'speakfreely', 'maga', 'gab']\n",
            "Text after Lemmatization: ['big', 'smile', 'face', 'think', 'college', 'snowflake', 'close', 'banning', 'free', 'speech', 'persecuting', 'white', 'male', 'white', 'male', 'popularizing', 'feminism', 'lesbianism', 'forwarding', 'end', 'time', 'suddenly', 'shitty', 'life', 'must', 'speakfreely', 'maga', 'gab']\n",
            "Final pre-processed text: big smile face think college snowflake close banning free speech persecuting white male white male popularizing feminism lesbianism forwarding end time suddenly shitty life must speakfreely maga gab\n",
            "Text after removing HTML tags: were you saying something i took out the trash you are next tomorrow nazi faggot nite lol\n",
            "Text after removing non-alphabetic characters and converting to lowercase: were you saying something i took out the trash you are next tomorrow nazi faggot nite lol\n",
            "Text after tokenization: ['were', 'you', 'saying', 'something', 'i', 'took', 'out', 'the', 'trash', 'you', 'are', 'next', 'tomorrow', 'nazi', 'faggot', 'nite', 'lol']\n",
            "Text after removing stop words: ['saying', 'something', 'took', 'trash', 'next', 'tomorrow', 'nazi', 'faggot', 'nite', 'lol']\n",
            "Text after Lemmatization: ['saying', 'something', 'took', 'trash', 'next', 'tomorrow', 'nazi', 'faggot', 'nite', 'lol']\n",
            "Final pre-processed text: saying something took trash next tomorrow nazi faggot nite lol\n",
            "Text after removing HTML tags: i am the reaction to the shitskin invasion\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i am the reaction to the shitskin invasion\n",
            "Text after tokenization: ['i', 'am', 'the', 'reaction', 'to', 'the', 'shitskin', 'invasion']\n",
            "Text after removing stop words: ['reaction', 'shitskin', 'invasion']\n",
            "Text after Lemmatization: ['reaction', 'shitskin', 'invasion']\n",
            "Final pre-processed text: reaction shitskin invasion\n",
            "Text after removing HTML tags: german jewish girl  found raped and murdered by this muslim savage an extradition only when a jew victim is involved but none for the goys\n",
            "Text after removing non-alphabetic characters and converting to lowercase: german jewish girl  found raped and murdered by this muslim savage an extradition only when a jew victim is involved but none for the goys\n",
            "Text after tokenization: ['german', 'jewish', 'girl', 'found', 'raped', 'and', 'murdered', 'by', 'this', 'muslim', 'savage', 'an', 'extradition', 'only', 'when', 'a', 'jew', 'victim', 'is', 'involved', 'but', 'none', 'for', 'the', 'goys']\n",
            "Text after removing stop words: ['german', 'jewish', 'girl', 'found', 'raped', 'murdered', 'muslim', 'savage', 'extradition', 'jew', 'victim', 'involved', 'none', 'goys']\n",
            "Text after Lemmatization: ['german', 'jewish', 'girl', 'found', 'raped', 'murdered', 'muslim', 'savage', 'extradition', 'jew', 'victim', 'involved', 'none', 'goy']\n",
            "Final pre-processed text: german jewish girl found raped murdered muslim savage extradition jew victim involved none goy\n",
            "Text after removing HTML tags:  the people have an average iq of moron these people have an average iq higher than that of the average negro by only  points\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  the people have an average iq of moron these people have an average iq higher than that of the average negro by only  points\n",
            "Text after tokenization: ['the', 'people', 'have', 'an', 'average', 'iq', 'of', 'moron', 'these', 'people', 'have', 'an', 'average', 'iq', 'higher', 'than', 'that', 'of', 'the', 'average', 'negro', 'by', 'only', 'points']\n",
            "Text after removing stop words: ['people', 'average', 'iq', 'moron', 'people', 'average', 'iq', 'higher', 'average', 'negro', 'points']\n",
            "Text after Lemmatization: ['people', 'average', 'iq', 'moron', 'people', 'average', 'iq', 'higher', 'average', 'negro', 'point']\n",
            "Final pre-processed text: people average iq moron people average iq higher average negro point\n",
            "Text after removing HTML tags: according to you what do you do to prove youre not a pussy name one thing you have done that is not pozzed in fact kike loving faggot\n",
            "Text after removing non-alphabetic characters and converting to lowercase: according to you what do you do to prove youre not a pussy name one thing you have done that is not pozzed in fact kike loving faggot\n",
            "Text after tokenization: ['according', 'to', 'you', 'what', 'do', 'you', 'do', 'to', 'prove', 'youre', 'not', 'a', 'pussy', 'name', 'one', 'thing', 'you', 'have', 'done', 'that', 'is', 'not', 'pozzed', 'in', 'fact', 'kike', 'loving', 'faggot']\n",
            "Text after removing stop words: ['according', 'prove', 'youre', 'pussy', 'name', 'one', 'thing', 'done', 'pozzed', 'fact', 'kike', 'loving', 'faggot']\n",
            "Text after Lemmatization: ['according', 'prove', 'youre', 'pussy', 'name', 'one', 'thing', 'done', 'pozzed', 'fact', 'kike', 'loving', 'faggot']\n",
            "Final pre-processed text: according prove youre pussy name one thing done pozzed fact kike loving faggot\n",
            "Text after removing HTML tags: if you are looking to make a lot of money and set your family up for life then appeal yourself to the illegal immigrant vote\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if you are looking to make a lot of money and set your family up for life then appeal yourself to the illegal immigrant vote\n",
            "Text after tokenization: ['if', 'you', 'are', 'looking', 'to', 'make', 'a', 'lot', 'of', 'money', 'and', 'set', 'your', 'family', 'up', 'for', 'life', 'then', 'appeal', 'yourself', 'to', 'the', 'illegal', 'immigrant', 'vote']\n",
            "Text after removing stop words: ['looking', 'make', 'lot', 'money', 'set', 'family', 'life', 'appeal', 'illegal', 'immigrant', 'vote']\n",
            "Text after Lemmatization: ['looking', 'make', 'lot', 'money', 'set', 'family', 'life', 'appeal', 'illegal', 'immigrant', 'vote']\n",
            "Final pre-processed text: looking make lot money set family life appeal illegal immigrant vote\n",
            "Text after removing HTML tags: uh huh which is why when christians matching that supposed description controlled an absolute and overwhelming majority in this country and the whole west tolerance and communism and feminism and gay rights and nigger victim intervention civil rights spread like an unopposed cancer through the minds of its christian population\n",
            "Text after removing non-alphabetic characters and converting to lowercase: uh huh which is why when christians matching that supposed description controlled an absolute and overwhelming majority in this country and the whole west tolerance and communism and feminism and gay rights and nigger victim intervention civil rights spread like an unopposed cancer through the minds of its christian population\n",
            "Text after tokenization: ['uh', 'huh', 'which', 'is', 'why', 'when', 'christians', 'matching', 'that', 'supposed', 'description', 'controlled', 'an', 'absolute', 'and', 'overwhelming', 'majority', 'in', 'this', 'country', 'and', 'the', 'whole', 'west', 'tolerance', 'and', 'communism', 'and', 'feminism', 'and', 'gay', 'rights', 'and', 'nigger', 'victim', 'intervention', 'civil', 'rights', 'spread', 'like', 'an', 'unopposed', 'cancer', 'through', 'the', 'minds', 'of', 'its', 'christian', 'population']\n",
            "Text after removing stop words: ['uh', 'huh', 'christians', 'matching', 'supposed', 'description', 'controlled', 'absolute', 'overwhelming', 'majority', 'country', 'whole', 'west', 'tolerance', 'communism', 'feminism', 'gay', 'rights', 'nigger', 'victim', 'intervention', 'civil', 'rights', 'spread', 'like', 'unopposed', 'cancer', 'minds', 'christian', 'population']\n",
            "Text after Lemmatization: ['uh', 'huh', 'christian', 'matching', 'supposed', 'description', 'controlled', 'absolute', 'overwhelming', 'majority', 'country', 'whole', 'west', 'tolerance', 'communism', 'feminism', 'gay', 'right', 'nigger', 'victim', 'intervention', 'civil', 'right', 'spread', 'like', 'unopposed', 'cancer', 'mind', 'christian', 'population']\n",
            "Final pre-processed text: uh huh christian matching supposed description controlled absolute overwhelming majority country whole west tolerance communism feminism gay right nigger victim intervention civil right spread like unopposed cancer mind christian population\n",
            "Text after removing HTML tags: why go to a movie to watch women steal just spend a couple of hours in family court system\n",
            "Text after removing non-alphabetic characters and converting to lowercase: why go to a movie to watch women steal just spend a couple of hours in family court system\n",
            "Text after tokenization: ['why', 'go', 'to', 'a', 'movie', 'to', 'watch', 'women', 'steal', 'just', 'spend', 'a', 'couple', 'of', 'hours', 'in', 'family', 'court', 'system']\n",
            "Text after removing stop words: ['go', 'movie', 'watch', 'women', 'steal', 'spend', 'couple', 'hours', 'family', 'court', 'system']\n",
            "Text after Lemmatization: ['go', 'movie', 'watch', 'woman', 'steal', 'spend', 'couple', 'hour', 'family', 'court', 'system']\n",
            "Final pre-processed text: go movie watch woman steal spend couple hour family court system\n",
            "Text after removing HTML tags: muzzies should be kicked out of the usa these barbarian goathumpers and wifebeaters add nothing to american society send them back to the shithole they came here from they are her for the free stuff and have on intention of assimilating\n",
            "Text after removing non-alphabetic characters and converting to lowercase: muzzies should be kicked out of the usa these barbarian goathumpers and wifebeaters add nothing to american society send them back to the shithole they came here from they are her for the free stuff and have on intention of assimilating\n",
            "Text after tokenization: ['muzzies', 'should', 'be', 'kicked', 'out', 'of', 'the', 'usa', 'these', 'barbarian', 'goathumpers', 'and', 'wifebeaters', 'add', 'nothing', 'to', 'american', 'society', 'send', 'them', 'back', 'to', 'the', 'shithole', 'they', 'came', 'here', 'from', 'they', 'are', 'her', 'for', 'the', 'free', 'stuff', 'and', 'have', 'on', 'intention', 'of', 'assimilating']\n",
            "Text after removing stop words: ['muzzies', 'kicked', 'usa', 'barbarian', 'goathumpers', 'wifebeaters', 'add', 'nothing', 'american', 'society', 'send', 'back', 'shithole', 'came', 'free', 'stuff', 'intention', 'assimilating']\n",
            "Text after Lemmatization: ['muzzies', 'kicked', 'usa', 'barbarian', 'goathumpers', 'wifebeaters', 'add', 'nothing', 'american', 'society', 'send', 'back', 'shithole', 'came', 'free', 'stuff', 'intention', 'assimilating']\n",
            "Final pre-processed text: muzzies kicked usa barbarian goathumpers wifebeaters add nothing american society send back shithole came free stuff intention assimilating\n",
            "Text after removing HTML tags: the niggers love the smell of grandma depends\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the niggers love the smell of grandma depends\n",
            "Text after tokenization: ['the', 'niggers', 'love', 'the', 'smell', 'of', 'grandma', 'depends']\n",
            "Text after removing stop words: ['niggers', 'love', 'smell', 'grandma', 'depends']\n",
            "Text after Lemmatization: ['nigger', 'love', 'smell', 'grandma', 'depends']\n",
            "Final pre-processed text: nigger love smell grandma depends\n",
            "Text after removing HTML tags: i hope the sand niggers shank this shitlib prick to death in his cell tbh fam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i hope the sand niggers shank this shitlib prick to death in his cell tbh fam\n",
            "Text after tokenization: ['i', 'hope', 'the', 'sand', 'niggers', 'shank', 'this', 'shitlib', 'prick', 'to', 'death', 'in', 'his', 'cell', 'tbh', 'fam']\n",
            "Text after removing stop words: ['hope', 'sand', 'niggers', 'shank', 'shitlib', 'prick', 'death', 'cell', 'tbh', 'fam']\n",
            "Text after Lemmatization: ['hope', 'sand', 'nigger', 'shank', 'shitlib', 'prick', 'death', 'cell', 'tbh', 'fam']\n",
            "Final pre-processed text: hope sand nigger shank shitlib prick death cell tbh fam\n",
            "Text after removing HTML tags: yes it is not all about jews lots of heads on the hydra but were there gassings at awwshitz does america get dragged into serving israel\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yes it is not all about jews lots of heads on the hydra but were there gassings at awwshitz does america get dragged into serving israel\n",
            "Text after tokenization: ['yes', 'it', 'is', 'not', 'all', 'about', 'jews', 'lots', 'of', 'heads', 'on', 'the', 'hydra', 'but', 'were', 'there', 'gassings', 'at', 'awwshitz', 'does', 'america', 'get', 'dragged', 'into', 'serving', 'israel']\n",
            "Text after removing stop words: ['yes', 'jews', 'lots', 'heads', 'hydra', 'gassings', 'awwshitz', 'america', 'get', 'dragged', 'serving', 'israel']\n",
            "Text after Lemmatization: ['yes', 'jew', 'lot', 'head', 'hydra', 'gassing', 'awwshitz', 'america', 'get', 'dragged', 'serving', 'israel']\n",
            "Final pre-processed text: yes jew lot head hydra gassing awwshitz america get dragged serving israel\n",
            "Text after removing HTML tags: the white middle class college educated useful idiots of the left fail to realize that the leftist establishment only cares about immigration for votes and cheap labor they will not understand this until they are laying in a ditch raped by some spic bleeding to death\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the white middle class college educated useful idiots of the left fail to realize that the leftist establishment only cares about immigration for votes and cheap labor they will not understand this until they are laying in a ditch raped by some spic bleeding to death\n",
            "Text after tokenization: ['the', 'white', 'middle', 'class', 'college', 'educated', 'useful', 'idiots', 'of', 'the', 'left', 'fail', 'to', 'realize', 'that', 'the', 'leftist', 'establishment', 'only', 'cares', 'about', 'immigration', 'for', 'votes', 'and', 'cheap', 'labor', 'they', 'will', 'not', 'understand', 'this', 'until', 'they', 'are', 'laying', 'in', 'a', 'ditch', 'raped', 'by', 'some', 'spic', 'bleeding', 'to', 'death']\n",
            "Text after removing stop words: ['white', 'middle', 'class', 'college', 'educated', 'useful', 'idiots', 'left', 'fail', 'realize', 'leftist', 'establishment', 'cares', 'immigration', 'votes', 'cheap', 'labor', 'understand', 'laying', 'ditch', 'raped', 'spic', 'bleeding', 'death']\n",
            "Text after Lemmatization: ['white', 'middle', 'class', 'college', 'educated', 'useful', 'idiot', 'left', 'fail', 'realize', 'leftist', 'establishment', 'care', 'immigration', 'vote', 'cheap', 'labor', 'understand', 'laying', 'ditch', 'raped', 'spic', 'bleeding', 'death']\n",
            "Final pre-processed text: white middle class college educated useful idiot left fail realize leftist establishment care immigration vote cheap labor understand laying ditch raped spic bleeding death\n",
            "Text after removing HTML tags: lebron james takes a shot at trump he posed for picture on cover of magazine in white suit with safety pin but now we can say he looks like giant baby in diaper trump maga speakfreely\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lebron james takes a shot at trump he posed for picture on cover of magazine in white suit with safety pin but now we can say he looks like giant baby in diaper trump maga speakfreely\n",
            "Text after tokenization: ['lebron', 'james', 'takes', 'a', 'shot', 'at', 'trump', 'he', 'posed', 'for', 'picture', 'on', 'cover', 'of', 'magazine', 'in', 'white', 'suit', 'with', 'safety', 'pin', 'but', 'now', 'we', 'can', 'say', 'he', 'looks', 'like', 'giant', 'baby', 'in', 'diaper', 'trump', 'maga', 'speakfreely']\n",
            "Text after removing stop words: ['lebron', 'james', 'takes', 'shot', 'trump', 'posed', 'picture', 'cover', 'magazine', 'white', 'suit', 'safety', 'pin', 'say', 'looks', 'like', 'giant', 'baby', 'diaper', 'trump', 'maga', 'speakfreely']\n",
            "Text after Lemmatization: ['lebron', 'james', 'take', 'shot', 'trump', 'posed', 'picture', 'cover', 'magazine', 'white', 'suit', 'safety', 'pin', 'say', 'look', 'like', 'giant', 'baby', 'diaper', 'trump', 'maga', 'speakfreely']\n",
            "Final pre-processed text: lebron james take shot trump posed picture cover magazine white suit safety pin say look like giant baby diaper trump maga speakfreely\n",
            "Text after removing HTML tags:    but they are not coming from a place of constructive criticism they are complaining about how boys have pockets they do not which is absolutely retarded i actually design clothing for a living so i will tell you why women pants pockets are so shitty  women just do not\n",
            "Text after removing non-alphabetic characters and converting to lowercase:    but they are not coming from a place of constructive criticism they are complaining about how boys have pockets they do not which is absolutely retarded i actually design clothing for a living so i will tell you why women pants pockets are so shitty  women just do not\n",
            "Text after tokenization: ['but', 'they', 'are', 'not', 'coming', 'from', 'a', 'place', 'of', 'constructive', 'criticism', 'they', 'are', 'complaining', 'about', 'how', 'boys', 'have', 'pockets', 'they', 'do', 'not', 'which', 'is', 'absolutely', 'retarded', 'i', 'actually', 'design', 'clothing', 'for', 'a', 'living', 'so', 'i', 'will', 'tell', 'you', 'why', 'women', 'pants', 'pockets', 'are', 'so', 'shitty', 'women', 'just', 'do', 'not']\n",
            "Text after removing stop words: ['coming', 'place', 'constructive', 'criticism', 'complaining', 'boys', 'pockets', 'absolutely', 'retarded', 'actually', 'design', 'clothing', 'living', 'tell', 'women', 'pants', 'pockets', 'shitty', 'women']\n",
            "Text after Lemmatization: ['coming', 'place', 'constructive', 'criticism', 'complaining', 'boy', 'pocket', 'absolutely', 'retarded', 'actually', 'design', 'clothing', 'living', 'tell', 'woman', 'pant', 'pocket', 'shitty', 'woman']\n",
            "Final pre-processed text: coming place constructive criticism complaining boy pocket absolutely retarded actually design clothing living tell woman pant pocket shitty woman\n",
            "Text after removing HTML tags: i should give my passport to a jihadi for safekeeping so it never gets lost or damaged berlinchristmas\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i should give my passport to a jihadi for safekeeping so it never gets lost or damaged berlinchristmas\n",
            "Text after tokenization: ['i', 'should', 'give', 'my', 'passport', 'to', 'a', 'jihadi', 'for', 'safekeeping', 'so', 'it', 'never', 'gets', 'lost', 'or', 'damaged', 'berlinchristmas']\n",
            "Text after removing stop words: ['give', 'passport', 'jihadi', 'safekeeping', 'never', 'gets', 'lost', 'damaged', 'berlinchristmas']\n",
            "Text after Lemmatization: ['give', 'passport', 'jihadi', 'safekeeping', 'never', 'get', 'lost', 'damaged', 'berlinchristmas']\n",
            "Final pre-processed text: give passport jihadi safekeeping never get lost damaged berlinchristmas\n",
            "Text after removing HTML tags:  exactly right every time i hear these fools saying radical islam i want to scream no you imbecile it is islam period these are faithful muslims that are becoming terrorists damn the world better wake up fast\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  exactly right every time i hear these fools saying radical islam i want to scream no you imbecile it is islam period these are faithful muslims that are becoming terrorists damn the world better wake up fast\n",
            "Text after tokenization: ['exactly', 'right', 'every', 'time', 'i', 'hear', 'these', 'fools', 'saying', 'radical', 'islam', 'i', 'want', 'to', 'scream', 'no', 'you', 'imbecile', 'it', 'is', 'islam', 'period', 'these', 'are', 'faithful', 'muslims', 'that', 'are', 'becoming', 'terrorists', 'damn', 'the', 'world', 'better', 'wake', 'up', 'fast']\n",
            "Text after removing stop words: ['exactly', 'right', 'every', 'time', 'hear', 'fools', 'saying', 'radical', 'islam', 'want', 'scream', 'imbecile', 'islam', 'period', 'faithful', 'muslims', 'becoming', 'terrorists', 'damn', 'world', 'better', 'wake', 'fast']\n",
            "Text after Lemmatization: ['exactly', 'right', 'every', 'time', 'hear', 'fool', 'saying', 'radical', 'islam', 'want', 'scream', 'imbecile', 'islam', 'period', 'faithful', 'muslim', 'becoming', 'terrorist', 'damn', 'world', 'better', 'wake', 'fast']\n",
            "Final pre-processed text: exactly right every time hear fool saying radical islam want scream imbecile islam period faithful muslim becoming terrorist damn world better wake fast\n",
            "Text after removing HTML tags: will george soros continue to fund organizations that intend to stop free speech will spones quickly lose all their credibility after a recent exposure of them was released will the flood of refugees get proper and extreme vetting find out on the next episode of super generation z\n",
            "Text after removing non-alphabetic characters and converting to lowercase: will george soros continue to fund organizations that intend to stop free speech will spones quickly lose all their credibility after a recent exposure of them was released will the flood of refugees get proper and extreme vetting find out on the next episode of super generation z\n",
            "Text after tokenization: ['will', 'george', 'soros', 'continue', 'to', 'fund', 'organizations', 'that', 'intend', 'to', 'stop', 'free', 'speech', 'will', 'spones', 'quickly', 'lose', 'all', 'their', 'credibility', 'after', 'a', 'recent', 'exposure', 'of', 'them', 'was', 'released', 'will', 'the', 'flood', 'of', 'refugees', 'get', 'proper', 'and', 'extreme', 'vetting', 'find', 'out', 'on', 'the', 'next', 'episode', 'of', 'super', 'generation', 'z']\n",
            "Text after removing stop words: ['george', 'soros', 'continue', 'fund', 'organizations', 'intend', 'stop', 'free', 'speech', 'spones', 'quickly', 'lose', 'credibility', 'recent', 'exposure', 'released', 'flood', 'refugees', 'get', 'proper', 'extreme', 'vetting', 'find', 'next', 'episode', 'super', 'generation', 'z']\n",
            "Text after Lemmatization: ['george', 'soros', 'continue', 'fund', 'organization', 'intend', 'stop', 'free', 'speech', 'spones', 'quickly', 'lose', 'credibility', 'recent', 'exposure', 'released', 'flood', 'refugee', 'get', 'proper', 'extreme', 'vetting', 'find', 'next', 'episode', 'super', 'generation', 'z']\n",
            "Final pre-processed text: george soros continue fund organization intend stop free speech spones quickly lose credibility recent exposure released flood refugee get proper extreme vetting find next episode super generation z\n",
            "Text after removing HTML tags: fox news ‏  · tw  there are  refugees who are going to be settled in the united states this year alone thefive pic twitter com lgggawica8\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fox news      tw  there are  refugees who are going to be settled in the united states this year alone thefive pic twitter com lgggawica \n",
            "Text after tokenization: ['fox', 'news', 'tw', 'there', 'are', 'refugees', 'who', 'are', 'going', 'to', 'be', 'settled', 'in', 'the', 'united', 'states', 'this', 'year', 'alone', 'thefive', 'pic', 'twitter', 'com', 'lgggawica']\n",
            "Text after removing stop words: ['fox', 'news', 'tw', 'refugees', 'going', 'settled', 'united', 'states', 'year', 'alone', 'thefive', 'pic', 'twitter', 'com', 'lgggawica']\n",
            "Text after Lemmatization: ['fox', 'news', 'tw', 'refugee', 'going', 'settled', 'united', 'state', 'year', 'alone', 'thefive', 'pic', 'twitter', 'com', 'lgggawica']\n",
            "Final pre-processed text: fox news tw refugee going settled united state year alone thefive pic twitter com lgggawica\n",
            "Text after removing HTML tags: ·  downvote me faggot or i will put mmbop live nobody wants that\n",
            "Text after removing non-alphabetic characters and converting to lowercase:    downvote me faggot or i will put mmbop live nobody wants that\n",
            "Text after tokenization: ['downvote', 'me', 'faggot', 'or', 'i', 'will', 'put', 'mmbop', 'live', 'nobody', 'wants', 'that']\n",
            "Text after removing stop words: ['downvote', 'faggot', 'put', 'mmbop', 'live', 'nobody', 'wants']\n",
            "Text after Lemmatization: ['downvote', 'faggot', 'put', 'mmbop', 'live', 'nobody', 'want']\n",
            "Final pre-processed text: downvote faggot put mmbop live nobody want\n",
            "Text after removing HTML tags:  because i criticize the role jews are playing in society this is literally some of the most cucked stuff i have heard are you also critical of asians for their work ethic or mediterraneans for their fiery tempers such an immature grouping\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  because i criticize the role jews are playing in society this is literally some of the most cucked stuff i have heard are you also critical of asians for their work ethic or mediterraneans for their fiery tempers such an immature grouping\n",
            "Text after tokenization: ['because', 'i', 'criticize', 'the', 'role', 'jews', 'are', 'playing', 'in', 'society', 'this', 'is', 'literally', 'some', 'of', 'the', 'most', 'cucked', 'stuff', 'i', 'have', 'heard', 'are', 'you', 'also', 'critical', 'of', 'asians', 'for', 'their', 'work', 'ethic', 'or', 'mediterraneans', 'for', 'their', 'fiery', 'tempers', 'such', 'an', 'immature', 'grouping']\n",
            "Text after removing stop words: ['criticize', 'role', 'jews', 'playing', 'society', 'literally', 'cucked', 'stuff', 'heard', 'also', 'critical', 'asians', 'work', 'ethic', 'mediterraneans', 'fiery', 'tempers', 'immature', 'grouping']\n",
            "Text after Lemmatization: ['criticize', 'role', 'jew', 'playing', 'society', 'literally', 'cucked', 'stuff', 'heard', 'also', 'critical', 'asian', 'work', 'ethic', 'mediterranean', 'fiery', 'temper', 'immature', 'grouping']\n",
            "Final pre-processed text: criticize role jew playing society literally cucked stuff heard also critical asian work ethic mediterranean fiery temper immature grouping\n",
            "Text after removing HTML tags:  i thought women do not fart\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  i thought women do not fart\n",
            "Text after tokenization: ['i', 'thought', 'women', 'do', 'not', 'fart']\n",
            "Text after removing stop words: ['thought', 'women', 'fart']\n",
            "Text after Lemmatization: ['thought', 'woman', 'fart']\n",
            "Final pre-processed text: thought woman fart\n",
            "Text after removing HTML tags: in the last week alone the obama administration blocked future oil and gas leases in swaths of the arctic and atlantic oceans granted a record number of pardons and commutations for a single day and scrapped a dormant registry for male immigrants from a list of largely muslim countries\n",
            "Text after removing non-alphabetic characters and converting to lowercase: in the last week alone the obama administration blocked future oil and gas leases in swaths of the arctic and atlantic oceans granted a record number of pardons and commutations for a single day and scrapped a dormant registry for male immigrants from a list of largely muslim countries\n",
            "Text after tokenization: ['in', 'the', 'last', 'week', 'alone', 'the', 'obama', 'administration', 'blocked', 'future', 'oil', 'and', 'gas', 'leases', 'in', 'swaths', 'of', 'the', 'arctic', 'and', 'atlantic', 'oceans', 'granted', 'a', 'record', 'number', 'of', 'pardons', 'and', 'commutations', 'for', 'a', 'single', 'day', 'and', 'scrapped', 'a', 'dormant', 'registry', 'for', 'male', 'immigrants', 'from', 'a', 'list', 'of', 'largely', 'muslim', 'countries']\n",
            "Text after removing stop words: ['last', 'week', 'alone', 'obama', 'administration', 'blocked', 'future', 'oil', 'gas', 'leases', 'swaths', 'arctic', 'atlantic', 'oceans', 'granted', 'record', 'number', 'pardons', 'commutations', 'single', 'day', 'scrapped', 'dormant', 'registry', 'male', 'immigrants', 'list', 'largely', 'muslim', 'countries']\n",
            "Text after Lemmatization: ['last', 'week', 'alone', 'obama', 'administration', 'blocked', 'future', 'oil', 'gas', 'lease', 'swath', 'arctic', 'atlantic', 'ocean', 'granted', 'record', 'number', 'pardon', 'commutation', 'single', 'day', 'scrapped', 'dormant', 'registry', 'male', 'immigrant', 'list', 'largely', 'muslim', 'country']\n",
            "Final pre-processed text: last week alone obama administration blocked future oil gas lease swath arctic atlantic ocean granted record number pardon commutation single day scrapped dormant registry male immigrant list largely muslim country\n",
            "Text after removing HTML tags: you can call anybody a nazi and not get sued you can call anybody a racist and not get sued we need more un sueable anti prog epithets\n",
            "Text after removing non-alphabetic characters and converting to lowercase: you can call anybody a nazi and not get sued you can call anybody a racist and not get sued we need more un sueable anti prog epithets\n",
            "Text after tokenization: ['you', 'can', 'call', 'anybody', 'a', 'nazi', 'and', 'not', 'get', 'sued', 'you', 'can', 'call', 'anybody', 'a', 'racist', 'and', 'not', 'get', 'sued', 'we', 'need', 'more', 'un', 'sueable', 'anti', 'prog', 'epithets']\n",
            "Text after removing stop words: ['call', 'anybody', 'nazi', 'get', 'sued', 'call', 'anybody', 'racist', 'get', 'sued', 'need', 'un', 'sueable', 'anti', 'prog', 'epithets']\n",
            "Text after Lemmatization: ['call', 'anybody', 'nazi', 'get', 'sued', 'call', 'anybody', 'racist', 'get', 'sued', 'need', 'un', 'sueable', 'anti', 'prog', 'epithet']\n",
            "Final pre-processed text: call anybody nazi get sued call anybody racist get sued need un sueable anti prog epithet\n",
            "Text after removing HTML tags: elected while promising to keep train fares at same level gor two years london mayor khan has done the usual muslim trick of lying as fares are up \n",
            "Text after removing non-alphabetic characters and converting to lowercase: elected while promising to keep train fares at same level gor two years london mayor khan has done the usual muslim trick of lying as fares are up \n",
            "Text after tokenization: ['elected', 'while', 'promising', 'to', 'keep', 'train', 'fares', 'at', 'same', 'level', 'gor', 'two', 'years', 'london', 'mayor', 'khan', 'has', 'done', 'the', 'usual', 'muslim', 'trick', 'of', 'lying', 'as', 'fares', 'are', 'up']\n",
            "Text after removing stop words: ['elected', 'promising', 'keep', 'train', 'fares', 'level', 'gor', 'two', 'years', 'london', 'mayor', 'khan', 'done', 'usual', 'muslim', 'trick', 'lying', 'fares']\n",
            "Text after Lemmatization: ['elected', 'promising', 'keep', 'train', 'fare', 'level', 'gor', 'two', 'year', 'london', 'mayor', 'khan', 'done', 'usual', 'muslim', 'trick', 'lying', 'fare']\n",
            "Final pre-processed text: elected promising keep train fare level gor two year london mayor khan done usual muslim trick lying fare\n",
            "Text after removing HTML tags:  all race baiting rabble rouser obama did was foment violence and god is going to punish his vile little muslim $$\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  all race baiting rabble rouser obama did was foment violence and god is going to punish his vile little muslim   \n",
            "Text after tokenization: ['all', 'race', 'baiting', 'rabble', 'rouser', 'obama', 'did', 'was', 'foment', 'violence', 'and', 'god', 'is', 'going', 'to', 'punish', 'his', 'vile', 'little', 'muslim']\n",
            "Text after removing stop words: ['race', 'baiting', 'rabble', 'rouser', 'obama', 'foment', 'violence', 'god', 'going', 'punish', 'vile', 'little', 'muslim']\n",
            "Text after Lemmatization: ['race', 'baiting', 'rabble', 'rouser', 'obama', 'foment', 'violence', 'god', 'going', 'punish', 'vile', 'little', 'muslim']\n",
            "Final pre-processed text: race baiting rabble rouser obama foment violence god going punish vile little muslim\n",
            "Text after removing HTML tags: you are arguing that white people willfully stop black people from succeeding do you even read your own posts why bother is built into the black ghetto culture you have to seek self betterment rather than teach grabbing from others dnc tells the black community they must grab \n",
            "Text after removing non-alphabetic characters and converting to lowercase: you are arguing that white people willfully stop black people from succeeding do you even read your own posts why bother is built into the black ghetto culture you have to seek self betterment rather than teach grabbing from others dnc tells the black community they must grab \n",
            "Text after tokenization: ['you', 'are', 'arguing', 'that', 'white', 'people', 'willfully', 'stop', 'black', 'people', 'from', 'succeeding', 'do', 'you', 'even', 'read', 'your', 'own', 'posts', 'why', 'bother', 'is', 'built', 'into', 'the', 'black', 'ghetto', 'culture', 'you', 'have', 'to', 'seek', 'self', 'betterment', 'rather', 'than', 'teach', 'grabbing', 'from', 'others', 'dnc', 'tells', 'the', 'black', 'community', 'they', 'must', 'grab']\n",
            "Text after removing stop words: ['arguing', 'white', 'people', 'willfully', 'stop', 'black', 'people', 'succeeding', 'even', 'read', 'posts', 'bother', 'built', 'black', 'ghetto', 'culture', 'seek', 'self', 'betterment', 'rather', 'teach', 'grabbing', 'others', 'dnc', 'tells', 'black', 'community', 'must', 'grab']\n",
            "Text after Lemmatization: ['arguing', 'white', 'people', 'willfully', 'stop', 'black', 'people', 'succeeding', 'even', 'read', 'post', 'bother', 'built', 'black', 'ghetto', 'culture', 'seek', 'self', 'betterment', 'rather', 'teach', 'grabbing', 'others', 'dnc', 'tell', 'black', 'community', 'must', 'grab']\n",
            "Final pre-processed text: arguing white people willfully stop black people succeeding even read post bother built black ghetto culture seek self betterment rather teach grabbing others dnc tell black community must grab\n",
            "Text after removing HTML tags:  while jewish involvement is undeniable muslims are the ones who are actually raping and murdering across europe do not get me wrong oven the jews too but my hatred of muslims will always be greater and more personal\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  while jewish involvement is undeniable muslims are the ones who are actually raping and murdering across europe do not get me wrong oven the jews too but my hatred of muslims will always be greater and more personal\n",
            "Text after tokenization: ['while', 'jewish', 'involvement', 'is', 'undeniable', 'muslims', 'are', 'the', 'ones', 'who', 'are', 'actually', 'raping', 'and', 'murdering', 'across', 'europe', 'do', 'not', 'get', 'me', 'wrong', 'oven', 'the', 'jews', 'too', 'but', 'my', 'hatred', 'of', 'muslims', 'will', 'always', 'be', 'greater', 'and', 'more', 'personal']\n",
            "Text after removing stop words: ['jewish', 'involvement', 'undeniable', 'muslims', 'ones', 'actually', 'raping', 'murdering', 'across', 'europe', 'get', 'wrong', 'oven', 'jews', 'hatred', 'muslims', 'always', 'greater', 'personal']\n",
            "Text after Lemmatization: ['jewish', 'involvement', 'undeniable', 'muslim', 'one', 'actually', 'raping', 'murdering', 'across', 'europe', 'get', 'wrong', 'oven', 'jew', 'hatred', 'muslim', 'always', 'greater', 'personal']\n",
            "Final pre-processed text: jewish involvement undeniable muslim one actually raping murdering across europe get wrong oven jew hatred muslim always greater personal\n",
            "Text after removing HTML tags: snoop dogg says any black entertainers who perform at trump inauguration are uncle toms just more prove that blacks must act think talk like niggers or they have their ethnicity revoked by the ghetto trash who refuse to evolve\n",
            "Text after removing non-alphabetic characters and converting to lowercase: snoop dogg says any black entertainers who perform at trump inauguration are uncle toms just more prove that blacks must act think talk like niggers or they have their ethnicity revoked by the ghetto trash who refuse to evolve\n",
            "Text after tokenization: ['snoop', 'dogg', 'says', 'any', 'black', 'entertainers', 'who', 'perform', 'at', 'trump', 'inauguration', 'are', 'uncle', 'toms', 'just', 'more', 'prove', 'that', 'blacks', 'must', 'act', 'think', 'talk', 'like', 'niggers', 'or', 'they', 'have', 'their', 'ethnicity', 'revoked', 'by', 'the', 'ghetto', 'trash', 'who', 'refuse', 'to', 'evolve']\n",
            "Text after removing stop words: ['snoop', 'dogg', 'says', 'black', 'entertainers', 'perform', 'trump', 'inauguration', 'uncle', 'toms', 'prove', 'blacks', 'must', 'act', 'think', 'talk', 'like', 'niggers', 'ethnicity', 'revoked', 'ghetto', 'trash', 'refuse', 'evolve']\n",
            "Text after Lemmatization: ['snoop', 'dogg', 'say', 'black', 'entertainer', 'perform', 'trump', 'inauguration', 'uncle', 'tom', 'prove', 'black', 'must', 'act', 'think', 'talk', 'like', 'nigger', 'ethnicity', 'revoked', 'ghetto', 'trash', 'refuse', 'evolve']\n",
            "Final pre-processed text: snoop dogg say black entertainer perform trump inauguration uncle tom prove black must act think talk like nigger ethnicity revoked ghetto trash refuse evolve\n",
            "Text after removing HTML tags: there are half a million women in washington protesting the result of the election they are there because if you went to any polling place from my area anyway  of voters were male\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there are half a million women in washington protesting the result of the election they are there because if you went to any polling place from my area anyway  of voters were male\n",
            "Text after tokenization: ['there', 'are', 'half', 'a', 'million', 'women', 'in', 'washington', 'protesting', 'the', 'result', 'of', 'the', 'election', 'they', 'are', 'there', 'because', 'if', 'you', 'went', 'to', 'any', 'polling', 'place', 'from', 'my', 'area', 'anyway', 'of', 'voters', 'were', 'male']\n",
            "Text after removing stop words: ['half', 'million', 'women', 'washington', 'protesting', 'result', 'election', 'went', 'polling', 'place', 'area', 'anyway', 'voters', 'male']\n",
            "Text after Lemmatization: ['half', 'million', 'woman', 'washington', 'protesting', 'result', 'election', 'went', 'polling', 'place', 'area', 'anyway', 'voter', 'male']\n",
            "Final pre-processed text: half million woman washington protesting result election went polling place area anyway voter male\n",
            "Text after removing HTML tags:  and non i bet graduated high school do not they know the planned parenthood is a billion dollar industry that uses these women aborted fetuses to sell as body parts these women have no brains at all\n",
            "Text after removing non-alphabetic characters and converting to lowercase:  and non i bet graduated high school do not they know the planned parenthood is a billion dollar industry that uses these women aborted fetuses to sell as body parts these women have no brains at all\n",
            "Text after tokenization: ['and', 'non', 'i', 'bet', 'graduated', 'high', 'school', 'do', 'not', 'they', 'know', 'the', 'planned', 'parenthood', 'is', 'a', 'billion', 'dollar', 'industry', 'that', 'uses', 'these', 'women', 'aborted', 'fetuses', 'to', 'sell', 'as', 'body', 'parts', 'these', 'women', 'have', 'no', 'brains', 'at', 'all']\n",
            "Text after removing stop words: ['non', 'bet', 'graduated', 'high', 'school', 'know', 'planned', 'parenthood', 'billion', 'dollar', 'industry', 'uses', 'women', 'aborted', 'fetuses', 'sell', 'body', 'parts', 'women', 'brains']\n",
            "Text after Lemmatization: ['non', 'bet', 'graduated', 'high', 'school', 'know', 'planned', 'parenthood', 'billion', 'dollar', 'industry', 'us', 'woman', 'aborted', 'fetus', 'sell', 'body', 'part', 'woman', 'brain']\n",
            "Final pre-processed text: non bet graduated high school know planned parenthood billion dollar industry us woman aborted fetus sell body part woman brain\n",
            "Text after removing HTML tags: journalists are showing themselves to be the retarded liberals they are the tradition of ap can die the traditional press corps can die in gas station bathrooms\n",
            "Text after removing non-alphabetic characters and converting to lowercase: journalists are showing themselves to be the retarded liberals they are the tradition of ap can die the traditional press corps can die in gas station bathrooms\n",
            "Text after tokenization: ['journalists', 'are', 'showing', 'themselves', 'to', 'be', 'the', 'retarded', 'liberals', 'they', 'are', 'the', 'tradition', 'of', 'ap', 'can', 'die', 'the', 'traditional', 'press', 'corps', 'can', 'die', 'in', 'gas', 'station', 'bathrooms']\n",
            "Text after removing stop words: ['journalists', 'showing', 'retarded', 'liberals', 'tradition', 'ap', 'die', 'traditional', 'press', 'corps', 'die', 'gas', 'station', 'bathrooms']\n",
            "Text after Lemmatization: ['journalist', 'showing', 'retarded', 'liberal', 'tradition', 'ap', 'die', 'traditional', 'press', 'corp', 'die', 'gas', 'station', 'bathroom']\n",
            "Final pre-processed text: journalist showing retarded liberal tradition ap die traditional press corp die gas station bathroom\n",
            "Text after removing HTML tags: we have a green carded mexican immigrant that works for us he sitting in the break room on his lunch break with his phone to his ear listening to a trump speech maga my friends\n",
            "Text after removing non-alphabetic characters and converting to lowercase: we have a green carded mexican immigrant that works for us he sitting in the break room on his lunch break with his phone to his ear listening to a trump speech maga my friends\n",
            "Text after tokenization: ['we', 'have', 'a', 'green', 'carded', 'mexican', 'immigrant', 'that', 'works', 'for', 'us', 'he', 'sitting', 'in', 'the', 'break', 'room', 'on', 'his', 'lunch', 'break', 'with', 'his', 'phone', 'to', 'his', 'ear', 'listening', 'to', 'a', 'trump', 'speech', 'maga', 'my', 'friends']\n",
            "Text after removing stop words: ['green', 'carded', 'mexican', 'immigrant', 'works', 'us', 'sitting', 'break', 'room', 'lunch', 'break', 'phone', 'ear', 'listening', 'trump', 'speech', 'maga', 'friends']\n",
            "Text after Lemmatization: ['green', 'carded', 'mexican', 'immigrant', 'work', 'u', 'sitting', 'break', 'room', 'lunch', 'break', 'phone', 'ear', 'listening', 'trump', 'speech', 'maga', 'friend']\n",
            "Final pre-processed text: green carded mexican immigrant work u sitting break room lunch break phone ear listening trump speech maga friend\n",
            "Text after removing HTML tags: anyone see them collecting illegal immigrants on livepd last night my kid is like i feel sorry for their kids but their parents are stupid maga\n",
            "Text after removing non-alphabetic characters and converting to lowercase: anyone see them collecting illegal immigrants on livepd last night my kid is like i feel sorry for their kids but their parents are stupid maga\n",
            "Text after tokenization: ['anyone', 'see', 'them', 'collecting', 'illegal', 'immigrants', 'on', 'livepd', 'last', 'night', 'my', 'kid', 'is', 'like', 'i', 'feel', 'sorry', 'for', 'their', 'kids', 'but', 'their', 'parents', 'are', 'stupid', 'maga']\n",
            "Text after removing stop words: ['anyone', 'see', 'collecting', 'illegal', 'immigrants', 'livepd', 'last', 'night', 'kid', 'like', 'feel', 'sorry', 'kids', 'parents', 'stupid', 'maga']\n",
            "Text after Lemmatization: ['anyone', 'see', 'collecting', 'illegal', 'immigrant', 'livepd', 'last', 'night', 'kid', 'like', 'feel', 'sorry', 'kid', 'parent', 'stupid', 'maga']\n",
            "Final pre-processed text: anyone see collecting illegal immigrant livepd last night kid like feel sorry kid parent stupid maga\n",
            "Text after removing HTML tags: possibly the single best refutation of islam as a religion that i have ever come across\n",
            "Text after removing non-alphabetic characters and converting to lowercase: possibly the single best refutation of islam as a religion that i have ever come across\n",
            "Text after tokenization: ['possibly', 'the', 'single', 'best', 'refutation', 'of', 'islam', 'as', 'a', 'religion', 'that', 'i', 'have', 'ever', 'come', 'across']\n",
            "Text after removing stop words: ['possibly', 'single', 'best', 'refutation', 'islam', 'religion', 'ever', 'come', 'across']\n",
            "Text after Lemmatization: ['possibly', 'single', 'best', 'refutation', 'islam', 'religion', 'ever', 'come', 'across']\n",
            "Final pre-processed text: possibly single best refutation islam religion ever come across\n",
            "Text after removing HTML tags: the islamic and sharia controlled middle eastern countries bitch and complain about the u banning muslims really but they like to do it hmm what that word that sums this up starts with an h islam islamism muslims muslim banislam banmuslims maga gabfam americafirst trump\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the islamic and sharia controlled middle eastern countries bitch and complain about the u banning muslims really but they like to do it hmm what that word that sums this up starts with an h islam islamism muslims muslim banislam banmuslims maga gabfam americafirst trump\n",
            "Text after tokenization: ['the', 'islamic', 'and', 'sharia', 'controlled', 'middle', 'eastern', 'countries', 'bitch', 'and', 'complain', 'about', 'the', 'u', 'banning', 'muslims', 'really', 'but', 'they', 'like', 'to', 'do', 'it', 'hmm', 'what', 'that', 'word', 'that', 'sums', 'this', 'up', 'starts', 'with', 'an', 'h', 'islam', 'islamism', 'muslims', 'muslim', 'banislam', 'banmuslims', 'maga', 'gabfam', 'americafirst', 'trump']\n",
            "Text after removing stop words: ['islamic', 'sharia', 'controlled', 'middle', 'eastern', 'countries', 'bitch', 'complain', 'u', 'banning', 'muslims', 'really', 'like', 'hmm', 'word', 'sums', 'starts', 'h', 'islam', 'islamism', 'muslims', 'muslim', 'banislam', 'banmuslims', 'maga', 'gabfam', 'americafirst', 'trump']\n",
            "Text after Lemmatization: ['islamic', 'sharia', 'controlled', 'middle', 'eastern', 'country', 'bitch', 'complain', 'u', 'banning', 'muslim', 'really', 'like', 'hmm', 'word', 'sum', 'start', 'h', 'islam', 'islamism', 'muslim', 'muslim', 'banislam', 'banmuslims', 'maga', 'gabfam', 'americafirst', 'trump']\n",
            "Final pre-processed text: islamic sharia controlled middle eastern country bitch complain u banning muslim really like hmm word sum start h islam islamism muslim muslim banislam banmuslims maga gabfam americafirst trump\n",
            "Text after removing HTML tags: remember this evening libtards bitch and moan anything good its what evil assholes do and this includes idiots like mccasshole and grahamster\n",
            "Text after removing non-alphabetic characters and converting to lowercase: remember this evening libtards bitch and moan anything good its what evil assholes do and this includes idiots like mccasshole and grahamster\n",
            "Text after tokenization: ['remember', 'this', 'evening', 'libtards', 'bitch', 'and', 'moan', 'anything', 'good', 'its', 'what', 'evil', 'assholes', 'do', 'and', 'this', 'includes', 'idiots', 'like', 'mccasshole', 'and', 'grahamster']\n",
            "Text after removing stop words: ['remember', 'evening', 'libtards', 'bitch', 'moan', 'anything', 'good', 'evil', 'assholes', 'includes', 'idiots', 'like', 'mccasshole', 'grahamster']\n",
            "Text after Lemmatization: ['remember', 'evening', 'libtards', 'bitch', 'moan', 'anything', 'good', 'evil', 'asshole', 'includes', 'idiot', 'like', 'mccasshole', 'grahamster']\n",
            "Final pre-processed text: remember evening libtards bitch moan anything good evil asshole includes idiot like mccasshole grahamster\n",
            "Text after removing HTML tags: someone please tell the mayor of berkeley that calling speech you hate hate speech does not give you the right to circumvent the first amendment let alone justify you abusing your authority and permitting violence against anyone or anyone property miloriots no i am still not over it\n",
            "Text after removing non-alphabetic characters and converting to lowercase: someone please tell the mayor of berkeley that calling speech you hate hate speech does not give you the right to circumvent the first amendment let alone justify you abusing your authority and permitting violence against anyone or anyone property miloriots no i am still not over it\n",
            "Text after tokenization: ['someone', 'please', 'tell', 'the', 'mayor', 'of', 'berkeley', 'that', 'calling', 'speech', 'you', 'hate', 'hate', 'speech', 'does', 'not', 'give', 'you', 'the', 'right', 'to', 'circumvent', 'the', 'first', 'amendment', 'let', 'alone', 'justify', 'you', 'abusing', 'your', 'authority', 'and', 'permitting', 'violence', 'against', 'anyone', 'or', 'anyone', 'property', 'miloriots', 'no', 'i', 'am', 'still', 'not', 'over', 'it']\n",
            "Text after removing stop words: ['someone', 'please', 'tell', 'mayor', 'berkeley', 'calling', 'speech', 'hate', 'hate', 'speech', 'give', 'right', 'circumvent', 'first', 'amendment', 'let', 'alone', 'justify', 'abusing', 'authority', 'permitting', 'violence', 'anyone', 'anyone', 'property', 'miloriots', 'still']\n",
            "Text after Lemmatization: ['someone', 'please', 'tell', 'mayor', 'berkeley', 'calling', 'speech', 'hate', 'hate', 'speech', 'give', 'right', 'circumvent', 'first', 'amendment', 'let', 'alone', 'justify', 'abusing', 'authority', 'permitting', 'violence', 'anyone', 'anyone', 'property', 'miloriots', 'still']\n",
            "Final pre-processed text: someone please tell mayor berkeley calling speech hate hate speech give right circumvent first amendment let alone justify abusing authority permitting violence anyone anyone property miloriots still\n",
            "Text after removing HTML tags: a misogynist is a man who dislikes women as much as they despise each other hl mencken misogyny\n",
            "Text after removing non-alphabetic characters and converting to lowercase: a misogynist is a man who dislikes women as much as they despise each other hl mencken misogyny\n",
            "Text after tokenization: ['a', 'misogynist', 'is', 'a', 'man', 'who', 'dislikes', 'women', 'as', 'much', 'as', 'they', 'despise', 'each', 'other', 'hl', 'mencken', 'misogyny']\n",
            "Text after removing stop words: ['misogynist', 'man', 'dislikes', 'women', 'much', 'despise', 'hl', 'mencken', 'misogyny']\n",
            "Text after Lemmatization: ['misogynist', 'man', 'dislike', 'woman', 'much', 'despise', 'hl', 'mencken', 'misogyny']\n",
            "Final pre-processed text: misogynist man dislike woman much despise hl mencken misogyny\n",
            "Text after removing HTML tags: in  nearly  of blacks were born in wedlock in  nearly  of blacks are born out of wedlock what changed in  years blacks became leftists blackpilled\n",
            "Text after removing non-alphabetic characters and converting to lowercase: in  nearly  of blacks were born in wedlock in  nearly  of blacks are born out of wedlock what changed in  years blacks became leftists blackpilled\n",
            "Text after tokenization: ['in', 'nearly', 'of', 'blacks', 'were', 'born', 'in', 'wedlock', 'in', 'nearly', 'of', 'blacks', 'are', 'born', 'out', 'of', 'wedlock', 'what', 'changed', 'in', 'years', 'blacks', 'became', 'leftists', 'blackpilled']\n",
            "Text after removing stop words: ['nearly', 'blacks', 'born', 'wedlock', 'nearly', 'blacks', 'born', 'wedlock', 'changed', 'years', 'blacks', 'became', 'leftists', 'blackpilled']\n",
            "Text after Lemmatization: ['nearly', 'black', 'born', 'wedlock', 'nearly', 'black', 'born', 'wedlock', 'changed', 'year', 'black', 'became', 'leftist', 'blackpilled']\n",
            "Final pre-processed text: nearly black born wedlock nearly black born wedlock changed year black became leftist blackpilled\n",
            "Text after removing HTML tags: the international scene is a separate issue i am hoping for a more jeffersonian view of trade with all entangling alliances with none the rioting and violence that is certainly coming will be pinned to the immigration issues but really its a culture war needful sadly and inevitable\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the international scene is a separate issue i am hoping for a more jeffersonian view of trade with all entangling alliances with none the rioting and violence that is certainly coming will be pinned to the immigration issues but really its a culture war needful sadly and inevitable\n",
            "Text after tokenization: ['the', 'international', 'scene', 'is', 'a', 'separate', 'issue', 'i', 'am', 'hoping', 'for', 'a', 'more', 'jeffersonian', 'view', 'of', 'trade', 'with', 'all', 'entangling', 'alliances', 'with', 'none', 'the', 'rioting', 'and', 'violence', 'that', 'is', 'certainly', 'coming', 'will', 'be', 'pinned', 'to', 'the', 'immigration', 'issues', 'but', 'really', 'its', 'a', 'culture', 'war', 'needful', 'sadly', 'and', 'inevitable']\n",
            "Text after removing stop words: ['international', 'scene', 'separate', 'issue', 'hoping', 'jeffersonian', 'view', 'trade', 'entangling', 'alliances', 'none', 'rioting', 'violence', 'certainly', 'coming', 'pinned', 'immigration', 'issues', 'really', 'culture', 'war', 'needful', 'sadly', 'inevitable']\n",
            "Text after Lemmatization: ['international', 'scene', 'separate', 'issue', 'hoping', 'jeffersonian', 'view', 'trade', 'entangling', 'alliance', 'none', 'rioting', 'violence', 'certainly', 'coming', 'pinned', 'immigration', 'issue', 'really', 'culture', 'war', 'needful', 'sadly', 'inevitable']\n",
            "Final pre-processed text: international scene separate issue hoping jeffersonian view trade entangling alliance none rioting violence certainly coming pinned immigration issue really culture war needful sadly inevitable\n",
            "Text after removing HTML tags: i do not lose temper when calling niggers niggers other than that do you really think nigger chimpouts in the us achieved anything every gains niggers reached in the us jews gave them niggers were never more than annoying subhumans merely tolerated at the whites mercy\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i do not lose temper when calling niggers niggers other than that do you really think nigger chimpouts in the us achieved anything every gains niggers reached in the us jews gave them niggers were never more than annoying subhumans merely tolerated at the whites mercy\n",
            "Text after tokenization: ['i', 'do', 'not', 'lose', 'temper', 'when', 'calling', 'niggers', 'niggers', 'other', 'than', 'that', 'do', 'you', 'really', 'think', 'nigger', 'chimpouts', 'in', 'the', 'us', 'achieved', 'anything', 'every', 'gains', 'niggers', 'reached', 'in', 'the', 'us', 'jews', 'gave', 'them', 'niggers', 'were', 'never', 'more', 'than', 'annoying', 'subhumans', 'merely', 'tolerated', 'at', 'the', 'whites', 'mercy']\n",
            "Text after removing stop words: ['lose', 'temper', 'calling', 'niggers', 'niggers', 'really', 'think', 'nigger', 'chimpouts', 'us', 'achieved', 'anything', 'every', 'gains', 'niggers', 'reached', 'us', 'jews', 'gave', 'niggers', 'never', 'annoying', 'subhumans', 'merely', 'tolerated', 'whites', 'mercy']\n",
            "Text after Lemmatization: ['lose', 'temper', 'calling', 'nigger', 'nigger', 'really', 'think', 'nigger', 'chimpouts', 'u', 'achieved', 'anything', 'every', 'gain', 'nigger', 'reached', 'u', 'jew', 'gave', 'nigger', 'never', 'annoying', 'subhumans', 'merely', 'tolerated', 'white', 'mercy']\n",
            "Final pre-processed text: lose temper calling nigger nigger really think nigger chimpouts u achieved anything every gain nigger reached u jew gave nigger never annoying subhumans merely tolerated white mercy\n",
            "Text after removing HTML tags: juz had a cravin fo sum ghetto lobstah but coonent waid ta cook it up raht an sheeit\n",
            "Text after removing non-alphabetic characters and converting to lowercase: juz had a cravin fo sum ghetto lobstah but coonent waid ta cook it up raht an sheeit\n",
            "Text after tokenization: ['juz', 'had', 'a', 'cravin', 'fo', 'sum', 'ghetto', 'lobstah', 'but', 'coonent', 'waid', 'ta', 'cook', 'it', 'up', 'raht', 'an', 'sheeit']\n",
            "Text after removing stop words: ['juz', 'cravin', 'fo', 'sum', 'ghetto', 'lobstah', 'coonent', 'waid', 'ta', 'cook', 'raht', 'sheeit']\n",
            "Text after Lemmatization: ['juz', 'cravin', 'fo', 'sum', 'ghetto', 'lobstah', 'coonent', 'waid', 'ta', 'cook', 'raht', 'sheeit']\n",
            "Final pre-processed text: juz cravin fo sum ghetto lobstah coonent waid ta cook raht sheeit\n",
            "Text after removing HTML tags: cultural marxism maintains that all human behaviour is a result of culture not heredity race and thus malleable cultural marxism seeks the takeover of education mainstream christianity the law finance and the annihilation of western civilization in general and white people in particular\n",
            "Text after removing non-alphabetic characters and converting to lowercase: cultural marxism maintains that all human behaviour is a result of culture not heredity race and thus malleable cultural marxism seeks the takeover of education mainstream christianity the law finance and the annihilation of western civilization in general and white people in particular\n",
            "Text after tokenization: ['cultural', 'marxism', 'maintains', 'that', 'all', 'human', 'behaviour', 'is', 'a', 'result', 'of', 'culture', 'not', 'heredity', 'race', 'and', 'thus', 'malleable', 'cultural', 'marxism', 'seeks', 'the', 'takeover', 'of', 'education', 'mainstream', 'christianity', 'the', 'law', 'finance', 'and', 'the', 'annihilation', 'of', 'western', 'civilization', 'in', 'general', 'and', 'white', 'people', 'in', 'particular']\n",
            "Text after removing stop words: ['cultural', 'marxism', 'maintains', 'human', 'behaviour', 'result', 'culture', 'heredity', 'race', 'thus', 'malleable', 'cultural', 'marxism', 'seeks', 'takeover', 'education', 'mainstream', 'christianity', 'law', 'finance', 'annihilation', 'western', 'civilization', 'general', 'white', 'people', 'particular']\n",
            "Text after Lemmatization: ['cultural', 'marxism', 'maintains', 'human', 'behaviour', 'result', 'culture', 'heredity', 'race', 'thus', 'malleable', 'cultural', 'marxism', 'seek', 'takeover', 'education', 'mainstream', 'christianity', 'law', 'finance', 'annihilation', 'western', 'civilization', 'general', 'white', 'people', 'particular']\n",
            "Final pre-processed text: cultural marxism maintains human behaviour result culture heredity race thus malleable cultural marxism seek takeover education mainstream christianity law finance annihilation western civilization general white people particular\n",
            "Text after removing HTML tags: well devildog you and i both know arabs turn and run\n",
            "Text after removing non-alphabetic characters and converting to lowercase: well devildog you and i both know arabs turn and run\n",
            "Text after tokenization: ['well', 'devildog', 'you', 'and', 'i', 'both', 'know', 'arabs', 'turn', 'and', 'run']\n",
            "Text after removing stop words: ['well', 'devildog', 'know', 'arabs', 'turn', 'run']\n",
            "Text after Lemmatization: ['well', 'devildog', 'know', 'arab', 'turn', 'run']\n",
            "Final pre-processed text: well devildog know arab turn run\n",
            "Text after removing HTML tags: obama is not the only one with a private army traitor to 🇹 🇷 gulen muslim billionaire puppet master of obama and erdogan in pennsylvania boasts a small private army also\n",
            "Text after removing non-alphabetic characters and converting to lowercase: obama is not the only one with a private army traitor to     gulen muslim billionaire puppet master of obama and erdogan in pennsylvania boasts a small private army also\n",
            "Text after tokenization: ['obama', 'is', 'not', 'the', 'only', 'one', 'with', 'a', 'private', 'army', 'traitor', 'to', 'gulen', 'muslim', 'billionaire', 'puppet', 'master', 'of', 'obama', 'and', 'erdogan', 'in', 'pennsylvania', 'boasts', 'a', 'small', 'private', 'army', 'also']\n",
            "Text after removing stop words: ['obama', 'one', 'private', 'army', 'traitor', 'gulen', 'muslim', 'billionaire', 'puppet', 'master', 'obama', 'erdogan', 'pennsylvania', 'boasts', 'small', 'private', 'army', 'also']\n",
            "Text after Lemmatization: ['obama', 'one', 'private', 'army', 'traitor', 'gulen', 'muslim', 'billionaire', 'puppet', 'master', 'obama', 'erdogan', 'pennsylvania', 'boast', 'small', 'private', 'army', 'also']\n",
            "Final pre-processed text: obama one private army traitor gulen muslim billionaire puppet master obama erdogan pennsylvania boast small private army also\n",
            "Text after removing HTML tags: yes hitler inself ordered to not kill jews at the kristallnacht everything have a reason we must accept it and look forward\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yes hitler inself ordered to not kill jews at the kristallnacht everything have a reason we must accept it and look forward\n",
            "Text after tokenization: ['yes', 'hitler', 'inself', 'ordered', 'to', 'not', 'kill', 'jews', 'at', 'the', 'kristallnacht', 'everything', 'have', 'a', 'reason', 'we', 'must', 'accept', 'it', 'and', 'look', 'forward']\n",
            "Text after removing stop words: ['yes', 'hitler', 'inself', 'ordered', 'kill', 'jews', 'kristallnacht', 'everything', 'reason', 'must', 'accept', 'look', 'forward']\n",
            "Text after Lemmatization: ['yes', 'hitler', 'inself', 'ordered', 'kill', 'jew', 'kristallnacht', 'everything', 'reason', 'must', 'accept', 'look', 'forward']\n",
            "Final pre-processed text: yes hitler inself ordered kill jew kristallnacht everything reason must accept look forward\n",
            "Text after removing HTML tags: the dalai lama is more informed on the refugee crisis than the pope refugeecrisis europe\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the dalai lama is more informed on the refugee crisis than the pope refugeecrisis europe\n",
            "Text after tokenization: ['the', 'dalai', 'lama', 'is', 'more', 'informed', 'on', 'the', 'refugee', 'crisis', 'than', 'the', 'pope', 'refugeecrisis', 'europe']\n",
            "Text after removing stop words: ['dalai', 'lama', 'informed', 'refugee', 'crisis', 'pope', 'refugeecrisis', 'europe']\n",
            "Text after Lemmatization: ['dalai', 'lama', 'informed', 'refugee', 'crisis', 'pope', 'refugeecrisis', 'europe']\n",
            "Final pre-processed text: dalai lama informed refugee crisis pope refugeecrisis europe\n",
            "Text after removing HTML tags: in sweden immigrants trying to live normal lives are demanding that criminal immigrants be deported they want families of immigrants to be deported it is not clear what that means they think they are under attack from criminals in some of the no go zones and want more police\n",
            "Text after removing non-alphabetic characters and converting to lowercase: in sweden immigrants trying to live normal lives are demanding that criminal immigrants be deported they want families of immigrants to be deported it is not clear what that means they think they are under attack from criminals in some of the no go zones and want more police\n",
            "Text after tokenization: ['in', 'sweden', 'immigrants', 'trying', 'to', 'live', 'normal', 'lives', 'are', 'demanding', 'that', 'criminal', 'immigrants', 'be', 'deported', 'they', 'want', 'families', 'of', 'immigrants', 'to', 'be', 'deported', 'it', 'is', 'not', 'clear', 'what', 'that', 'means', 'they', 'think', 'they', 'are', 'under', 'attack', 'from', 'criminals', 'in', 'some', 'of', 'the', 'no', 'go', 'zones', 'and', 'want', 'more', 'police']\n",
            "Text after removing stop words: ['sweden', 'immigrants', 'trying', 'live', 'normal', 'lives', 'demanding', 'criminal', 'immigrants', 'deported', 'want', 'families', 'immigrants', 'deported', 'clear', 'means', 'think', 'attack', 'criminals', 'go', 'zones', 'want', 'police']\n",
            "Text after Lemmatization: ['sweden', 'immigrant', 'trying', 'live', 'normal', 'life', 'demanding', 'criminal', 'immigrant', 'deported', 'want', 'family', 'immigrant', 'deported', 'clear', 'mean', 'think', 'attack', 'criminal', 'go', 'zone', 'want', 'police']\n",
            "Final pre-processed text: sweden immigrant trying live normal life demanding criminal immigrant deported want family immigrant deported clear mean think attack criminal go zone want police\n",
            "Text after removing HTML tags: white supremacists and nazi larpers should be very concerned with jewish infiltration and agitation\n",
            "Text after removing non-alphabetic characters and converting to lowercase: white supremacists and nazi larpers should be very concerned with jewish infiltration and agitation\n",
            "Text after tokenization: ['white', 'supremacists', 'and', 'nazi', 'larpers', 'should', 'be', 'very', 'concerned', 'with', 'jewish', 'infiltration', 'and', 'agitation']\n",
            "Text after removing stop words: ['white', 'supremacists', 'nazi', 'larpers', 'concerned', 'jewish', 'infiltration', 'agitation']\n",
            "Text after Lemmatization: ['white', 'supremacist', 'nazi', 'larpers', 'concerned', 'jewish', 'infiltration', 'agitation']\n",
            "Final pre-processed text: white supremacist nazi larpers concerned jewish infiltration agitation\n",
            "Text after removing HTML tags: getting better paying jobs for them kk original issue keep the ghetto down and they did not stand when trump talked about better schools shall i go on\n",
            "Text after removing non-alphabetic characters and converting to lowercase: getting better paying jobs for them kk original issue keep the ghetto down and they did not stand when trump talked about better schools shall i go on\n",
            "Text after tokenization: ['getting', 'better', 'paying', 'jobs', 'for', 'them', 'kk', 'original', 'issue', 'keep', 'the', 'ghetto', 'down', 'and', 'they', 'did', 'not', 'stand', 'when', 'trump', 'talked', 'about', 'better', 'schools', 'shall', 'i', 'go', 'on']\n",
            "Text after removing stop words: ['getting', 'better', 'paying', 'jobs', 'kk', 'original', 'issue', 'keep', 'ghetto', 'stand', 'trump', 'talked', 'better', 'schools', 'shall', 'go']\n",
            "Text after Lemmatization: ['getting', 'better', 'paying', 'job', 'kk', 'original', 'issue', 'keep', 'ghetto', 'stand', 'trump', 'talked', 'better', 'school', 'shall', 'go']\n",
            "Final pre-processed text: getting better paying job kk original issue keep ghetto stand trump talked better school shall go\n",
            "Text after removing HTML tags: so here something i have had a hunch about so i have yet to really look into in their plan after white have been all but extinct seeing as they d be the last civilized group left they d go after the asians next\n",
            "Text after removing non-alphabetic characters and converting to lowercase: so here something i have had a hunch about so i have yet to really look into in their plan after white have been all but extinct seeing as they d be the last civilized group left they d go after the asians next\n",
            "Text after tokenization: ['so', 'here', 'something', 'i', 'have', 'had', 'a', 'hunch', 'about', 'so', 'i', 'have', 'yet', 'to', 'really', 'look', 'into', 'in', 'their', 'plan', 'after', 'white', 'have', 'been', 'all', 'but', 'extinct', 'seeing', 'as', 'they', 'd', 'be', 'the', 'last', 'civilized', 'group', 'left', 'they', 'd', 'go', 'after', 'the', 'asians', 'next']\n",
            "Text after removing stop words: ['something', 'hunch', 'yet', 'really', 'look', 'plan', 'white', 'extinct', 'seeing', 'last', 'civilized', 'group', 'left', 'go', 'asians', 'next']\n",
            "Text after Lemmatization: ['something', 'hunch', 'yet', 'really', 'look', 'plan', 'white', 'extinct', 'seeing', 'last', 'civilized', 'group', 'left', 'go', 'asian', 'next']\n",
            "Final pre-processed text: something hunch yet really look plan white extinct seeing last civilized group left go asian next\n",
            "Text after removing HTML tags: horizon zero dawn looks awesome but no way i am having a smug empowered rope haired hippy bitch as my avatar i will pass\n",
            "Text after removing non-alphabetic characters and converting to lowercase: horizon zero dawn looks awesome but no way i am having a smug empowered rope haired hippy bitch as my avatar i will pass\n",
            "Text after tokenization: ['horizon', 'zero', 'dawn', 'looks', 'awesome', 'but', 'no', 'way', 'i', 'am', 'having', 'a', 'smug', 'empowered', 'rope', 'haired', 'hippy', 'bitch', 'as', 'my', 'avatar', 'i', 'will', 'pass']\n",
            "Text after removing stop words: ['horizon', 'zero', 'dawn', 'looks', 'awesome', 'way', 'smug', 'empowered', 'rope', 'haired', 'hippy', 'bitch', 'avatar', 'pass']\n",
            "Text after Lemmatization: ['horizon', 'zero', 'dawn', 'look', 'awesome', 'way', 'smug', 'empowered', 'rope', 'haired', 'hippy', 'bitch', 'avatar', 'pas']\n",
            "Final pre-processed text: horizon zero dawn look awesome way smug empowered rope haired hippy bitch avatar pas\n",
            "Text after removing HTML tags: i feel like i am on a permanent war footing just this afternoon in newcastle city centre it was wall to wall niggers pakis and camel fuckers i wanted to kill every last one of them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i feel like i am on a permanent war footing just this afternoon in newcastle city centre it was wall to wall niggers pakis and camel fuckers i wanted to kill every last one of them\n",
            "Text after tokenization: ['i', 'feel', 'like', 'i', 'am', 'on', 'a', 'permanent', 'war', 'footing', 'just', 'this', 'afternoon', 'in', 'newcastle', 'city', 'centre', 'it', 'was', 'wall', 'to', 'wall', 'niggers', 'pakis', 'and', 'camel', 'fuckers', 'i', 'wanted', 'to', 'kill', 'every', 'last', 'one', 'of', 'them']\n",
            "Text after removing stop words: ['feel', 'like', 'permanent', 'war', 'footing', 'afternoon', 'newcastle', 'city', 'centre', 'wall', 'wall', 'niggers', 'pakis', 'camel', 'fuckers', 'wanted', 'kill', 'every', 'last', 'one']\n",
            "Text after Lemmatization: ['feel', 'like', 'permanent', 'war', 'footing', 'afternoon', 'newcastle', 'city', 'centre', 'wall', 'wall', 'nigger', 'pakis', 'camel', 'fucker', 'wanted', 'kill', 'every', 'last', 'one']\n",
            "Final pre-processed text: feel like permanent war footing afternoon newcastle city centre wall wall nigger pakis camel fucker wanted kill every last one\n",
            "Text after removing HTML tags: fox news ‏ sessions more than  people who came here as refugees are under an fbi investigation today for potential terrorism related activities\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fox news   sessions more than  people who came here as refugees are under an fbi investigation today for potential terrorism related activities\n",
            "Text after tokenization: ['fox', 'news', 'sessions', 'more', 'than', 'people', 'who', 'came', 'here', 'as', 'refugees', 'are', 'under', 'an', 'fbi', 'investigation', 'today', 'for', 'potential', 'terrorism', 'related', 'activities']\n",
            "Text after removing stop words: ['fox', 'news', 'sessions', 'people', 'came', 'refugees', 'fbi', 'investigation', 'today', 'potential', 'terrorism', 'related', 'activities']\n",
            "Text after Lemmatization: ['fox', 'news', 'session', 'people', 'came', 'refugee', 'fbi', 'investigation', 'today', 'potential', 'terrorism', 'related', 'activity']\n",
            "Final pre-processed text: fox news session people came refugee fbi investigation today potential terrorism related activity\n",
            "Text after removing HTML tags: fugly ghetto niggly puff she beast sasquach looking motherfucker\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fugly ghetto niggly puff she beast sasquach looking motherfucker\n",
            "Text after tokenization: ['fugly', 'ghetto', 'niggly', 'puff', 'she', 'beast', 'sasquach', 'looking', 'motherfucker']\n",
            "Text after removing stop words: ['fugly', 'ghetto', 'niggly', 'puff', 'beast', 'sasquach', 'looking', 'motherfucker']\n",
            "Text after Lemmatization: ['fugly', 'ghetto', 'niggly', 'puff', 'beast', 'sasquach', 'looking', 'motherfucker']\n",
            "Final pre-processed text: fugly ghetto niggly puff beast sasquach looking motherfucker\n",
            "Text after removing HTML tags: the redskins is trending is there us teams also called the niggers spics kikes bitches or faggots asking for a friend\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the redskins is trending is there us teams also called the niggers spics kikes bitches or faggots asking for a friend\n",
            "Text after tokenization: ['the', 'redskins', 'is', 'trending', 'is', 'there', 'us', 'teams', 'also', 'called', 'the', 'niggers', 'spics', 'kikes', 'bitches', 'or', 'faggots', 'asking', 'for', 'a', 'friend']\n",
            "Text after removing stop words: ['redskins', 'trending', 'us', 'teams', 'also', 'called', 'niggers', 'spics', 'kikes', 'bitches', 'faggots', 'asking', 'friend']\n",
            "Text after Lemmatization: ['redskin', 'trending', 'u', 'team', 'also', 'called', 'nigger', 'spic', 'kike', 'bitch', 'faggot', 'asking', 'friend']\n",
            "Final pre-processed text: redskin trending u team also called nigger spic kike bitch faggot asking friend\n",
            "Text after removing HTML tags: new followers uhm hi look if you hate liberals lgbt muslim people jewish people millennials or whatever then kindly fuck off i suppose i cant make you but you are gonna want to unfollow anyway after i start posting if not then yo have a nice day i guess\n",
            "Text after removing non-alphabetic characters and converting to lowercase: new followers uhm hi look if you hate liberals lgbt muslim people jewish people millennials or whatever then kindly fuck off i suppose i cant make you but you are gonna want to unfollow anyway after i start posting if not then yo have a nice day i guess\n",
            "Text after tokenization: ['new', 'followers', 'uhm', 'hi', 'look', 'if', 'you', 'hate', 'liberals', 'lgbt', 'muslim', 'people', 'jewish', 'people', 'millennials', 'or', 'whatever', 'then', 'kindly', 'fuck', 'off', 'i', 'suppose', 'i', 'cant', 'make', 'you', 'but', 'you', 'are', 'gon', 'na', 'want', 'to', 'unfollow', 'anyway', 'after', 'i', 'start', 'posting', 'if', 'not', 'then', 'yo', 'have', 'a', 'nice', 'day', 'i', 'guess']\n",
            "Text after removing stop words: ['new', 'followers', 'uhm', 'hi', 'look', 'hate', 'liberals', 'lgbt', 'muslim', 'people', 'jewish', 'people', 'millennials', 'whatever', 'kindly', 'fuck', 'suppose', 'cant', 'make', 'gon', 'na', 'want', 'unfollow', 'anyway', 'start', 'posting', 'yo', 'nice', 'day', 'guess']\n",
            "Text after Lemmatization: ['new', 'follower', 'uhm', 'hi', 'look', 'hate', 'liberal', 'lgbt', 'muslim', 'people', 'jewish', 'people', 'millennials', 'whatever', 'kindly', 'fuck', 'suppose', 'cant', 'make', 'gon', 'na', 'want', 'unfollow', 'anyway', 'start', 'posting', 'yo', 'nice', 'day', 'guess']\n",
            "Final pre-processed text: new follower uhm hi look hate liberal lgbt muslim people jewish people millennials whatever kindly fuck suppose cant make gon na want unfollow anyway start posting yo nice day guess\n",
            "Text after removing HTML tags: the gay harry potter of journalism\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the gay harry potter of journalism\n",
            "Text after tokenization: ['the', 'gay', 'harry', 'potter', 'of', 'journalism']\n",
            "Text after removing stop words: ['gay', 'harry', 'potter', 'journalism']\n",
            "Text after Lemmatization: ['gay', 'harry', 'potter', 'journalism']\n",
            "Final pre-processed text: gay harry potter journalism\n",
            "Text after removing HTML tags: ah eurovision is it still girls kissing each other men in dresses it all got so boringly predictable in a lefty sort of way if they want to be really outrageous condemn isis or feminism or gay marriage use some imagination\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ah eurovision is it still girls kissing each other men in dresses it all got so boringly predictable in a lefty sort of way if they want to be really outrageous condemn isis or feminism or gay marriage use some imagination\n",
            "Text after tokenization: ['ah', 'eurovision', 'is', 'it', 'still', 'girls', 'kissing', 'each', 'other', 'men', 'in', 'dresses', 'it', 'all', 'got', 'so', 'boringly', 'predictable', 'in', 'a', 'lefty', 'sort', 'of', 'way', 'if', 'they', 'want', 'to', 'be', 'really', 'outrageous', 'condemn', 'isis', 'or', 'feminism', 'or', 'gay', 'marriage', 'use', 'some', 'imagination']\n",
            "Text after removing stop words: ['ah', 'eurovision', 'still', 'girls', 'kissing', 'men', 'dresses', 'got', 'boringly', 'predictable', 'lefty', 'sort', 'way', 'want', 'really', 'outrageous', 'condemn', 'isis', 'feminism', 'gay', 'marriage', 'use', 'imagination']\n",
            "Text after Lemmatization: ['ah', 'eurovision', 'still', 'girl', 'kissing', 'men', 'dress', 'got', 'boringly', 'predictable', 'lefty', 'sort', 'way', 'want', 'really', 'outrageous', 'condemn', 'isi', 'feminism', 'gay', 'marriage', 'use', 'imagination']\n",
            "Final pre-processed text: ah eurovision still girl kissing men dress got boringly predictable lefty sort way want really outrageous condemn isi feminism gay marriage use imagination\n",
            "Text after removing HTML tags: lestor dolt on nbc just interrupted broadcasting with uk mudslime runs over pedestrian and is shot by police eventually nothing mentioned about islam or mudslimes of course\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lestor dolt on nbc just interrupted broadcasting with uk mudslime runs over pedestrian and is shot by police eventually nothing mentioned about islam or mudslimes of course\n",
            "Text after tokenization: ['lestor', 'dolt', 'on', 'nbc', 'just', 'interrupted', 'broadcasting', 'with', 'uk', 'mudslime', 'runs', 'over', 'pedestrian', 'and', 'is', 'shot', 'by', 'police', 'eventually', 'nothing', 'mentioned', 'about', 'islam', 'or', 'mudslimes', 'of', 'course']\n",
            "Text after removing stop words: ['lestor', 'dolt', 'nbc', 'interrupted', 'broadcasting', 'uk', 'mudslime', 'runs', 'pedestrian', 'shot', 'police', 'eventually', 'nothing', 'mentioned', 'islam', 'mudslimes', 'course']\n",
            "Text after Lemmatization: ['lestor', 'dolt', 'nbc', 'interrupted', 'broadcasting', 'uk', 'mudslime', 'run', 'pedestrian', 'shot', 'police', 'eventually', 'nothing', 'mentioned', 'islam', 'mudslimes', 'course']\n",
            "Final pre-processed text: lestor dolt nbc interrupted broadcasting uk mudslime run pedestrian shot police eventually nothing mentioned islam mudslimes course\n",
            "Text after removing HTML tags: anyone who tells you that muslim intent is not bad is either naive or has alliance with leftwing muslim monsters radical islam is evil\n",
            "Text after removing non-alphabetic characters and converting to lowercase: anyone who tells you that muslim intent is not bad is either naive or has alliance with leftwing muslim monsters radical islam is evil\n",
            "Text after tokenization: ['anyone', 'who', 'tells', 'you', 'that', 'muslim', 'intent', 'is', 'not', 'bad', 'is', 'either', 'naive', 'or', 'has', 'alliance', 'with', 'leftwing', 'muslim', 'monsters', 'radical', 'islam', 'is', 'evil']\n",
            "Text after removing stop words: ['anyone', 'tells', 'muslim', 'intent', 'bad', 'either', 'naive', 'alliance', 'leftwing', 'muslim', 'monsters', 'radical', 'islam', 'evil']\n",
            "Text after Lemmatization: ['anyone', 'tell', 'muslim', 'intent', 'bad', 'either', 'naive', 'alliance', 'leftwing', 'muslim', 'monster', 'radical', 'islam', 'evil']\n",
            "Final pre-processed text: anyone tell muslim intent bad either naive alliance leftwing muslim monster radical islam evil\n",
            "Text after removing HTML tags: note how the blacks do not seem to be observing any sort of consistency in their ascending and descending of the stairway causing the young chinaman to have to wind his way around these irrational beast people it will not get any better china it ll only get worse get rid of them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: note how the blacks do not seem to be observing any sort of consistency in their ascending and descending of the stairway causing the young chinaman to have to wind his way around these irrational beast people it will not get any better china it ll only get worse get rid of them\n",
            "Text after tokenization: ['note', 'how', 'the', 'blacks', 'do', 'not', 'seem', 'to', 'be', 'observing', 'any', 'sort', 'of', 'consistency', 'in', 'their', 'ascending', 'and', 'descending', 'of', 'the', 'stairway', 'causing', 'the', 'young', 'chinaman', 'to', 'have', 'to', 'wind', 'his', 'way', 'around', 'these', 'irrational', 'beast', 'people', 'it', 'will', 'not', 'get', 'any', 'better', 'china', 'it', 'll', 'only', 'get', 'worse', 'get', 'rid', 'of', 'them']\n",
            "Text after removing stop words: ['note', 'blacks', 'seem', 'observing', 'sort', 'consistency', 'ascending', 'descending', 'stairway', 'causing', 'young', 'chinaman', 'wind', 'way', 'around', 'irrational', 'beast', 'people', 'get', 'better', 'china', 'get', 'worse', 'get', 'rid']\n",
            "Text after Lemmatization: ['note', 'black', 'seem', 'observing', 'sort', 'consistency', 'ascending', 'descending', 'stairway', 'causing', 'young', 'chinaman', 'wind', 'way', 'around', 'irrational', 'beast', 'people', 'get', 'better', 'china', 'get', 'worse', 'get', 'rid']\n",
            "Final pre-processed text: note black seem observing sort consistency ascending descending stairway causing young chinaman wind way around irrational beast people get better china get worse get rid\n",
            "Text after removing HTML tags: the vast majority of hispanics who have east asian heritage do so cause of ancient migration patterns of their ancestors but some countries like brazil and peru took in asian immigrants relatively recently even mexico has a small asian community\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the vast majority of hispanics who have east asian heritage do so cause of ancient migration patterns of their ancestors but some countries like brazil and peru took in asian immigrants relatively recently even mexico has a small asian community\n",
            "Text after tokenization: ['the', 'vast', 'majority', 'of', 'hispanics', 'who', 'have', 'east', 'asian', 'heritage', 'do', 'so', 'cause', 'of', 'ancient', 'migration', 'patterns', 'of', 'their', 'ancestors', 'but', 'some', 'countries', 'like', 'brazil', 'and', 'peru', 'took', 'in', 'asian', 'immigrants', 'relatively', 'recently', 'even', 'mexico', 'has', 'a', 'small', 'asian', 'community']\n",
            "Text after removing stop words: ['vast', 'majority', 'hispanics', 'east', 'asian', 'heritage', 'cause', 'ancient', 'migration', 'patterns', 'ancestors', 'countries', 'like', 'brazil', 'peru', 'took', 'asian', 'immigrants', 'relatively', 'recently', 'even', 'mexico', 'small', 'asian', 'community']\n",
            "Text after Lemmatization: ['vast', 'majority', 'hispanic', 'east', 'asian', 'heritage', 'cause', 'ancient', 'migration', 'pattern', 'ancestor', 'country', 'like', 'brazil', 'peru', 'took', 'asian', 'immigrant', 'relatively', 'recently', 'even', 'mexico', 'small', 'asian', 'community']\n",
            "Final pre-processed text: vast majority hispanic east asian heritage cause ancient migration pattern ancestor country like brazil peru took asian immigrant relatively recently even mexico small asian community\n",
            "Text after removing HTML tags: saw a head bagger in walmart tonight 1 st slave i have seen in my county full spaghetti vestibule hanging on the front i find it disgusting that these islamic invaders are allowed to subjugate women like that in america slavery has been abolished here fuckislam banislam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: saw a head bagger in walmart tonight   st slave i have seen in my county full spaghetti vestibule hanging on the front i find it disgusting that these islamic invaders are allowed to subjugate women like that in america slavery has been abolished here fuckislam banislam\n",
            "Text after tokenization: ['saw', 'a', 'head', 'bagger', 'in', 'walmart', 'tonight', 'st', 'slave', 'i', 'have', 'seen', 'in', 'my', 'county', 'full', 'spaghetti', 'vestibule', 'hanging', 'on', 'the', 'front', 'i', 'find', 'it', 'disgusting', 'that', 'these', 'islamic', 'invaders', 'are', 'allowed', 'to', 'subjugate', 'women', 'like', 'that', 'in', 'america', 'slavery', 'has', 'been', 'abolished', 'here', 'fuckislam', 'banislam']\n",
            "Text after removing stop words: ['saw', 'head', 'bagger', 'walmart', 'tonight', 'st', 'slave', 'seen', 'county', 'full', 'spaghetti', 'vestibule', 'hanging', 'front', 'find', 'disgusting', 'islamic', 'invaders', 'allowed', 'subjugate', 'women', 'like', 'america', 'slavery', 'abolished', 'fuckislam', 'banislam']\n",
            "Text after Lemmatization: ['saw', 'head', 'bagger', 'walmart', 'tonight', 'st', 'slave', 'seen', 'county', 'full', 'spaghetti', 'vestibule', 'hanging', 'front', 'find', 'disgusting', 'islamic', 'invader', 'allowed', 'subjugate', 'woman', 'like', 'america', 'slavery', 'abolished', 'fuckislam', 'banislam']\n",
            "Final pre-processed text: saw head bagger walmart tonight st slave seen county full spaghetti vestibule hanging front find disgusting islamic invader allowed subjugate woman like america slavery abolished fuckislam banislam\n",
            "Text after removing HTML tags: men must not confuse chivalry with letting women walk all over you\n",
            "Text after removing non-alphabetic characters and converting to lowercase: men must not confuse chivalry with letting women walk all over you\n",
            "Text after tokenization: ['men', 'must', 'not', 'confuse', 'chivalry', 'with', 'letting', 'women', 'walk', 'all', 'over', 'you']\n",
            "Text after removing stop words: ['men', 'must', 'confuse', 'chivalry', 'letting', 'women', 'walk']\n",
            "Text after Lemmatization: ['men', 'must', 'confuse', 'chivalry', 'letting', 'woman', 'walk']\n",
            "Final pre-processed text: men must confuse chivalry letting woman walk\n",
            "Text after removing HTML tags: yes retard thats ever so convincing mong face\n",
            "Text after removing non-alphabetic characters and converting to lowercase: yes retard thats ever so convincing mong face\n",
            "Text after tokenization: ['yes', 'retard', 'thats', 'ever', 'so', 'convincing', 'mong', 'face']\n",
            "Text after removing stop words: ['yes', 'retard', 'thats', 'ever', 'convincing', 'mong', 'face']\n",
            "Text after Lemmatization: ['yes', 'retard', 'thats', 'ever', 'convincing', 'mong', 'face']\n",
            "Final pre-processed text: yes retard thats ever convincing mong face\n",
            "Text after removing HTML tags: we have  million invaders and refugees work hard for them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: we have  million invaders and refugees work hard for them\n",
            "Text after tokenization: ['we', 'have', 'million', 'invaders', 'and', 'refugees', 'work', 'hard', 'for', 'them']\n",
            "Text after removing stop words: ['million', 'invaders', 'refugees', 'work', 'hard']\n",
            "Text after Lemmatization: ['million', 'invader', 'refugee', 'work', 'hard']\n",
            "Final pre-processed text: million invader refugee work hard\n",
            "Text after removing HTML tags: these coptic women wear a covering on their head while praying this christian practice predates islam and was enjoined by the apostles all christian sects practiced it until feminism came on the scene headcovering prayercovering patriarchy faith christianity\n",
            "Text after removing non-alphabetic characters and converting to lowercase: these coptic women wear a covering on their head while praying this christian practice predates islam and was enjoined by the apostles all christian sects practiced it until feminism came on the scene headcovering prayercovering patriarchy faith christianity\n",
            "Text after tokenization: ['these', 'coptic', 'women', 'wear', 'a', 'covering', 'on', 'their', 'head', 'while', 'praying', 'this', 'christian', 'practice', 'predates', 'islam', 'and', 'was', 'enjoined', 'by', 'the', 'apostles', 'all', 'christian', 'sects', 'practiced', 'it', 'until', 'feminism', 'came', 'on', 'the', 'scene', 'headcovering', 'prayercovering', 'patriarchy', 'faith', 'christianity']\n",
            "Text after removing stop words: ['coptic', 'women', 'wear', 'covering', 'head', 'praying', 'christian', 'practice', 'predates', 'islam', 'enjoined', 'apostles', 'christian', 'sects', 'practiced', 'feminism', 'came', 'scene', 'headcovering', 'prayercovering', 'patriarchy', 'faith', 'christianity']\n",
            "Text after Lemmatization: ['coptic', 'woman', 'wear', 'covering', 'head', 'praying', 'christian', 'practice', 'predates', 'islam', 'enjoined', 'apostle', 'christian', 'sect', 'practiced', 'feminism', 'came', 'scene', 'headcovering', 'prayercovering', 'patriarchy', 'faith', 'christianity']\n",
            "Final pre-processed text: coptic woman wear covering head praying christian practice predates islam enjoined apostle christian sect practiced feminism came scene headcovering prayercovering patriarchy faith christianity\n",
            "Text after removing HTML tags: it the current year and blacks in africa are still eating albinos because they think it gives them magical powers altright\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it the current year and blacks in africa are still eating albinos because they think it gives them magical powers altright\n",
            "Text after tokenization: ['it', 'the', 'current', 'year', 'and', 'blacks', 'in', 'africa', 'are', 'still', 'eating', 'albinos', 'because', 'they', 'think', 'it', 'gives', 'them', 'magical', 'powers', 'altright']\n",
            "Text after removing stop words: ['current', 'year', 'blacks', 'africa', 'still', 'eating', 'albinos', 'think', 'gives', 'magical', 'powers', 'altright']\n",
            "Text after Lemmatization: ['current', 'year', 'black', 'africa', 'still', 'eating', 'albino', 'think', 'give', 'magical', 'power', 'altright']\n",
            "Final pre-processed text: current year black africa still eating albino think give magical power altright\n",
            "Text after removing HTML tags: why are not more muzzies embarrassed by their fellow muzrat islamofacists who murder and commit other heinous crimes in the name of their book falseprophet may piss be upon him and moongoddess fuc kislam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: why are not more muzzies embarrassed by their fellow muzrat islamofacists who murder and commit other heinous crimes in the name of their book falseprophet may piss be upon him and moongoddess fuc kislam\n",
            "Text after tokenization: ['why', 'are', 'not', 'more', 'muzzies', 'embarrassed', 'by', 'their', 'fellow', 'muzrat', 'islamofacists', 'who', 'murder', 'and', 'commit', 'other', 'heinous', 'crimes', 'in', 'the', 'name', 'of', 'their', 'book', 'falseprophet', 'may', 'piss', 'be', 'upon', 'him', 'and', 'moongoddess', 'fuc', 'kislam']\n",
            "Text after removing stop words: ['muzzies', 'embarrassed', 'fellow', 'muzrat', 'islamofacists', 'murder', 'commit', 'heinous', 'crimes', 'name', 'book', 'falseprophet', 'may', 'piss', 'upon', 'moongoddess', 'fuc', 'kislam']\n",
            "Text after Lemmatization: ['muzzies', 'embarrassed', 'fellow', 'muzrat', 'islamofacists', 'murder', 'commit', 'heinous', 'crime', 'name', 'book', 'falseprophet', 'may', 'piss', 'upon', 'moongoddess', 'fuc', 'kislam']\n",
            "Final pre-processed text: muzzies embarrassed fellow muzrat islamofacists murder commit heinous crime name book falseprophet may piss upon moongoddess fuc kislam\n",
            "Text after removing HTML tags: there are no black south africans they are just invaders south africa belongs to the dutch settlers\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there are no black south africans they are just invaders south africa belongs to the dutch settlers\n",
            "Text after tokenization: ['there', 'are', 'no', 'black', 'south', 'africans', 'they', 'are', 'just', 'invaders', 'south', 'africa', 'belongs', 'to', 'the', 'dutch', 'settlers']\n",
            "Text after removing stop words: ['black', 'south', 'africans', 'invaders', 'south', 'africa', 'belongs', 'dutch', 'settlers']\n",
            "Text after Lemmatization: ['black', 'south', 'african', 'invader', 'south', 'africa', 'belongs', 'dutch', 'settler']\n",
            "Final pre-processed text: black south african invader south africa belongs dutch settler\n",
            "Text after removing HTML tags: the ideas and goals of the alt right versus the religious right are extremely similar the means of getting there and the rhetoric is different the alt right tries to tap into values that are not there socialists are often fundamentally white but rarely fundamentally christian\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the ideas and goals of the alt right versus the religious right are extremely similar the means of getting there and the rhetoric is different the alt right tries to tap into values that are not there socialists are often fundamentally white but rarely fundamentally christian\n",
            "Text after tokenization: ['the', 'ideas', 'and', 'goals', 'of', 'the', 'alt', 'right', 'versus', 'the', 'religious', 'right', 'are', 'extremely', 'similar', 'the', 'means', 'of', 'getting', 'there', 'and', 'the', 'rhetoric', 'is', 'different', 'the', 'alt', 'right', 'tries', 'to', 'tap', 'into', 'values', 'that', 'are', 'not', 'there', 'socialists', 'are', 'often', 'fundamentally', 'white', 'but', 'rarely', 'fundamentally', 'christian']\n",
            "Text after removing stop words: ['ideas', 'goals', 'alt', 'right', 'versus', 'religious', 'right', 'extremely', 'similar', 'means', 'getting', 'rhetoric', 'different', 'alt', 'right', 'tries', 'tap', 'values', 'socialists', 'often', 'fundamentally', 'white', 'rarely', 'fundamentally', 'christian']\n",
            "Text after Lemmatization: ['idea', 'goal', 'alt', 'right', 'versus', 'religious', 'right', 'extremely', 'similar', 'mean', 'getting', 'rhetoric', 'different', 'alt', 'right', 'try', 'tap', 'value', 'socialist', 'often', 'fundamentally', 'white', 'rarely', 'fundamentally', 'christian']\n",
            "Final pre-processed text: idea goal alt right versus religious right extremely similar mean getting rhetoric different alt right try tap value socialist often fundamentally white rarely fundamentally christian\n",
            "Text after removing HTML tags: stop saying hillary clinton lost because she a woman the corrupt bitch does not even help women and girls banfgm banslavery maga speakfreely gabfam trump\n",
            "Text after removing non-alphabetic characters and converting to lowercase: stop saying hillary clinton lost because she a woman the corrupt bitch does not even help women and girls banfgm banslavery maga speakfreely gabfam trump\n",
            "Text after tokenization: ['stop', 'saying', 'hillary', 'clinton', 'lost', 'because', 'she', 'a', 'woman', 'the', 'corrupt', 'bitch', 'does', 'not', 'even', 'help', 'women', 'and', 'girls', 'banfgm', 'banslavery', 'maga', 'speakfreely', 'gabfam', 'trump']\n",
            "Text after removing stop words: ['stop', 'saying', 'hillary', 'clinton', 'lost', 'woman', 'corrupt', 'bitch', 'even', 'help', 'women', 'girls', 'banfgm', 'banslavery', 'maga', 'speakfreely', 'gabfam', 'trump']\n",
            "Text after Lemmatization: ['stop', 'saying', 'hillary', 'clinton', 'lost', 'woman', 'corrupt', 'bitch', 'even', 'help', 'woman', 'girl', 'banfgm', 'banslavery', 'maga', 'speakfreely', 'gabfam', 'trump']\n",
            "Final pre-processed text: stop saying hillary clinton lost woman corrupt bitch even help woman girl banfgm banslavery maga speakfreely gabfam trump\n",
            "Text after removing HTML tags: i know where they ended up but ashkenazim wherever they are are of that italian me descent was under the impression main change in me na was just arabs replacing mixing with whoever was already there\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i know where they ended up but ashkenazim wherever they are are of that italian me descent was under the impression main change in me na was just arabs replacing mixing with whoever was already there\n",
            "Text after tokenization: ['i', 'know', 'where', 'they', 'ended', 'up', 'but', 'ashkenazim', 'wherever', 'they', 'are', 'are', 'of', 'that', 'italian', 'me', 'descent', 'was', 'under', 'the', 'impression', 'main', 'change', 'in', 'me', 'na', 'was', 'just', 'arabs', 'replacing', 'mixing', 'with', 'whoever', 'was', 'already', 'there']\n",
            "Text after removing stop words: ['know', 'ended', 'ashkenazim', 'wherever', 'italian', 'descent', 'impression', 'main', 'change', 'na', 'arabs', 'replacing', 'mixing', 'whoever', 'already']\n",
            "Text after Lemmatization: ['know', 'ended', 'ashkenazi', 'wherever', 'italian', 'descent', 'impression', 'main', 'change', 'na', 'arab', 'replacing', 'mixing', 'whoever', 'already']\n",
            "Final pre-processed text: know ended ashkenazi wherever italian descent impression main change na arab replacing mixing whoever already\n",
            "Text after removing HTML tags: very short nose for a hispanic whilst arabs also have long noses moroccans and north africans tend not to he could be from the middle east but it would be an outrageous construction for the cops\n",
            "Text after removing non-alphabetic characters and converting to lowercase: very short nose for a hispanic whilst arabs also have long noses moroccans and north africans tend not to he could be from the middle east but it would be an outrageous construction for the cops\n",
            "Text after tokenization: ['very', 'short', 'nose', 'for', 'a', 'hispanic', 'whilst', 'arabs', 'also', 'have', 'long', 'noses', 'moroccans', 'and', 'north', 'africans', 'tend', 'not', 'to', 'he', 'could', 'be', 'from', 'the', 'middle', 'east', 'but', 'it', 'would', 'be', 'an', 'outrageous', 'construction', 'for', 'the', 'cops']\n",
            "Text after removing stop words: ['short', 'nose', 'hispanic', 'whilst', 'arabs', 'also', 'long', 'noses', 'moroccans', 'north', 'africans', 'tend', 'could', 'middle', 'east', 'would', 'outrageous', 'construction', 'cops']\n",
            "Text after Lemmatization: ['short', 'nose', 'hispanic', 'whilst', 'arab', 'also', 'long', 'nose', 'moroccan', 'north', 'african', 'tend', 'could', 'middle', 'east', 'would', 'outrageous', 'construction', 'cop']\n",
            "Final pre-processed text: short nose hispanic whilst arab also long nose moroccan north african tend could middle east would outrageous construction cop\n",
            "Text after removing HTML tags: sneaky asians affirmative action is was always anti anglo\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sneaky asians affirmative action is was always anti anglo\n",
            "Text after tokenization: ['sneaky', 'asians', 'affirmative', 'action', 'is', 'was', 'always', 'anti', 'anglo']\n",
            "Text after removing stop words: ['sneaky', 'asians', 'affirmative', 'action', 'always', 'anti', 'anglo']\n",
            "Text after Lemmatization: ['sneaky', 'asian', 'affirmative', 'action', 'always', 'anti', 'anglo']\n",
            "Final pre-processed text: sneaky asian affirmative action always anti anglo\n",
            "Text after removing HTML tags: biggest safe space online they banned me for being too pro trump and questioning beta faggots who had not sworn allegiance to the god emperor apparently i was concern trolling please mods are pussies who are not willing to do what it takes to take their community to the next level\n",
            "Text after removing non-alphabetic characters and converting to lowercase: biggest safe space online they banned me for being too pro trump and questioning beta faggots who had not sworn allegiance to the god emperor apparently i was concern trolling please mods are pussies who are not willing to do what it takes to take their community to the next level\n",
            "Text after tokenization: ['biggest', 'safe', 'space', 'online', 'they', 'banned', 'me', 'for', 'being', 'too', 'pro', 'trump', 'and', 'questioning', 'beta', 'faggots', 'who', 'had', 'not', 'sworn', 'allegiance', 'to', 'the', 'god', 'emperor', 'apparently', 'i', 'was', 'concern', 'trolling', 'please', 'mods', 'are', 'pussies', 'who', 'are', 'not', 'willing', 'to', 'do', 'what', 'it', 'takes', 'to', 'take', 'their', 'community', 'to', 'the', 'next', 'level']\n",
            "Text after removing stop words: ['biggest', 'safe', 'space', 'online', 'banned', 'pro', 'trump', 'questioning', 'beta', 'faggots', 'sworn', 'allegiance', 'god', 'emperor', 'apparently', 'concern', 'trolling', 'please', 'mods', 'pussies', 'willing', 'takes', 'take', 'community', 'next', 'level']\n",
            "Text after Lemmatization: ['biggest', 'safe', 'space', 'online', 'banned', 'pro', 'trump', 'questioning', 'beta', 'faggot', 'sworn', 'allegiance', 'god', 'emperor', 'apparently', 'concern', 'trolling', 'please', 'mod', 'pussy', 'willing', 'take', 'take', 'community', 'next', 'level']\n",
            "Final pre-processed text: biggest safe space online banned pro trump questioning beta faggot sworn allegiance god emperor apparently concern trolling please mod pussy willing take take community next level\n",
            "Text after removing HTML tags: mlk really was a fraud who plagiarized his phd constantly cheated on his wife while lecturing america on morals and routinely banged high school girls as a married reverend in his 3 0 also malcom x really was a gay prostitute\n",
            "Text after removing non-alphabetic characters and converting to lowercase: mlk really was a fraud who plagiarized his phd constantly cheated on his wife while lecturing america on morals and routinely banged high school girls as a married reverend in his     also malcom x really was a gay prostitute\n",
            "Text after tokenization: ['mlk', 'really', 'was', 'a', 'fraud', 'who', 'plagiarized', 'his', 'phd', 'constantly', 'cheated', 'on', 'his', 'wife', 'while', 'lecturing', 'america', 'on', 'morals', 'and', 'routinely', 'banged', 'high', 'school', 'girls', 'as', 'a', 'married', 'reverend', 'in', 'his', 'also', 'malcom', 'x', 'really', 'was', 'a', 'gay', 'prostitute']\n",
            "Text after removing stop words: ['mlk', 'really', 'fraud', 'plagiarized', 'phd', 'constantly', 'cheated', 'wife', 'lecturing', 'america', 'morals', 'routinely', 'banged', 'high', 'school', 'girls', 'married', 'reverend', 'also', 'malcom', 'x', 'really', 'gay', 'prostitute']\n",
            "Text after Lemmatization: ['mlk', 'really', 'fraud', 'plagiarized', 'phd', 'constantly', 'cheated', 'wife', 'lecturing', 'america', 'moral', 'routinely', 'banged', 'high', 'school', 'girl', 'married', 'reverend', 'also', 'malcom', 'x', 'really', 'gay', 'prostitute']\n",
            "Final pre-processed text: mlk really fraud plagiarized phd constantly cheated wife lecturing america moral routinely banged high school girl married reverend also malcom x really gay prostitute\n",
            "Text after removing HTML tags: when niggers spics erupting rags are forced into white neighborhoods by the fucking feds itz wondrous diversity when moneyed whites move into ghetto shitholes remake them into liveable pleasant places itz racist gentrification\n",
            "Text after removing non-alphabetic characters and converting to lowercase: when niggers spics erupting rags are forced into white neighborhoods by the fucking feds itz wondrous diversity when moneyed whites move into ghetto shitholes remake them into liveable pleasant places itz racist gentrification\n",
            "Text after tokenization: ['when', 'niggers', 'spics', 'erupting', 'rags', 'are', 'forced', 'into', 'white', 'neighborhoods', 'by', 'the', 'fucking', 'feds', 'itz', 'wondrous', 'diversity', 'when', 'moneyed', 'whites', 'move', 'into', 'ghetto', 'shitholes', 'remake', 'them', 'into', 'liveable', 'pleasant', 'places', 'itz', 'racist', 'gentrification']\n",
            "Text after removing stop words: ['niggers', 'spics', 'erupting', 'rags', 'forced', 'white', 'neighborhoods', 'fucking', 'feds', 'itz', 'wondrous', 'diversity', 'moneyed', 'whites', 'move', 'ghetto', 'shitholes', 'remake', 'liveable', 'pleasant', 'places', 'itz', 'racist', 'gentrification']\n",
            "Text after Lemmatization: ['nigger', 'spic', 'erupting', 'rag', 'forced', 'white', 'neighborhood', 'fucking', 'fed', 'itz', 'wondrous', 'diversity', 'moneyed', 'white', 'move', 'ghetto', 'shitholes', 'remake', 'liveable', 'pleasant', 'place', 'itz', 'racist', 'gentrification']\n",
            "Final pre-processed text: nigger spic erupting rag forced white neighborhood fucking fed itz wondrous diversity moneyed white move ghetto shitholes remake liveable pleasant place itz racist gentrification\n",
            "Text after removing HTML tags: just remember that the bitch commie girlies and their low iq muds are going to slink beneath the shield of the cuckservatives and alt lite when their independent operation folds like a congolese coup then we will have our civil war with someone who can fight\n",
            "Text after removing non-alphabetic characters and converting to lowercase: just remember that the bitch commie girlies and their low iq muds are going to slink beneath the shield of the cuckservatives and alt lite when their independent operation folds like a congolese coup then we will have our civil war with someone who can fight\n",
            "Text after tokenization: ['just', 'remember', 'that', 'the', 'bitch', 'commie', 'girlies', 'and', 'their', 'low', 'iq', 'muds', 'are', 'going', 'to', 'slink', 'beneath', 'the', 'shield', 'of', 'the', 'cuckservatives', 'and', 'alt', 'lite', 'when', 'their', 'independent', 'operation', 'folds', 'like', 'a', 'congolese', 'coup', 'then', 'we', 'will', 'have', 'our', 'civil', 'war', 'with', 'someone', 'who', 'can', 'fight']\n",
            "Text after removing stop words: ['remember', 'bitch', 'commie', 'girlies', 'low', 'iq', 'muds', 'going', 'slink', 'beneath', 'shield', 'cuckservatives', 'alt', 'lite', 'independent', 'operation', 'folds', 'like', 'congolese', 'coup', 'civil', 'war', 'someone', 'fight']\n",
            "Text after Lemmatization: ['remember', 'bitch', 'commie', 'girlies', 'low', 'iq', 'mud', 'going', 'slink', 'beneath', 'shield', 'cuckservatives', 'alt', 'lite', 'independent', 'operation', 'fold', 'like', 'congolese', 'coup', 'civil', 'war', 'someone', 'fight']\n",
            "Final pre-processed text: remember bitch commie girlies low iq mud going slink beneath shield cuckservatives alt lite independent operation fold like congolese coup civil war someone fight\n",
            "Text after removing HTML tags: it true if i am a moderate muzzy and do not stand against the extremists within my religion i am a worthless dog who deserves the title of muzzy terrorist period\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it true if i am a moderate muzzy and do not stand against the extremists within my religion i am a worthless dog who deserves the title of muzzy terrorist period\n",
            "Text after tokenization: ['it', 'true', 'if', 'i', 'am', 'a', 'moderate', 'muzzy', 'and', 'do', 'not', 'stand', 'against', 'the', 'extremists', 'within', 'my', 'religion', 'i', 'am', 'a', 'worthless', 'dog', 'who', 'deserves', 'the', 'title', 'of', 'muzzy', 'terrorist', 'period']\n",
            "Text after removing stop words: ['true', 'moderate', 'muzzy', 'stand', 'extremists', 'within', 'religion', 'worthless', 'dog', 'deserves', 'title', 'muzzy', 'terrorist', 'period']\n",
            "Text after Lemmatization: ['true', 'moderate', 'muzzy', 'stand', 'extremist', 'within', 'religion', 'worthless', 'dog', 'deserves', 'title', 'muzzy', 'terrorist', 'period']\n",
            "Final pre-processed text: true moderate muzzy stand extremist within religion worthless dog deserves title muzzy terrorist period\n",
            "Text after removing HTML tags: the nigger is hoping the cops beat him up and arrest him so he can wail and cry on camera and get a huge ghetto lottery payout 🐒\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the nigger is hoping the cops beat him up and arrest him so he can wail and cry on camera and get a huge ghetto lottery payout  \n",
            "Text after tokenization: ['the', 'nigger', 'is', 'hoping', 'the', 'cops', 'beat', 'him', 'up', 'and', 'arrest', 'him', 'so', 'he', 'can', 'wail', 'and', 'cry', 'on', 'camera', 'and', 'get', 'a', 'huge', 'ghetto', 'lottery', 'payout']\n",
            "Text after removing stop words: ['nigger', 'hoping', 'cops', 'beat', 'arrest', 'wail', 'cry', 'camera', 'get', 'huge', 'ghetto', 'lottery', 'payout']\n",
            "Text after Lemmatization: ['nigger', 'hoping', 'cop', 'beat', 'arrest', 'wail', 'cry', 'camera', 'get', 'huge', 'ghetto', 'lottery', 'payout']\n",
            "Final pre-processed text: nigger hoping cop beat arrest wail cry camera get huge ghetto lottery payout\n",
            "Text after removing HTML tags: downvoting is for faggots fam everyone knows that much love tho ♥ 🎈\n",
            "Text after removing non-alphabetic characters and converting to lowercase: downvoting is for faggots fam everyone knows that much love tho    \n",
            "Text after tokenization: ['downvoting', 'is', 'for', 'faggots', 'fam', 'everyone', 'knows', 'that', 'much', 'love', 'tho']\n",
            "Text after removing stop words: ['downvoting', 'faggots', 'fam', 'everyone', 'knows', 'much', 'love', 'tho']\n",
            "Text after Lemmatization: ['downvoting', 'faggot', 'fam', 'everyone', 'know', 'much', 'love', 'tho']\n",
            "Final pre-processed text: downvoting faggot fam everyone know much love tho\n",
            "Text after removing HTML tags: the fake news msm does not want americans to know that today black ghettos were almost all white ghettos not politically correct to report facts\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the fake news msm does not want americans to know that today black ghettos were almost all white ghettos not politically correct to report facts\n",
            "Text after tokenization: ['the', 'fake', 'news', 'msm', 'does', 'not', 'want', 'americans', 'to', 'know', 'that', 'today', 'black', 'ghettos', 'were', 'almost', 'all', 'white', 'ghettos', 'not', 'politically', 'correct', 'to', 'report', 'facts']\n",
            "Text after removing stop words: ['fake', 'news', 'msm', 'want', 'americans', 'know', 'today', 'black', 'ghettos', 'almost', 'white', 'ghettos', 'politically', 'correct', 'report', 'facts']\n",
            "Text after Lemmatization: ['fake', 'news', 'msm', 'want', 'american', 'know', 'today', 'black', 'ghetto', 'almost', 'white', 'ghetto', 'politically', 'correct', 'report', 'fact']\n",
            "Final pre-processed text: fake news msm want american know today black ghetto almost white ghetto politically correct report fact\n",
            "Text after removing HTML tags: is this how smelly rapey inbred muzzies see ham fuckislam hamadam\n",
            "Text after removing non-alphabetic characters and converting to lowercase: is this how smelly rapey inbred muzzies see ham fuckislam hamadam\n",
            "Text after tokenization: ['is', 'this', 'how', 'smelly', 'rapey', 'inbred', 'muzzies', 'see', 'ham', 'fuckislam', 'hamadam']\n",
            "Text after removing stop words: ['smelly', 'rapey', 'inbred', 'muzzies', 'see', 'ham', 'fuckislam', 'hamadam']\n",
            "Text after Lemmatization: ['smelly', 'rapey', 'inbred', 'muzzies', 'see', 'ham', 'fuckislam', 'hamadam']\n",
            "Final pre-processed text: smelly rapey inbred muzzies see ham fuckislam hamadam\n",
            "Text after removing HTML tags: this woman needs to just go away i am so sick of her loser bullshit ghetto mafia she is pond scum it is an honor to be deplorable in comparison to these people\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this woman needs to just go away i am so sick of her loser bullshit ghetto mafia she is pond scum it is an honor to be deplorable in comparison to these people\n",
            "Text after tokenization: ['this', 'woman', 'needs', 'to', 'just', 'go', 'away', 'i', 'am', 'so', 'sick', 'of', 'her', 'loser', 'bullshit', 'ghetto', 'mafia', 'she', 'is', 'pond', 'scum', 'it', 'is', 'an', 'honor', 'to', 'be', 'deplorable', 'in', 'comparison', 'to', 'these', 'people']\n",
            "Text after removing stop words: ['woman', 'needs', 'go', 'away', 'sick', 'loser', 'bullshit', 'ghetto', 'mafia', 'pond', 'scum', 'honor', 'deplorable', 'comparison', 'people']\n",
            "Text after Lemmatization: ['woman', 'need', 'go', 'away', 'sick', 'loser', 'bullshit', 'ghetto', 'mafia', 'pond', 'scum', 'honor', 'deplorable', 'comparison', 'people']\n",
            "Final pre-processed text: woman need go away sick loser bullshit ghetto mafia pond scum honor deplorable comparison people\n",
            "Text after removing HTML tags: bottom ‏  no dignity respect 4 women acid attacks rape abuse or gay people thrown off rooftops have we not just fought for 1 0 0 yrs 2 get where we r 🤔\n",
            "Text after removing non-alphabetic characters and converting to lowercase: bottom    no dignity respect   women acid attacks rape abuse or gay people thrown off rooftops have we not just fought for       yrs   get where we r  \n",
            "Text after tokenization: ['bottom', 'no', 'dignity', 'respect', 'women', 'acid', 'attacks', 'rape', 'abuse', 'or', 'gay', 'people', 'thrown', 'off', 'rooftops', 'have', 'we', 'not', 'just', 'fought', 'for', 'yrs', 'get', 'where', 'we', 'r']\n",
            "Text after removing stop words: ['bottom', 'dignity', 'respect', 'women', 'acid', 'attacks', 'rape', 'abuse', 'gay', 'people', 'thrown', 'rooftops', 'fought', 'yrs', 'get', 'r']\n",
            "Text after Lemmatization: ['bottom', 'dignity', 'respect', 'woman', 'acid', 'attack', 'rape', 'abuse', 'gay', 'people', 'thrown', 'rooftop', 'fought', 'yr', 'get', 'r']\n",
            "Final pre-processed text: bottom dignity respect woman acid attack rape abuse gay people thrown rooftop fought yr get r\n",
            "Text after removing HTML tags:   sahih international adds before mohammed and muslim comentators agree this verse does not apply to today christians ibn abbas even thought this verse was abrogated by   conclusion doesnt apply for today christians\n",
            "Text after removing non-alphabetic characters and converting to lowercase:   sahih international adds before mohammed and muslim comentators agree this verse does not apply to today christians ibn abbas even thought this verse was abrogated by   conclusion doesnt apply for today christians\n",
            "Text after tokenization: ['sahih', 'international', 'adds', 'before', 'mohammed', 'and', 'muslim', 'comentators', 'agree', 'this', 'verse', 'does', 'not', 'apply', 'to', 'today', 'christians', 'ibn', 'abbas', 'even', 'thought', 'this', 'verse', 'was', 'abrogated', 'by', 'conclusion', 'doesnt', 'apply', 'for', 'today', 'christians']\n",
            "Text after removing stop words: ['sahih', 'international', 'adds', 'mohammed', 'muslim', 'comentators', 'agree', 'verse', 'apply', 'today', 'christians', 'ibn', 'abbas', 'even', 'thought', 'verse', 'abrogated', 'conclusion', 'doesnt', 'apply', 'today', 'christians']\n",
            "Text after Lemmatization: ['sahih', 'international', 'add', 'mohammed', 'muslim', 'comentators', 'agree', 'verse', 'apply', 'today', 'christian', 'ibn', 'abbas', 'even', 'thought', 'verse', 'abrogated', 'conclusion', 'doesnt', 'apply', 'today', 'christian']\n",
            "Final pre-processed text: sahih international add mohammed muslim comentators agree verse apply today christian ibn abbas even thought verse abrogated conclusion doesnt apply today christian\n",
            "Text after removing HTML tags: sorry those niggers are not christian they do not even claim to be they are heathens and many even avow they follow muslime anyone can be christian but they are not never heard of them and no it does not sound like they are christians\n",
            "Text after removing non-alphabetic characters and converting to lowercase: sorry those niggers are not christian they do not even claim to be they are heathens and many even avow they follow muslime anyone can be christian but they are not never heard of them and no it does not sound like they are christians\n",
            "Text after tokenization: ['sorry', 'those', 'niggers', 'are', 'not', 'christian', 'they', 'do', 'not', 'even', 'claim', 'to', 'be', 'they', 'are', 'heathens', 'and', 'many', 'even', 'avow', 'they', 'follow', 'muslime', 'anyone', 'can', 'be', 'christian', 'but', 'they', 'are', 'not', 'never', 'heard', 'of', 'them', 'and', 'no', 'it', 'does', 'not', 'sound', 'like', 'they', 'are', 'christians']\n",
            "Text after removing stop words: ['sorry', 'niggers', 'christian', 'even', 'claim', 'heathens', 'many', 'even', 'avow', 'follow', 'muslime', 'anyone', 'christian', 'never', 'heard', 'sound', 'like', 'christians']\n",
            "Text after Lemmatization: ['sorry', 'nigger', 'christian', 'even', 'claim', 'heathen', 'many', 'even', 'avow', 'follow', 'muslime', 'anyone', 'christian', 'never', 'heard', 'sound', 'like', 'christian']\n",
            "Final pre-processed text: sorry nigger christian even claim heathen many even avow follow muslime anyone christian never heard sound like christian\n",
            "Text after removing HTML tags: ill honk as i drive through your ghetto cant stop i have white business to attend to\n",
            "Text after removing non-alphabetic characters and converting to lowercase: ill honk as i drive through your ghetto cant stop i have white business to attend to\n",
            "Text after tokenization: ['ill', 'honk', 'as', 'i', 'drive', 'through', 'your', 'ghetto', 'cant', 'stop', 'i', 'have', 'white', 'business', 'to', 'attend', 'to']\n",
            "Text after removing stop words: ['ill', 'honk', 'drive', 'ghetto', 'cant', 'stop', 'white', 'business', 'attend']\n",
            "Text after Lemmatization: ['ill', 'honk', 'drive', 'ghetto', 'cant', 'stop', 'white', 'business', 'attend']\n",
            "Final pre-processed text: ill honk drive ghetto cant stop white business attend\n",
            "Text after removing HTML tags: shut the fuck up you autistic retard\n",
            "Text after removing non-alphabetic characters and converting to lowercase: shut the fuck up you autistic retard\n",
            "Text after tokenization: ['shut', 'the', 'fuck', 'up', 'you', 'autistic', 'retard']\n",
            "Text after removing stop words: ['shut', 'fuck', 'autistic', 'retard']\n",
            "Text after Lemmatization: ['shut', 'fuck', 'autistic', 'retard']\n",
            "Final pre-processed text: shut fuck autistic retard\n",
            "Text after removing HTML tags: this is a win for maga do u think people realize these non profits are a back door to supporting immigrants plus all the federal so they get welfare non profit grant money taxpayers this is a racket that is on a scale of the clintonfoundation where are investigative reporters\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this is a win for maga do u think people realize these non profits are a back door to supporting immigrants plus all the federal so they get welfare non profit grant money taxpayers this is a racket that is on a scale of the clintonfoundation where are investigative reporters\n",
            "Text after tokenization: ['this', 'is', 'a', 'win', 'for', 'maga', 'do', 'u', 'think', 'people', 'realize', 'these', 'non', 'profits', 'are', 'a', 'back', 'door', 'to', 'supporting', 'immigrants', 'plus', 'all', 'the', 'federal', 'so', 'they', 'get', 'welfare', 'non', 'profit', 'grant', 'money', 'taxpayers', 'this', 'is', 'a', 'racket', 'that', 'is', 'on', 'a', 'scale', 'of', 'the', 'clintonfoundation', 'where', 'are', 'investigative', 'reporters']\n",
            "Text after removing stop words: ['win', 'maga', 'u', 'think', 'people', 'realize', 'non', 'profits', 'back', 'door', 'supporting', 'immigrants', 'plus', 'federal', 'get', 'welfare', 'non', 'profit', 'grant', 'money', 'taxpayers', 'racket', 'scale', 'clintonfoundation', 'investigative', 'reporters']\n",
            "Text after Lemmatization: ['win', 'maga', 'u', 'think', 'people', 'realize', 'non', 'profit', 'back', 'door', 'supporting', 'immigrant', 'plus', 'federal', 'get', 'welfare', 'non', 'profit', 'grant', 'money', 'taxpayer', 'racket', 'scale', 'clintonfoundation', 'investigative', 'reporter']\n",
            "Final pre-processed text: win maga u think people realize non profit back door supporting immigrant plus federal get welfare non profit grant money taxpayer racket scale clintonfoundation investigative reporter\n",
            "Text after removing HTML tags: the phrase assad uses chemicals weapons is code for israel wants a final solution implemented on the arabs\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the phrase assad uses chemicals weapons is code for israel wants a final solution implemented on the arabs\n",
            "Text after tokenization: ['the', 'phrase', 'assad', 'uses', 'chemicals', 'weapons', 'is', 'code', 'for', 'israel', 'wants', 'a', 'final', 'solution', 'implemented', 'on', 'the', 'arabs']\n",
            "Text after removing stop words: ['phrase', 'assad', 'uses', 'chemicals', 'weapons', 'code', 'israel', 'wants', 'final', 'solution', 'implemented', 'arabs']\n",
            "Text after Lemmatization: ['phrase', 'assad', 'us', 'chemical', 'weapon', 'code', 'israel', 'want', 'final', 'solution', 'implemented', 'arab']\n",
            "Final pre-processed text: phrase assad us chemical weapon code israel want final solution implemented arab\n",
            "Text after removing HTML tags: janet o hara2 hours ago ghetto college send your kids there and they can learn to speak ghetto too get your degree in ebonics you will be so desirable to employers you will be sought after in hiring lmao president is a wimp and lets the insane asylum run the school ﻿\n",
            "Text after removing non-alphabetic characters and converting to lowercase: janet o hara  hours ago ghetto college send your kids there and they can learn to speak ghetto too get your degree in ebonics you will be so desirable to employers you will be sought after in hiring lmao president is a wimp and lets the insane asylum run the school  \n",
            "Text after tokenization: ['janet', 'o', 'hara', 'hours', 'ago', 'ghetto', 'college', 'send', 'your', 'kids', 'there', 'and', 'they', 'can', 'learn', 'to', 'speak', 'ghetto', 'too', 'get', 'your', 'degree', 'in', 'ebonics', 'you', 'will', 'be', 'so', 'desirable', 'to', 'employers', 'you', 'will', 'be', 'sought', 'after', 'in', 'hiring', 'lmao', 'president', 'is', 'a', 'wimp', 'and', 'lets', 'the', 'insane', 'asylum', 'run', 'the', 'school']\n",
            "Text after removing stop words: ['janet', 'hara', 'hours', 'ago', 'ghetto', 'college', 'send', 'kids', 'learn', 'speak', 'ghetto', 'get', 'degree', 'ebonics', 'desirable', 'employers', 'sought', 'hiring', 'lmao', 'president', 'wimp', 'lets', 'insane', 'asylum', 'run', 'school']\n",
            "Text after Lemmatization: ['janet', 'hara', 'hour', 'ago', 'ghetto', 'college', 'send', 'kid', 'learn', 'speak', 'ghetto', 'get', 'degree', 'ebonics', 'desirable', 'employer', 'sought', 'hiring', 'lmao', 'president', 'wimp', 'let', 'insane', 'asylum', 'run', 'school']\n",
            "Final pre-processed text: janet hara hour ago ghetto college send kid learn speak ghetto get degree ebonics desirable employer sought hiring lmao president wimp let insane asylum run school\n",
            "Text after removing HTML tags: deecee faggots panicking over coyote sightings\n",
            "Text after removing non-alphabetic characters and converting to lowercase: deecee faggots panicking over coyote sightings\n",
            "Text after tokenization: ['deecee', 'faggots', 'panicking', 'over', 'coyote', 'sightings']\n",
            "Text after removing stop words: ['deecee', 'faggots', 'panicking', 'coyote', 'sightings']\n",
            "Text after Lemmatization: ['deecee', 'faggot', 'panicking', 'coyote', 'sighting']\n",
            "Final pre-processed text: deecee faggot panicking coyote sighting\n",
            "Text after removing HTML tags: someone should break a bat over that queer nigger head\n",
            "Text after removing non-alphabetic characters and converting to lowercase: someone should break a bat over that queer nigger head\n",
            "Text after tokenization: ['someone', 'should', 'break', 'a', 'bat', 'over', 'that', 'queer', 'nigger', 'head']\n",
            "Text after removing stop words: ['someone', 'break', 'bat', 'queer', 'nigger', 'head']\n",
            "Text after Lemmatization: ['someone', 'break', 'bat', 'queer', 'nigger', 'head']\n",
            "Final pre-processed text: someone break bat queer nigger head\n",
            "Text after removing HTML tags: it not over exaggerated they are not rumours it not an average amount of crime in the time it took you to read this post a crime has been committed by a refugee in europe refugees are supposed to be fleeing persecution not enforcing it in a different country\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it not over exaggerated they are not rumours it not an average amount of crime in the time it took you to read this post a crime has been committed by a refugee in europe refugees are supposed to be fleeing persecution not enforcing it in a different country\n",
            "Text after tokenization: ['it', 'not', 'over', 'exaggerated', 'they', 'are', 'not', 'rumours', 'it', 'not', 'an', 'average', 'amount', 'of', 'crime', 'in', 'the', 'time', 'it', 'took', 'you', 'to', 'read', 'this', 'post', 'a', 'crime', 'has', 'been', 'committed', 'by', 'a', 'refugee', 'in', 'europe', 'refugees', 'are', 'supposed', 'to', 'be', 'fleeing', 'persecution', 'not', 'enforcing', 'it', 'in', 'a', 'different', 'country']\n",
            "Text after removing stop words: ['exaggerated', 'rumours', 'average', 'amount', 'crime', 'time', 'took', 'read', 'post', 'crime', 'committed', 'refugee', 'europe', 'refugees', 'supposed', 'fleeing', 'persecution', 'enforcing', 'different', 'country']\n",
            "Text after Lemmatization: ['exaggerated', 'rumour', 'average', 'amount', 'crime', 'time', 'took', 'read', 'post', 'crime', 'committed', 'refugee', 'europe', 'refugee', 'supposed', 'fleeing', 'persecution', 'enforcing', 'different', 'country']\n",
            "Final pre-processed text: exaggerated rumour average amount crime time took read post crime committed refugee europe refugee supposed fleeing persecution enforcing different country\n",
            "Text after removing HTML tags: going to a resort for my sister wedding in a few months got some pretty dresses of all the middle aged women there who have had two  lb babies in the last  yr i will be the hottest my policy is set the bar high but achievable\n",
            "Text after removing non-alphabetic characters and converting to lowercase: going to a resort for my sister wedding in a few months got some pretty dresses of all the middle aged women there who have had two  lb babies in the last  yr i will be the hottest my policy is set the bar high but achievable\n",
            "Text after tokenization: ['going', 'to', 'a', 'resort', 'for', 'my', 'sister', 'wedding', 'in', 'a', 'few', 'months', 'got', 'some', 'pretty', 'dresses', 'of', 'all', 'the', 'middle', 'aged', 'women', 'there', 'who', 'have', 'had', 'two', 'lb', 'babies', 'in', 'the', 'last', 'yr', 'i', 'will', 'be', 'the', 'hottest', 'my', 'policy', 'is', 'set', 'the', 'bar', 'high', 'but', 'achievable']\n",
            "Text after removing stop words: ['going', 'resort', 'sister', 'wedding', 'months', 'got', 'pretty', 'dresses', 'middle', 'aged', 'women', 'two', 'lb', 'babies', 'last', 'yr', 'hottest', 'policy', 'set', 'bar', 'high', 'achievable']\n",
            "Text after Lemmatization: ['going', 'resort', 'sister', 'wedding', 'month', 'got', 'pretty', 'dress', 'middle', 'aged', 'woman', 'two', 'lb', 'baby', 'last', 'yr', 'hottest', 'policy', 'set', 'bar', 'high', 'achievable']\n",
            "Final pre-processed text: going resort sister wedding month got pretty dress middle aged woman two lb baby last yr hottest policy set bar high achievable\n",
            "Text after removing HTML tags: notice how none of the race retards has anything to say about the conspiracy behind the comeback leading to the women french open title omg it wuz too european white women so it must b teh pure\n",
            "Text after removing non-alphabetic characters and converting to lowercase: notice how none of the race retards has anything to say about the conspiracy behind the comeback leading to the women french open title omg it wuz too european white women so it must b teh pure\n",
            "Text after tokenization: ['notice', 'how', 'none', 'of', 'the', 'race', 'retards', 'has', 'anything', 'to', 'say', 'about', 'the', 'conspiracy', 'behind', 'the', 'comeback', 'leading', 'to', 'the', 'women', 'french', 'open', 'title', 'omg', 'it', 'wuz', 'too', 'european', 'white', 'women', 'so', 'it', 'must', 'b', 'teh', 'pure']\n",
            "Text after removing stop words: ['notice', 'none', 'race', 'retards', 'anything', 'say', 'conspiracy', 'behind', 'comeback', 'leading', 'women', 'french', 'open', 'title', 'omg', 'wuz', 'european', 'white', 'women', 'must', 'b', 'teh', 'pure']\n",
            "Text after Lemmatization: ['notice', 'none', 'race', 'retard', 'anything', 'say', 'conspiracy', 'behind', 'comeback', 'leading', 'woman', 'french', 'open', 'title', 'omg', 'wuz', 'european', 'white', 'woman', 'must', 'b', 'teh', 'pure']\n",
            "Final pre-processed text: notice none race retard anything say conspiracy behind comeback leading woman french open title omg wuz european white woman must b teh pure\n",
            "Text after removing HTML tags: i should write a comic book about a white superhero who takes a woodchipper to a ghetto hospital an throws all the newborn niglets into it 😹 😹 😹\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i should write a comic book about a white superhero who takes a woodchipper to a ghetto hospital an throws all the newborn niglets into it      \n",
            "Text after tokenization: ['i', 'should', 'write', 'a', 'comic', 'book', 'about', 'a', 'white', 'superhero', 'who', 'takes', 'a', 'woodchipper', 'to', 'a', 'ghetto', 'hospital', 'an', 'throws', 'all', 'the', 'newborn', 'niglets', 'into', 'it']\n",
            "Text after removing stop words: ['write', 'comic', 'book', 'white', 'superhero', 'takes', 'woodchipper', 'ghetto', 'hospital', 'throws', 'newborn', 'niglets']\n",
            "Text after Lemmatization: ['write', 'comic', 'book', 'white', 'superhero', 'take', 'woodchipper', 'ghetto', 'hospital', 'throw', 'newborn', 'niglets']\n",
            "Final pre-processed text: write comic book white superhero take woodchipper ghetto hospital throw newborn niglets\n",
            "Text after removing HTML tags: a gay who is respectful to others conducts his personal life privately discretely does not seek to force others to celebrate gayness obeys our laws especially those regarding controlled substances sexual contact with minors there are many such gays i have no problem with them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: a gay who is respectful to others conducts his personal life privately discretely does not seek to force others to celebrate gayness obeys our laws especially those regarding controlled substances sexual contact with minors there are many such gays i have no problem with them\n",
            "Text after tokenization: ['a', 'gay', 'who', 'is', 'respectful', 'to', 'others', 'conducts', 'his', 'personal', 'life', 'privately', 'discretely', 'does', 'not', 'seek', 'to', 'force', 'others', 'to', 'celebrate', 'gayness', 'obeys', 'our', 'laws', 'especially', 'those', 'regarding', 'controlled', 'substances', 'sexual', 'contact', 'with', 'minors', 'there', 'are', 'many', 'such', 'gays', 'i', 'have', 'no', 'problem', 'with', 'them']\n",
            "Text after removing stop words: ['gay', 'respectful', 'others', 'conducts', 'personal', 'life', 'privately', 'discretely', 'seek', 'force', 'others', 'celebrate', 'gayness', 'obeys', 'laws', 'especially', 'regarding', 'controlled', 'substances', 'sexual', 'contact', 'minors', 'many', 'gays', 'problem']\n",
            "Text after Lemmatization: ['gay', 'respectful', 'others', 'conduct', 'personal', 'life', 'privately', 'discretely', 'seek', 'force', 'others', 'celebrate', 'gayness', 'obeys', 'law', 'especially', 'regarding', 'controlled', 'substance', 'sexual', 'contact', 'minor', 'many', 'gay', 'problem']\n",
            "Final pre-processed text: gay respectful others conduct personal life privately discretely seek force others celebrate gayness obeys law especially regarding controlled substance sexual contact minor many gay problem\n",
            "Text after removing HTML tags: there must have been too many original inhabitant white londoners in that apartment building so the jihadi gangster london mayor sent round some of his lads to burn down the building also of note once again a tall building on fire did not collapse funny that innit\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there must have been too many original inhabitant white londoners in that apartment building so the jihadi gangster london mayor sent round some of his lads to burn down the building also of note once again a tall building on fire did not collapse funny that innit\n",
            "Text after tokenization: ['there', 'must', 'have', 'been', 'too', 'many', 'original', 'inhabitant', 'white', 'londoners', 'in', 'that', 'apartment', 'building', 'so', 'the', 'jihadi', 'gangster', 'london', 'mayor', 'sent', 'round', 'some', 'of', 'his', 'lads', 'to', 'burn', 'down', 'the', 'building', 'also', 'of', 'note', 'once', 'again', 'a', 'tall', 'building', 'on', 'fire', 'did', 'not', 'collapse', 'funny', 'that', 'innit']\n",
            "Text after removing stop words: ['must', 'many', 'original', 'inhabitant', 'white', 'londoners', 'apartment', 'building', 'jihadi', 'gangster', 'london', 'mayor', 'sent', 'round', 'lads', 'burn', 'building', 'also', 'note', 'tall', 'building', 'fire', 'collapse', 'funny', 'innit']\n",
            "Text after Lemmatization: ['must', 'many', 'original', 'inhabitant', 'white', 'londoner', 'apartment', 'building', 'jihadi', 'gangster', 'london', 'mayor', 'sent', 'round', 'lad', 'burn', 'building', 'also', 'note', 'tall', 'building', 'fire', 'collapse', 'funny', 'innit']\n",
            "Final pre-processed text: must many original inhabitant white londoner apartment building jihadi gangster london mayor sent round lad burn building also note tall building fire collapse funny innit\n",
            "Text after removing HTML tags: lol bill maher thinks moslems are brave a suicide jihadi would run away from me if i told him i had bacon bits on me to send him to hell\n",
            "Text after removing non-alphabetic characters and converting to lowercase: lol bill maher thinks moslems are brave a suicide jihadi would run away from me if i told him i had bacon bits on me to send him to hell\n",
            "Text after tokenization: ['lol', 'bill', 'maher', 'thinks', 'moslems', 'are', 'brave', 'a', 'suicide', 'jihadi', 'would', 'run', 'away', 'from', 'me', 'if', 'i', 'told', 'him', 'i', 'had', 'bacon', 'bits', 'on', 'me', 'to', 'send', 'him', 'to', 'hell']\n",
            "Text after removing stop words: ['lol', 'bill', 'maher', 'thinks', 'moslems', 'brave', 'suicide', 'jihadi', 'would', 'run', 'away', 'told', 'bacon', 'bits', 'send', 'hell']\n",
            "Text after Lemmatization: ['lol', 'bill', 'maher', 'think', 'moslem', 'brave', 'suicide', 'jihadi', 'would', 'run', 'away', 'told', 'bacon', 'bit', 'send', 'hell']\n",
            "Final pre-processed text: lol bill maher think moslem brave suicide jihadi would run away told bacon bit send hell\n",
            "Text after removing HTML tags: blacks fucked themselves by going for lbj great society programs hook line and sinker he said aboard af  after the law was passed i will have those niggers voting democrat for the next  years he was right\n",
            "Text after removing non-alphabetic characters and converting to lowercase: blacks fucked themselves by going for lbj great society programs hook line and sinker he said aboard af  after the law was passed i will have those niggers voting democrat for the next  years he was right\n",
            "Text after tokenization: ['blacks', 'fucked', 'themselves', 'by', 'going', 'for', 'lbj', 'great', 'society', 'programs', 'hook', 'line', 'and', 'sinker', 'he', 'said', 'aboard', 'af', 'after', 'the', 'law', 'was', 'passed', 'i', 'will', 'have', 'those', 'niggers', 'voting', 'democrat', 'for', 'the', 'next', 'years', 'he', 'was', 'right']\n",
            "Text after removing stop words: ['blacks', 'fucked', 'going', 'lbj', 'great', 'society', 'programs', 'hook', 'line', 'sinker', 'said', 'aboard', 'af', 'law', 'passed', 'niggers', 'voting', 'democrat', 'next', 'years', 'right']\n",
            "Text after Lemmatization: ['black', 'fucked', 'going', 'lbj', 'great', 'society', 'program', 'hook', 'line', 'sinker', 'said', 'aboard', 'af', 'law', 'passed', 'nigger', 'voting', 'democrat', 'next', 'year', 'right']\n",
            "Final pre-processed text: black fucked going lbj great society program hook line sinker said aboard af law passed nigger voting democrat next year right\n",
            "Text after removing HTML tags: the harder i work the more i get prison shower raped by my government u middle class\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the harder i work the more i get prison shower raped by my government u middle class\n",
            "Text after tokenization: ['the', 'harder', 'i', 'work', 'the', 'more', 'i', 'get', 'prison', 'shower', 'raped', 'by', 'my', 'government', 'u', 'middle', 'class']\n",
            "Text after removing stop words: ['harder', 'work', 'get', 'prison', 'shower', 'raped', 'government', 'u', 'middle', 'class']\n",
            "Text after Lemmatization: ['harder', 'work', 'get', 'prison', 'shower', 'raped', 'government', 'u', 'middle', 'class']\n",
            "Final pre-processed text: harder work get prison shower raped government u middle class\n",
            "Text after removing HTML tags: it said that at some point during their life every man will have a homosexual fantasy mine is to kick the fuck out of one\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it said that at some point during their life every man will have a homosexual fantasy mine is to kick the fuck out of one\n",
            "Text after tokenization: ['it', 'said', 'that', 'at', 'some', 'point', 'during', 'their', 'life', 'every', 'man', 'will', 'have', 'a', 'homosexual', 'fantasy', 'mine', 'is', 'to', 'kick', 'the', 'fuck', 'out', 'of', 'one']\n",
            "Text after removing stop words: ['said', 'point', 'life', 'every', 'man', 'homosexual', 'fantasy', 'mine', 'kick', 'fuck', 'one']\n",
            "Text after Lemmatization: ['said', 'point', 'life', 'every', 'man', 'homosexual', 'fantasy', 'mine', 'kick', 'fuck', 'one']\n",
            "Final pre-processed text: said point life every man homosexual fantasy mine kick fuck one\n",
            "Text after removing HTML tags: they are going around right now looking for racist zip codes they are getting into some of the nicest areas here right now full ghetto bullshit coming in diversity divides stl is fucking rough and no white person goes to their neighborhoods for a reason no reason to invite them in ours\n",
            "Text after removing non-alphabetic characters and converting to lowercase: they are going around right now looking for racist zip codes they are getting into some of the nicest areas here right now full ghetto bullshit coming in diversity divides stl is fucking rough and no white person goes to their neighborhoods for a reason no reason to invite them in ours\n",
            "Text after tokenization: ['they', 'are', 'going', 'around', 'right', 'now', 'looking', 'for', 'racist', 'zip', 'codes', 'they', 'are', 'getting', 'into', 'some', 'of', 'the', 'nicest', 'areas', 'here', 'right', 'now', 'full', 'ghetto', 'bullshit', 'coming', 'in', 'diversity', 'divides', 'stl', 'is', 'fucking', 'rough', 'and', 'no', 'white', 'person', 'goes', 'to', 'their', 'neighborhoods', 'for', 'a', 'reason', 'no', 'reason', 'to', 'invite', 'them', 'in', 'ours']\n",
            "Text after removing stop words: ['going', 'around', 'right', 'looking', 'racist', 'zip', 'codes', 'getting', 'nicest', 'areas', 'right', 'full', 'ghetto', 'bullshit', 'coming', 'diversity', 'divides', 'stl', 'fucking', 'rough', 'white', 'person', 'goes', 'neighborhoods', 'reason', 'reason', 'invite']\n",
            "Text after Lemmatization: ['going', 'around', 'right', 'looking', 'racist', 'zip', 'code', 'getting', 'nicest', 'area', 'right', 'full', 'ghetto', 'bullshit', 'coming', 'diversity', 'divide', 'stl', 'fucking', 'rough', 'white', 'person', 'go', 'neighborhood', 'reason', 'reason', 'invite']\n",
            "Final pre-processed text: going around right looking racist zip code getting nicest area right full ghetto bullshit coming diversity divide stl fucking rough white person go neighborhood reason reason invite\n",
            "Text after removing HTML tags: america has slowly begun to acknowledge yeah only after decades of kike brainwashing they always leave that essential element unmentioned\n",
            "Text after removing non-alphabetic characters and converting to lowercase: america has slowly begun to acknowledge yeah only after decades of kike brainwashing they always leave that essential element unmentioned\n",
            "Text after tokenization: ['america', 'has', 'slowly', 'begun', 'to', 'acknowledge', 'yeah', 'only', 'after', 'decades', 'of', 'kike', 'brainwashing', 'they', 'always', 'leave', 'that', 'essential', 'element', 'unmentioned']\n",
            "Text after removing stop words: ['america', 'slowly', 'begun', 'acknowledge', 'yeah', 'decades', 'kike', 'brainwashing', 'always', 'leave', 'essential', 'element', 'unmentioned']\n",
            "Text after Lemmatization: ['america', 'slowly', 'begun', 'acknowledge', 'yeah', 'decade', 'kike', 'brainwashing', 'always', 'leave', 'essential', 'element', 'unmentioned']\n",
            "Final pre-processed text: america slowly begun acknowledge yeah decade kike brainwashing always leave essential element unmentioned\n",
            "Text after removing HTML tags: the little girl did not really know what was going on its pretty shocking because we can diagnose her future if she remains in that ghetto disease rape and degradation ending with the removal of her genetic lineage from the white gene pool and macron is calling for more of this traitors must be hung\n",
            "Text after removing non-alphabetic characters and converting to lowercase: the little girl did not really know what was going on its pretty shocking because we can diagnose her future if she remains in that ghetto disease rape and degradation ending with the removal of her genetic lineage from the white gene pool and macron is calling for more of this traitors must be hung\n",
            "Text after tokenization: ['the', 'little', 'girl', 'did', 'not', 'really', 'know', 'what', 'was', 'going', 'on', 'its', 'pretty', 'shocking', 'because', 'we', 'can', 'diagnose', 'her', 'future', 'if', 'she', 'remains', 'in', 'that', 'ghetto', 'disease', 'rape', 'and', 'degradation', 'ending', 'with', 'the', 'removal', 'of', 'her', 'genetic', 'lineage', 'from', 'the', 'white', 'gene', 'pool', 'and', 'macron', 'is', 'calling', 'for', 'more', 'of', 'this', 'traitors', 'must', 'be', 'hung']\n",
            "Text after removing stop words: ['little', 'girl', 'really', 'know', 'going', 'pretty', 'shocking', 'diagnose', 'future', 'remains', 'ghetto', 'disease', 'rape', 'degradation', 'ending', 'removal', 'genetic', 'lineage', 'white', 'gene', 'pool', 'macron', 'calling', 'traitors', 'must', 'hung']\n",
            "Text after Lemmatization: ['little', 'girl', 'really', 'know', 'going', 'pretty', 'shocking', 'diagnose', 'future', 'remains', 'ghetto', 'disease', 'rape', 'degradation', 'ending', 'removal', 'genetic', 'lineage', 'white', 'gene', 'pool', 'macron', 'calling', 'traitor', 'must', 'hung']\n",
            "Final pre-processed text: little girl really know going pretty shocking diagnose future remains ghetto disease rape degradation ending removal genetic lineage white gene pool macron calling traitor must hung\n",
            "Text after removing HTML tags: fastest growing by percentile not numbers asians doubling in numbers would still be  whites for every asian you are in a university environment so all those women white or not should be assumed to be ruined until proven otherwise i am not pro ricemixing but it not the main problem here\n",
            "Text after removing non-alphabetic characters and converting to lowercase: fastest growing by percentile not numbers asians doubling in numbers would still be  whites for every asian you are in a university environment so all those women white or not should be assumed to be ruined until proven otherwise i am not pro ricemixing but it not the main problem here\n",
            "Text after tokenization: ['fastest', 'growing', 'by', 'percentile', 'not', 'numbers', 'asians', 'doubling', 'in', 'numbers', 'would', 'still', 'be', 'whites', 'for', 'every', 'asian', 'you', 'are', 'in', 'a', 'university', 'environment', 'so', 'all', 'those', 'women', 'white', 'or', 'not', 'should', 'be', 'assumed', 'to', 'be', 'ruined', 'until', 'proven', 'otherwise', 'i', 'am', 'not', 'pro', 'ricemixing', 'but', 'it', 'not', 'the', 'main', 'problem', 'here']\n",
            "Text after removing stop words: ['fastest', 'growing', 'percentile', 'numbers', 'asians', 'doubling', 'numbers', 'would', 'still', 'whites', 'every', 'asian', 'university', 'environment', 'women', 'white', 'assumed', 'ruined', 'proven', 'otherwise', 'pro', 'ricemixing', 'main', 'problem']\n",
            "Text after Lemmatization: ['fastest', 'growing', 'percentile', 'number', 'asian', 'doubling', 'number', 'would', 'still', 'white', 'every', 'asian', 'university', 'environment', 'woman', 'white', 'assumed', 'ruined', 'proven', 'otherwise', 'pro', 'ricemixing', 'main', 'problem']\n",
            "Final pre-processed text: fastest growing percentile number asian doubling number would still white every asian university environment woman white assumed ruined proven otherwise pro ricemixing main problem\n",
            "Text after removing HTML tags: tell that to the immigrants who came to america with almost nothing and achieved wealth\n",
            "Text after removing non-alphabetic characters and converting to lowercase: tell that to the immigrants who came to america with almost nothing and achieved wealth\n",
            "Text after tokenization: ['tell', 'that', 'to', 'the', 'immigrants', 'who', 'came', 'to', 'america', 'with', 'almost', 'nothing', 'and', 'achieved', 'wealth']\n",
            "Text after removing stop words: ['tell', 'immigrants', 'came', 'america', 'almost', 'nothing', 'achieved', 'wealth']\n",
            "Text after Lemmatization: ['tell', 'immigrant', 'came', 'america', 'almost', 'nothing', 'achieved', 'wealth']\n",
            "Final pre-processed text: tell immigrant came america almost nothing achieved wealth\n",
            "Text after removing HTML tags: russell also writes about how jewish and italian mobsters founded las vegas jewish copyright thieves founded hollywood and gay mafiosos helped spark the gay rights movement he thinks all of these things were good for society of course\n",
            "Text after removing non-alphabetic characters and converting to lowercase: russell also writes about how jewish and italian mobsters founded las vegas jewish copyright thieves founded hollywood and gay mafiosos helped spark the gay rights movement he thinks all of these things were good for society of course\n",
            "Text after tokenization: ['russell', 'also', 'writes', 'about', 'how', 'jewish', 'and', 'italian', 'mobsters', 'founded', 'las', 'vegas', 'jewish', 'copyright', 'thieves', 'founded', 'hollywood', 'and', 'gay', 'mafiosos', 'helped', 'spark', 'the', 'gay', 'rights', 'movement', 'he', 'thinks', 'all', 'of', 'these', 'things', 'were', 'good', 'for', 'society', 'of', 'course']\n",
            "Text after removing stop words: ['russell', 'also', 'writes', 'jewish', 'italian', 'mobsters', 'founded', 'las', 'vegas', 'jewish', 'copyright', 'thieves', 'founded', 'hollywood', 'gay', 'mafiosos', 'helped', 'spark', 'gay', 'rights', 'movement', 'thinks', 'things', 'good', 'society', 'course']\n",
            "Text after Lemmatization: ['russell', 'also', 'writes', 'jewish', 'italian', 'mobster', 'founded', 'la', 'vega', 'jewish', 'copyright', 'thief', 'founded', 'hollywood', 'gay', 'mafioso', 'helped', 'spark', 'gay', 'right', 'movement', 'think', 'thing', 'good', 'society', 'course']\n",
            "Final pre-processed text: russell also writes jewish italian mobster founded la vega jewish copyright thief founded hollywood gay mafioso helped spark gay right movement think thing good society course\n",
            "Text after removing HTML tags: there really is an international conspiracy to turn all white countries into non white countries they are increasingly open about it the neo nazi conspiracy theorists were not paranoid they were right all along\n",
            "Text after removing non-alphabetic characters and converting to lowercase: there really is an international conspiracy to turn all white countries into non white countries they are increasingly open about it the neo nazi conspiracy theorists were not paranoid they were right all along\n",
            "Text after tokenization: ['there', 'really', 'is', 'an', 'international', 'conspiracy', 'to', 'turn', 'all', 'white', 'countries', 'into', 'non', 'white', 'countries', 'they', 'are', 'increasingly', 'open', 'about', 'it', 'the', 'neo', 'nazi', 'conspiracy', 'theorists', 'were', 'not', 'paranoid', 'they', 'were', 'right', 'all', 'along']\n",
            "Text after removing stop words: ['really', 'international', 'conspiracy', 'turn', 'white', 'countries', 'non', 'white', 'countries', 'increasingly', 'open', 'neo', 'nazi', 'conspiracy', 'theorists', 'paranoid', 'right', 'along']\n",
            "Text after Lemmatization: ['really', 'international', 'conspiracy', 'turn', 'white', 'country', 'non', 'white', 'country', 'increasingly', 'open', 'neo', 'nazi', 'conspiracy', 'theorist', 'paranoid', 'right', 'along']\n",
            "Final pre-processed text: really international conspiracy turn white country non white country increasingly open neo nazi conspiracy theorist paranoid right along\n",
            "Text after removing HTML tags: wsh clearly shows that blacks enjoy watching blacks get the shit beat out of them\n",
            "Text after removing non-alphabetic characters and converting to lowercase: wsh clearly shows that blacks enjoy watching blacks get the shit beat out of them\n",
            "Text after tokenization: ['wsh', 'clearly', 'shows', 'that', 'blacks', 'enjoy', 'watching', 'blacks', 'get', 'the', 'shit', 'beat', 'out', 'of', 'them']\n",
            "Text after removing stop words: ['wsh', 'clearly', 'shows', 'blacks', 'enjoy', 'watching', 'blacks', 'get', 'shit', 'beat']\n",
            "Text after Lemmatization: ['wsh', 'clearly', 'show', 'black', 'enjoy', 'watching', 'black', 'get', 'shit', 'beat']\n",
            "Final pre-processed text: wsh clearly show black enjoy watching black get shit beat\n",
            "Text after removing HTML tags: it never really gone out of style it just got kept out of the usual places  a while tv media politics academia sand nigger sand groid dune coon dindu nuffin all valuable additions to the nigger lexicon oh sheeit dat remind me mexicon lexicon meh some beaners r ok\n",
            "Text after removing non-alphabetic characters and converting to lowercase: it never really gone out of style it just got kept out of the usual places  a while tv media politics academia sand nigger sand groid dune coon dindu nuffin all valuable additions to the nigger lexicon oh sheeit dat remind me mexicon lexicon meh some beaners r ok\n",
            "Text after tokenization: ['it', 'never', 'really', 'gone', 'out', 'of', 'style', 'it', 'just', 'got', 'kept', 'out', 'of', 'the', 'usual', 'places', 'a', 'while', 'tv', 'media', 'politics', 'academia', 'sand', 'nigger', 'sand', 'groid', 'dune', 'coon', 'dindu', 'nuffin', 'all', 'valuable', 'additions', 'to', 'the', 'nigger', 'lexicon', 'oh', 'sheeit', 'dat', 'remind', 'me', 'mexicon', 'lexicon', 'meh', 'some', 'beaners', 'r', 'ok']\n",
            "Text after removing stop words: ['never', 'really', 'gone', 'style', 'got', 'kept', 'usual', 'places', 'tv', 'media', 'politics', 'academia', 'sand', 'nigger', 'sand', 'groid', 'dune', 'coon', 'dindu', 'nuffin', 'valuable', 'additions', 'nigger', 'lexicon', 'oh', 'sheeit', 'dat', 'remind', 'mexicon', 'lexicon', 'meh', 'beaners', 'r', 'ok']\n",
            "Text after Lemmatization: ['never', 'really', 'gone', 'style', 'got', 'kept', 'usual', 'place', 'tv', 'medium', 'politics', 'academia', 'sand', 'nigger', 'sand', 'groid', 'dune', 'coon', 'dindu', 'nuffin', 'valuable', 'addition', 'nigger', 'lexicon', 'oh', 'sheeit', 'dat', 'remind', 'mexicon', 'lexicon', 'meh', 'beaner', 'r', 'ok']\n",
            "Final pre-processed text: never really gone style got kept usual place tv medium politics academia sand nigger sand groid dune coon dindu nuffin valuable addition nigger lexicon oh sheeit dat remind mexicon lexicon meh beaner r ok\n",
            "Text after removing HTML tags: this hit piece tries to paint anti hijra activist corcoran as a hypocrite since her parents are immigrants from ireland and germany and she has two adopted vietnamese children but are any of them trying to push for the erinization teutonization or vietization of america no\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this hit piece tries to paint anti hijra activist corcoran as a hypocrite since her parents are immigrants from ireland and germany and she has two adopted vietnamese children but are any of them trying to push for the erinization teutonization or vietization of america no\n",
            "Text after tokenization: ['this', 'hit', 'piece', 'tries', 'to', 'paint', 'anti', 'hijra', 'activist', 'corcoran', 'as', 'a', 'hypocrite', 'since', 'her', 'parents', 'are', 'immigrants', 'from', 'ireland', 'and', 'germany', 'and', 'she', 'has', 'two', 'adopted', 'vietnamese', 'children', 'but', 'are', 'any', 'of', 'them', 'trying', 'to', 'push', 'for', 'the', 'erinization', 'teutonization', 'or', 'vietization', 'of', 'america', 'no']\n",
            "Text after removing stop words: ['hit', 'piece', 'tries', 'paint', 'anti', 'hijra', 'activist', 'corcoran', 'hypocrite', 'since', 'parents', 'immigrants', 'ireland', 'germany', 'two', 'adopted', 'vietnamese', 'children', 'trying', 'push', 'erinization', 'teutonization', 'vietization', 'america']\n",
            "Text after Lemmatization: ['hit', 'piece', 'try', 'paint', 'anti', 'hijra', 'activist', 'corcoran', 'hypocrite', 'since', 'parent', 'immigrant', 'ireland', 'germany', 'two', 'adopted', 'vietnamese', 'child', 'trying', 'push', 'erinization', 'teutonization', 'vietization', 'america']\n",
            "Final pre-processed text: hit piece try paint anti hijra activist corcoran hypocrite since parent immigrant ireland germany two adopted vietnamese child trying push erinization teutonization vietization america\n",
            "Text after removing HTML tags: i was really happy when my wife told me she was pregnant and it was going to be a girl now i can stop all this gay shit with my son\n",
            "Text after removing non-alphabetic characters and converting to lowercase: i was really happy when my wife told me she was pregnant and it was going to be a girl now i can stop all this gay shit with my son\n",
            "Text after tokenization: ['i', 'was', 'really', 'happy', 'when', 'my', 'wife', 'told', 'me', 'she', 'was', 'pregnant', 'and', 'it', 'was', 'going', 'to', 'be', 'a', 'girl', 'now', 'i', 'can', 'stop', 'all', 'this', 'gay', 'shit', 'with', 'my', 'son']\n",
            "Text after removing stop words: ['really', 'happy', 'wife', 'told', 'pregnant', 'going', 'girl', 'stop', 'gay', 'shit', 'son']\n",
            "Text after Lemmatization: ['really', 'happy', 'wife', 'told', 'pregnant', 'going', 'girl', 'stop', 'gay', 'shit', 'son']\n",
            "Final pre-processed text: really happy wife told pregnant going girl stop gay shit son\n",
            "Text after removing HTML tags: jews white people need to be diluted so that cock sucking fucking jews can run the world they know that caucasians cannot be tamed\n",
            "Text after removing non-alphabetic characters and converting to lowercase: jews white people need to be diluted so that cock sucking fucking jews can run the world they know that caucasians cannot be tamed\n",
            "Text after tokenization: ['jews', 'white', 'people', 'need', 'to', 'be', 'diluted', 'so', 'that', 'cock', 'sucking', 'fucking', 'jews', 'can', 'run', 'the', 'world', 'they', 'know', 'that', 'caucasians', 'can', 'not', 'be', 'tamed']\n",
            "Text after removing stop words: ['jews', 'white', 'people', 'need', 'diluted', 'cock', 'sucking', 'fucking', 'jews', 'run', 'world', 'know', 'caucasians', 'tamed']\n",
            "Text after Lemmatization: ['jew', 'white', 'people', 'need', 'diluted', 'cock', 'sucking', 'fucking', 'jew', 'run', 'world', 'know', 'caucasian', 'tamed']\n",
            "Final pre-processed text: jew white people need diluted cock sucking fucking jew run world know caucasian tamed\n",
            "Text after removing HTML tags: why do you think all these various muslim groups from the middle east india and asscrackistan come here and insist their children become lawyers doctors politicians to better america and raise us up are you truly that stupid they are using our own system to subsume us wakeupamerica\n",
            "Text after removing non-alphabetic characters and converting to lowercase: why do you think all these various muslim groups from the middle east india and asscrackistan come here and insist their children become lawyers doctors politicians to better america and raise us up are you truly that stupid they are using our own system to subsume us wakeupamerica\n",
            "Text after tokenization: ['why', 'do', 'you', 'think', 'all', 'these', 'various', 'muslim', 'groups', 'from', 'the', 'middle', 'east', 'india', 'and', 'asscrackistan', 'come', 'here', 'and', 'insist', 'their', 'children', 'become', 'lawyers', 'doctors', 'politicians', 'to', 'better', 'america', 'and', 'raise', 'us', 'up', 'are', 'you', 'truly', 'that', 'stupid', 'they', 'are', 'using', 'our', 'own', 'system', 'to', 'subsume', 'us', 'wakeupamerica']\n",
            "Text after removing stop words: ['think', 'various', 'muslim', 'groups', 'middle', 'east', 'india', 'asscrackistan', 'come', 'insist', 'children', 'become', 'lawyers', 'doctors', 'politicians', 'better', 'america', 'raise', 'us', 'truly', 'stupid', 'using', 'system', 'subsume', 'us', 'wakeupamerica']\n",
            "Text after Lemmatization: ['think', 'various', 'muslim', 'group', 'middle', 'east', 'india', 'asscrackistan', 'come', 'insist', 'child', 'become', 'lawyer', 'doctor', 'politician', 'better', 'america', 'raise', 'u', 'truly', 'stupid', 'using', 'system', 'subsume', 'u', 'wakeupamerica']\n",
            "Final pre-processed text: think various muslim group middle east india asscrackistan come insist child become lawyer doctor politician better america raise u truly stupid using system subsume u wakeupamerica\n",
            "Text after removing HTML tags: this is pretty much the conclusion of the frankfurt school they said oh shit every time we jews stand out in white countries for the shitty things we do the whites notice and holocaust us only solution is to end white countries\n",
            "Text after removing non-alphabetic characters and converting to lowercase: this is pretty much the conclusion of the frankfurt school they said oh shit every time we jews stand out in white countries for the shitty things we do the whites notice and holocaust us only solution is to end white countries\n",
            "Text after tokenization: ['this', 'is', 'pretty', 'much', 'the', 'conclusion', 'of', 'the', 'frankfurt', 'school', 'they', 'said', 'oh', 'shit', 'every', 'time', 'we', 'jews', 'stand', 'out', 'in', 'white', 'countries', 'for', 'the', 'shitty', 'things', 'we', 'do', 'the', 'whites', 'notice', 'and', 'holocaust', 'us', 'only', 'solution', 'is', 'to', 'end', 'white', 'countries']\n",
            "Text after removing stop words: ['pretty', 'much', 'conclusion', 'frankfurt', 'school', 'said', 'oh', 'shit', 'every', 'time', 'jews', 'stand', 'white', 'countries', 'shitty', 'things', 'whites', 'notice', 'holocaust', 'us', 'solution', 'end', 'white', 'countries']\n",
            "Text after Lemmatization: ['pretty', 'much', 'conclusion', 'frankfurt', 'school', 'said', 'oh', 'shit', 'every', 'time', 'jew', 'stand', 'white', 'country', 'shitty', 'thing', 'white', 'notice', 'holocaust', 'u', 'solution', 'end', 'white', 'country']\n",
            "Final pre-processed text: pretty much conclusion frankfurt school said oh shit every time jew stand white country shitty thing white notice holocaust u solution end white country\n",
            "Text after removing HTML tags: graph straftaten gegen die sexuelle selbstbestimmung in deutschland von   nach pks des bka graph sexual harassment in germany from   by latest police statistic\n",
            "Text after removing non-alphabetic characters and converting to lowercase: graph straftaten gegen die sexuelle selbstbestimmung in deutschland von   nach pks des bka graph sexual harassment in germany from   by latest police statistic\n",
            "Text after tokenization: ['graph', 'straftaten', 'gegen', 'die', 'sexuelle', 'selbstbestimmung', 'in', 'deutschland', 'von', 'nach', 'pks', 'des', 'bka', 'graph', 'sexual', 'harassment', 'in', 'germany', 'from', 'by', 'latest', 'police', 'statistic']\n",
            "Text after removing stop words: ['graph', 'straftaten', 'gegen', 'die', 'sexuelle', 'selbstbestimmung', 'deutschland', 'von', 'nach', 'pks', 'des', 'bka', 'graph', 'sexual', 'harassment', 'germany', 'latest', 'police', 'statistic']\n",
            "Text after Lemmatization: ['graph', 'straftaten', 'gegen', 'die', 'sexuelle', 'selbstbestimmung', 'deutschland', 'von', 'nach', 'pks', 'de', 'bka', 'graph', 'sexual', 'harassment', 'germany', 'latest', 'police', 'statistic']\n",
            "Final pre-processed text: graph straftaten gegen die sexuelle selbstbestimmung deutschland von nach pks de bka graph sexual harassment germany latest police statistic\n",
            "Text after removing HTML tags: an afghani immigrant once told me that in afghanistan we do not believe in fences while our conversation was about livestock guardian dogs it now occurs to me what this really means is that there no belief in protection of persons or property\n",
            "Text after removing non-alphabetic characters and converting to lowercase: an afghani immigrant once told me that in afghanistan we do not believe in fences while our conversation was about livestock guardian dogs it now occurs to me what this really means is that there no belief in protection of persons or property\n",
            "Text after tokenization: ['an', 'afghani', 'immigrant', 'once', 'told', 'me', 'that', 'in', 'afghanistan', 'we', 'do', 'not', 'believe', 'in', 'fences', 'while', 'our', 'conversation', 'was', 'about', 'livestock', 'guardian', 'dogs', 'it', 'now', 'occurs', 'to', 'me', 'what', 'this', 'really', 'means', 'is', 'that', 'there', 'no', 'belief', 'in', 'protection', 'of', 'persons', 'or', 'property']\n",
            "Text after removing stop words: ['afghani', 'immigrant', 'told', 'afghanistan', 'believe', 'fences', 'conversation', 'livestock', 'guardian', 'dogs', 'occurs', 'really', 'means', 'belief', 'protection', 'persons', 'property']\n",
            "Text after Lemmatization: ['afghani', 'immigrant', 'told', 'afghanistan', 'believe', 'fence', 'conversation', 'livestock', 'guardian', 'dog', 'occurs', 'really', 'mean', 'belief', 'protection', 'person', 'property']\n",
            "Final pre-processed text: afghani immigrant told afghanistan believe fence conversation livestock guardian dog occurs really mean belief protection person property\n",
            "Text after removing HTML tags: if ur still on twitter tell carlton i said his alcoholic drug addicted teenage single mom slapped the stork that was forced to deliver his defective retarded stinky infant ass 😡 😡 😡\n",
            "Text after removing non-alphabetic characters and converting to lowercase: if ur still on twitter tell carlton i said his alcoholic drug addicted teenage single mom slapped the stork that was forced to deliver his defective retarded stinky infant ass      \n",
            "Text after tokenization: ['if', 'ur', 'still', 'on', 'twitter', 'tell', 'carlton', 'i', 'said', 'his', 'alcoholic', 'drug', 'addicted', 'teenage', 'single', 'mom', 'slapped', 'the', 'stork', 'that', 'was', 'forced', 'to', 'deliver', 'his', 'defective', 'retarded', 'stinky', 'infant', 'ass']\n",
            "Text after removing stop words: ['ur', 'still', 'twitter', 'tell', 'carlton', 'said', 'alcoholic', 'drug', 'addicted', 'teenage', 'single', 'mom', 'slapped', 'stork', 'forced', 'deliver', 'defective', 'retarded', 'stinky', 'infant', 'ass']\n",
            "Text after Lemmatization: ['ur', 'still', 'twitter', 'tell', 'carlton', 'said', 'alcoholic', 'drug', 'addicted', 'teenage', 'single', 'mom', 'slapped', 'stork', 'forced', 'deliver', 'defective', 'retarded', 'stinky', 'infant', 'as']\n",
            "Final pre-processed text: ur still twitter tell carlton said alcoholic drug addicted teenage single mom slapped stork forced deliver defective retarded stinky infant as\n",
            "CPU times: user 1min 24s, sys: 5.76 s, total: 1min 30s\n",
            "Wall time: 1min 34s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qne0bstpEGy4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "86a596d8-da75-44eb-ccd2-29b8b016de2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            Comment Text Comment Type  \\\n",
              "0      we cannot continue calling ourselves feminists...       normal   \n",
              "1                          nawt yall niggers ignoring me       normal   \n",
              "2      <user> i am bit confused coz chinese ppl can n...   hatespeech   \n",
              "3      and this is why i end up with nigger trainee d...   hatespeech   \n",
              "4                      nogs jews and dykes how enriching    offensive   \n",
              "...                                                  ...          ...   \n",
              "15378  this vile racist ghetto ass dumber than a reta...    offensive   \n",
              "15379  when i first got on here and said i hate trump...    offensive   \n",
              "15380  was macht der moslem wenn der zion gegen seine...       normal   \n",
              "15381  it is awful look at world demographics asians ...   hatespeech   \n",
              "15382  the jewish globalist elite have only imported ...    offensive   \n",
              "\n",
              "                                       preprocessed_text  \n",
              "0      continue calling feminist right womxn arent ad...  \n",
              "1                              nawt yall nigger ignoring  \n",
              "2      bit confused coz chinese ppl access twitter th...  \n",
              "3      end nigger trainee doctor speak properly lack ...  \n",
              "4                                 nog jew dyke enriching  \n",
              "...                                                  ...  \n",
              "15378  vile racist ghetto as dumber retarded tree sna...  \n",
              "15379  first got said hate trump considered troll peo...  \n",
              "15380  macht der moslem wenn der zion gegen seinen pr...  \n",
              "15381  awful look world demographic asian fucking eve...  \n",
              "15382  jewish globalist elite imported million muslim...  \n",
              "\n",
              "[15383 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-887d78c1-00ba-4d26-8ae2-e0bde375056b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Comment Text</th>\n",
              "      <th>Comment Type</th>\n",
              "      <th>preprocessed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>we cannot continue calling ourselves feminists...</td>\n",
              "      <td>normal</td>\n",
              "      <td>continue calling feminist right womxn arent ad...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>nawt yall niggers ignoring me</td>\n",
              "      <td>normal</td>\n",
              "      <td>nawt yall nigger ignoring</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>&lt;user&gt; i am bit confused coz chinese ppl can n...</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>bit confused coz chinese ppl access twitter th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>and this is why i end up with nigger trainee d...</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>end nigger trainee doctor speak properly lack ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>nogs jews and dykes how enriching</td>\n",
              "      <td>offensive</td>\n",
              "      <td>nog jew dyke enriching</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15378</th>\n",
              "      <td>this vile racist ghetto ass dumber than a reta...</td>\n",
              "      <td>offensive</td>\n",
              "      <td>vile racist ghetto as dumber retarded tree sna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15379</th>\n",
              "      <td>when i first got on here and said i hate trump...</td>\n",
              "      <td>offensive</td>\n",
              "      <td>first got said hate trump considered troll peo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15380</th>\n",
              "      <td>was macht der moslem wenn der zion gegen seine...</td>\n",
              "      <td>normal</td>\n",
              "      <td>macht der moslem wenn der zion gegen seinen pr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15381</th>\n",
              "      <td>it is awful look at world demographics asians ...</td>\n",
              "      <td>hatespeech</td>\n",
              "      <td>awful look world demographic asian fucking eve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15382</th>\n",
              "      <td>the jewish globalist elite have only imported ...</td>\n",
              "      <td>offensive</td>\n",
              "      <td>jewish globalist elite imported million muslim...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15383 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-887d78c1-00ba-4d26-8ae2-e0bde375056b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-887d78c1-00ba-4d26-8ae2-e0bde375056b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-887d78c1-00ba-4d26-8ae2-e0bde375056b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f77daf77-7d9a-4a28-9c90-c661f3bdbc44\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f77daf77-7d9a-4a28-9c90-c661f3bdbc44')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f77daf77-7d9a-4a28-9c90-c661f3bdbc44 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_31e11722-8267-4db8-9811-2c5110e29809\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_31e11722-8267-4db8-9811-2c5110e29809 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train",
              "summary": "{\n  \"name\": \"train\",\n  \"rows\": 15383,\n  \"fields\": [\n    {\n      \"column\": \"Comment Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15355,\n        \"samples\": [\n          \"this should not be called or even considered womensmarch real women do not lend themselves or rely on this new feminist bullshit its superflous judgejudy said it best when she owned katie couric these girls have no love let alone respect for themselves in return they recieve none slagfest\",\n          \"weev is a kike and ricky is a soy nigger\",\n          \"iranians and arabs lived in sweden when you were a kid i did not know thought i thought sweden was fairly homogenous until recently\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Comment Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"normal\",\n          \"hatespeech\",\n          \"offensive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preprocessed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15262,\n        \"samples\": [\n          \"twitter allow white person call jr smith nigger tonight probably great thing care much something jr smith screw second\",\n          \"could care le ideology pure white state form gov almost irrelevant care muh nigger equal punk faggot take cucks gtfo\",\n          \"yep moslem cock sucking labour mud force talking inciting religious hatred racial hatred video moslem show rampant religious hatred christian racial hatred white hate gay one word labour co rely moslem vote traitor\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode Comment Type labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train['label_encoded'] = label_encoder.fit_transform(train['Comment Type'])\n",
        "valid['label_encoded'] = label_encoder.transform(valid['Comment Type'])\n",
        "test['label_encoded'] = label_encoder.transform(test['Comment Type'])"
      ],
      "metadata": {
        "id": "isM_GOo5R7-Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word2Vec Embedding"
      ],
      "metadata": {
        "id": "HU4Q_UZNAEPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import Word2Vec\n",
        "# word2vec_path = ('/content/drive/MyDrive/GoogleNews-vectors-negative300 (1).bin.gz')\n",
        "import os\n",
        "\n",
        "word2vec_path = '/content/drive/MyDrive/GoogleNews-vectors-negative300 (1).bin.gz'\n",
        "\n",
        "if os.path.isfile(word2vec_path):\n",
        "    print(\"File exists and is accessible.\")\n",
        "else:\n",
        "    print(\"File does not exist or is not accessible.\")\n",
        "\n",
        "# def get_google_word2vec_embeddings(data, word2vec_path):\n",
        "#     # Load the Google News Word2Vec model\n",
        "#     word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "#     tokenized_sentences = [sentence.split() for sentence in data]\n",
        "#     embeddings = []\n",
        "\n",
        "#     for sentence in tokenized_sentences:\n",
        "#         sentence_embeddings = []\n",
        "#         for word in sentence:\n",
        "#             if word in word2vec_model:\n",
        "#                 sentence_embeddings.append(word2vec_model[word])\n",
        "#         if sentence_embeddings:\n",
        "#             embeddings.append(np.mean(sentence_embeddings, axis=0))\n",
        "#         else:\n",
        "#             embeddings.append(np.zeros(300))  # If no embeddings found, use zeros\n",
        "\n",
        "#     return np.array(embeddings)\n",
        "def get_embeddings(data, batch_size=100):\n",
        "  word2vec_path = '/content/drive/MyDrive/GoogleNews-vectors-negative300 (1).bin.gz'\n",
        "  try:\n",
        "    word2vec_model = KeyedVectors.load_word2vec_format(word2vec_path, binary=True)\n",
        "    print(\"Word2Vec model loaded successfully.\")\n",
        "  except Exception as e:\n",
        "    print(f\"Error loading Word2Vec model: {e}\")\n",
        "    return None\n",
        "\n",
        "  tokenized_sentences = [sentence.split() for sentence in data]\n",
        "  embeddings = []\n",
        "\n",
        "  for sentence in tokenized_sentences:\n",
        "      sentence_embeddings = []\n",
        "      for word in sentence:\n",
        "          if word in word2vec_model:\n",
        "              sentence_embeddings.append(word2vec_model[word])\n",
        "      if sentence_embeddings:\n",
        "          embeddings.append(np.mean(sentence_embeddings, axis=0))\n",
        "      else:\n",
        "          embeddings.append(np.zeros(300))\n",
        "\n",
        "  return np.array(embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG1jGV01GnvP",
        "outputId": "23e1fb72-fd0f-4dc0-ae2c-95c5908128f1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists and is accessible.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Generate input features using Word2Vec embeddings\n",
        "#X_train = get_embeddings(train['preprocessed_text'])\n",
        "#X_valid = get_embeddings(valid['preprocessed_text'])\n",
        "#X_test = get_embeddings(test['preprocessed_text'])\n",
        "#y_train = train['label_encoded'].values\n",
        "#y_valid = valid['label_encoded'].values\n",
        "#y_test = test['label_encoded'].values\n",
        "#X_train"
      ],
      "metadata": {
        "id": "MPBeQaBthY2N"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##CNN Model"
      ],
      "metadata": {
        "id": "oJ4LtsNqAMK4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_cnn_model(input_dim, num_classes):\n",
        "    model = Sequential([\n",
        "        Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(input_dim, 1)),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.3),\n",
        "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
        "        MaxPooling1D(pool_size=2),\n",
        "        Dropout(0.3),\n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "\n",
        "    return model\n",
        "\n",
        "def train_nn_with_word2vec(train, valid, test, word2vec_path, num_classes, epochs=20, batch_size=256):\n",
        "\n",
        "    # Generate input features using Word2Vec embeddings\n",
        "    X_train = get_embeddings(train['preprocessed_text'])\n",
        "    X_valid = get_embeddings(valid['preprocessed_text'])\n",
        "    X_test = get_embeddings(test['preprocessed_text'])\n",
        "\n",
        "    y_train = train['label_encoded'].values\n",
        "    y_valid = valid['label_encoded'].values\n",
        "    y_test = test['label_encoded'].values\n",
        "\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the validation and test data\n",
        "    X_valid_scaled = scaler.transform(X_valid)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    model = create_cnn_model(X_train_scaled.shape[1], num_classes)\n",
        "    print(model.summary() )\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Define callbacks to save the best model\n",
        "    checkpoint_callback = ModelCheckpoint('best_model_nn.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(X_train_scaled, y_train,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(X_valid_scaled, y_valid),\n",
        "                        callbacks=[checkpoint_callback])\n",
        "\n",
        "    # Load the best model\n",
        "    model.load_weights('best_model_nn.h5')\n",
        "\n",
        "    # Evaluate on test data\n",
        "    test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=1)\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "     # Predict on test data\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Calculate macro-F1 score\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    print(f'Macro-F1 Score: {macro_f1:.4f}')\n",
        "\n",
        "    return model, test_accuracy, macro_f1"
      ],
      "metadata": {
        "id": "48ZR2h5uekpa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model, test_accuracy, macro_f1 = train_nn_with_word2vec(train, valid, test, word2vec_path, num_classes=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rE6J5yiyeklh",
        "outputId": "66021a1d-432d-47a1-e562-bf8912b1005d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model loaded successfully.\n",
            "Word2Vec model loaded successfully.\n",
            "Word2Vec model loaded successfully.\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 298, 64)           256       \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1  (None, 149, 64)           0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 149, 64)           0         \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 147, 128)          24704     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPoolin  (None, 73, 128)           0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 73, 128)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 9344)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               4784640   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               32896     \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4974211 (18.98 MB)\n",
            "Trainable params: 4974211 (18.98 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.0601 - accuracy: 0.4469\n",
            "Epoch 1: val_loss improved from inf to 0.98823, saving model to best_model_nn.h5\n",
            "61/61 [==============================] - 34s 525ms/step - loss: 1.0601 - accuracy: 0.4469 - val_loss: 0.9882 - val_accuracy: 0.5291\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "61/61 [==============================] - ETA: 0s - loss: 0.9534 - accuracy: 0.5542\n",
            "Epoch 2: val_loss improved from 0.98823 to 0.93832, saving model to best_model_nn.h5\n",
            "61/61 [==============================] - 30s 485ms/step - loss: 0.9534 - accuracy: 0.5542 - val_loss: 0.9383 - val_accuracy: 0.5624\n",
            "Epoch 3/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.9175 - accuracy: 0.5791\n",
            "Epoch 3: val_loss improved from 0.93832 to 0.92579, saving model to best_model_nn.h5\n",
            "61/61 [==============================] - 30s 481ms/step - loss: 0.9175 - accuracy: 0.5791 - val_loss: 0.9258 - val_accuracy: 0.5734\n",
            "Epoch 4/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8933 - accuracy: 0.5953\n",
            "Epoch 4: val_loss improved from 0.92579 to 0.89968, saving model to best_model_nn.h5\n",
            "61/61 [==============================] - 30s 480ms/step - loss: 0.8933 - accuracy: 0.5953 - val_loss: 0.8997 - val_accuracy: 0.5864\n",
            "Epoch 5/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8641 - accuracy: 0.6097\n",
            "Epoch 5: val_loss did not improve from 0.89968\n",
            "61/61 [==============================] - 29s 478ms/step - loss: 0.8641 - accuracy: 0.6097 - val_loss: 0.9028 - val_accuracy: 0.5869\n",
            "Epoch 6/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8474 - accuracy: 0.6200\n",
            "Epoch 6: val_loss improved from 0.89968 to 0.88548, saving model to best_model_nn.h5\n",
            "61/61 [==============================] - 30s 494ms/step - loss: 0.8474 - accuracy: 0.6200 - val_loss: 0.8855 - val_accuracy: 0.6015\n",
            "Epoch 7/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8300 - accuracy: 0.6299\n",
            "Epoch 7: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 31s 503ms/step - loss: 0.8300 - accuracy: 0.6299 - val_loss: 0.8950 - val_accuracy: 0.5822\n",
            "Epoch 8/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8051 - accuracy: 0.6425\n",
            "Epoch 8: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 30s 499ms/step - loss: 0.8051 - accuracy: 0.6425 - val_loss: 0.8873 - val_accuracy: 0.5978\n",
            "Epoch 9/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7797 - accuracy: 0.6573\n",
            "Epoch 9: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 35s 580ms/step - loss: 0.7797 - accuracy: 0.6573 - val_loss: 0.8869 - val_accuracy: 0.5942\n",
            "Epoch 10/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7469 - accuracy: 0.6789\n",
            "Epoch 10: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 29s 477ms/step - loss: 0.7469 - accuracy: 0.6789 - val_loss: 0.8968 - val_accuracy: 0.5874\n",
            "Epoch 11/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7056 - accuracy: 0.6951\n",
            "Epoch 11: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 29s 480ms/step - loss: 0.7056 - accuracy: 0.6951 - val_loss: 0.9177 - val_accuracy: 0.5874\n",
            "Epoch 12/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6726 - accuracy: 0.7123\n",
            "Epoch 12: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 30s 482ms/step - loss: 0.6726 - accuracy: 0.7123 - val_loss: 0.9188 - val_accuracy: 0.5848\n",
            "Epoch 13/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6358 - accuracy: 0.7325\n",
            "Epoch 13: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 30s 494ms/step - loss: 0.6358 - accuracy: 0.7325 - val_loss: 0.9264 - val_accuracy: 0.5822\n",
            "Epoch 14/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5961 - accuracy: 0.7521\n",
            "Epoch 14: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 36s 597ms/step - loss: 0.5961 - accuracy: 0.7521 - val_loss: 0.9507 - val_accuracy: 0.5874\n",
            "Epoch 15/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7783\n",
            "Epoch 15: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 30s 494ms/step - loss: 0.5460 - accuracy: 0.7783 - val_loss: 1.0269 - val_accuracy: 0.5879\n",
            "Epoch 16/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5057 - accuracy: 0.7930\n",
            "Epoch 16: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 33s 543ms/step - loss: 0.5057 - accuracy: 0.7930 - val_loss: 1.0633 - val_accuracy: 0.5739\n",
            "Epoch 17/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.4672 - accuracy: 0.8135\n",
            "Epoch 17: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 30s 484ms/step - loss: 0.4672 - accuracy: 0.8135 - val_loss: 1.0442 - val_accuracy: 0.5848\n",
            "Epoch 18/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.4237 - accuracy: 0.8322\n",
            "Epoch 18: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 30s 485ms/step - loss: 0.4237 - accuracy: 0.8322 - val_loss: 1.1513 - val_accuracy: 0.5801\n",
            "Epoch 19/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.3895 - accuracy: 0.8450\n",
            "Epoch 19: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 30s 483ms/step - loss: 0.3895 - accuracy: 0.8450 - val_loss: 1.1674 - val_accuracy: 0.5614\n",
            "Epoch 20/20\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.3679 - accuracy: 0.8557\n",
            "Epoch 20: val_loss did not improve from 0.88548\n",
            "61/61 [==============================] - 30s 500ms/step - loss: 0.3679 - accuracy: 0.8557 - val_loss: 1.1812 - val_accuracy: 0.5583\n",
            "61/61 [==============================] - 1s 17ms/step - loss: 0.8815 - accuracy: 0.5899\n",
            "Test Loss: 0.8815\n",
            "Test Accuracy: 0.5899\n",
            "61/61 [==============================] - 1s 17ms/step\n",
            "Macro-F1 Score: 0.5450\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Calculate Intersection of Sentences\n",
        "\n",
        "def calculate_intersection(train, valid, test):\n",
        "    train_sentences = set(train['preprocessed_text'])\n",
        "    valid_sentences = set(valid['preprocessed_text'])\n",
        "    test_sentences = set(test['preprocessed_text'])\n",
        "\n",
        "    intersection_train = train_sentences.intersection(test_sentences)\n",
        "    intersection_valid = valid_sentences.intersection(test_sentences)\n",
        "\n",
        "    print(f'Number of common sentences between train and test sets: {len(intersection_train)}')\n",
        "    print(f'Number of common sentences between validation and test sets: {len(intersection_valid)}')\n",
        "\n",
        "# Calculate and print intersection\n",
        "calculate_intersection(train, valid, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKlvR6c1H9gk",
        "outputId": "b71ab930-78b9-435d-9ace-a275c9a4f03f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of common sentences between train and test sets: 20\n",
            "Number of common sentences between validation and test sets: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RNN Model"
      ],
      "metadata": {
        "id": "yPvTEdXEAb61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Preparing the data\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "\n",
        "# Tokenize text and convert to sequences\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train['preprocessed_text'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train['preprocessed_text'])\n",
        "X_valid = tokenizer.texts_to_sequences(valid['preprocessed_text'])\n",
        "X_test = tokenizer.texts_to_sequences(test['preprocessed_text'])\n",
        "\n",
        "# Pad sequences to max_seq_length\n",
        "X_train = pad_sequences(X_train, maxlen=max_len, padding='post', truncating='post')\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len, padding='post', truncating='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_len, padding='post', truncating='post')\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Transform the validation and test data\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = to_categorical(train['label_encoded'])\n",
        "y_valid = to_categorical(valid['label_encoded'])\n",
        "y_test = to_categorical(test['label_encoded'])"
      ],
      "metadata": {
        "id": "hIhx-3--Tdku"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label encode the target variable\n",
        "le = LabelEncoder()\n",
        "train['label_encoded'] = le.fit_transform(train['Comment Type'])\n",
        "valid['label_encoded'] = le.transform(valid['Comment Type'])\n",
        "test['label_encoded'] = le.transform(test['Comment Type'])\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train = np.array(train['label_encoded'])\n",
        "y_valid = np.array(valid['label_encoded'])\n",
        "y_test = np.array(test['label_encoded'])\n",
        "\n",
        "# Tokenize the data\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train['Comment Text'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train['Comment Text'])\n",
        "X_test = tokenizer.texts_to_sequences(test['Comment Text'])\n",
        "X_valid = tokenizer.texts_to_sequences(valid['Comment Text'])\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
        "# Initialize the scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#t and transform the training data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "#ansform the validation and test data\n",
        "X_valid_scaled = scaler.transform(X_valid)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "model.add(SimpleRNN(128))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set up early stopping\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train_scaled, to_categorical(y_train), validation_data=(X_valid_scaled, to_categorical(y_valid)),\n",
        "                    epochs=10, batch_size=256, callbacks=[early_stop])\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_scaled, to_categorical(y_test))\n",
        "\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I_euXyePgZ2",
        "outputId": "1bf4be55-7630-4902-e02d-dd7bb3b243a5"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "61/61 [==============================] - 35s 453ms/step - loss: 1.1013 - accuracy: 0.3806 - val_loss: 1.0874 - val_accuracy: 0.4053\n",
            "Epoch 2/10\n",
            "61/61 [==============================] - 17s 281ms/step - loss: 1.0908 - accuracy: 0.3990 - val_loss: 1.0884 - val_accuracy: 0.4053\n",
            "Epoch 3/10\n",
            "61/61 [==============================] - 18s 300ms/step - loss: 1.0870 - accuracy: 0.4054 - val_loss: 1.0893 - val_accuracy: 0.4006\n",
            "Epoch 4/10\n",
            "61/61 [==============================] - 18s 290ms/step - loss: 1.0870 - accuracy: 0.4013 - val_loss: 1.0946 - val_accuracy: 0.4058\n",
            "Epoch 5/10\n",
            "61/61 [==============================] - 18s 290ms/step - loss: 1.0858 - accuracy: 0.4045 - val_loss: 1.0887 - val_accuracy: 0.4027\n",
            "Epoch 6/10\n",
            "61/61 [==============================] - 17s 278ms/step - loss: 1.0847 - accuracy: 0.4069 - val_loss: 1.0883 - val_accuracy: 0.4037\n",
            "Epoch 7/10\n",
            "61/61 [==============================] - 17s 279ms/step - loss: 1.0851 - accuracy: 0.4067 - val_loss: 1.0874 - val_accuracy: 0.4037\n",
            "Epoch 8/10\n",
            "61/61 [==============================] - 18s 289ms/step - loss: 1.0836 - accuracy: 0.4086 - val_loss: 1.0892 - val_accuracy: 0.4022\n",
            "Epoch 9/10\n",
            "61/61 [==============================] - 18s 300ms/step - loss: 1.0853 - accuracy: 0.4045 - val_loss: 1.0884 - val_accuracy: 0.4037\n",
            "Epoch 10/10\n",
            "61/61 [==============================] - 18s 291ms/step - loss: 1.0855 - accuracy: 0.4051 - val_loss: 1.0885 - val_accuracy: 0.4011\n",
            "61/61 [==============================] - 1s 13ms/step - loss: 1.0878 - accuracy: 0.4018\n",
            "Test accuracy: 0.40176716446876526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wugtt75W4X_y",
        "outputId": "b87b0df0-1f39-45ca-be22-7820bf98880d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   0,    0,    0, ..., 6706,  865, 4818],\n",
              "       [   0,    0,    0, ...,   86, 2784,   49],\n",
              "       [   0,    0,    0, ...,  665,  665,  665],\n",
              "       ...,\n",
              "       [   0,    0,    0, ...,  441, 8589, 9990],\n",
              "       [   0,    0,    0, ...,  530,    9,  188],\n",
              "       [   0,    0,    0, ...,  881, 3481,  747]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Define RNN model\n",
        "def create_rnn_model(vocab_size, max_seq_length, num_classes):\n",
        "    model = Sequential([\n",
        "        Embedding(vocab_size, 300, input_length=max_seq_length),  # Using larger embedding size\n",
        "        LSTM(256, dropout=0.5, recurrent_dropout=0.3, return_sequences=True),  # Bidirectional LSTM with more units\n",
        "        LSTM(128, dropout=0.3, recurrent_dropout=0.2),  # Additional LSTM layer\n",
        "        Dense(128, activation='relu'),  # Increased dense layer size\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "# Create RNN model\n",
        "rnn_model = create_rnn_model(max_words, max_len, len(label_encoder.classes_))\n",
        "rnn_model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Define callbacks for early stopping and model checkpoint\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model_rnn.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# Train the model\n",
        "history = rnn_model.fit(X_train, y_train,\n",
        "                        epochs=10,\n",
        "                        batch_size=256,\n",
        "                        validation_data=(X_valid, y_valid),\n",
        "                        callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = rnn_model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "GBejKiZ6Paqd",
        "outputId": "83a9b555-2757-44a5-adcf-5bf10c5b590a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 2/61 [..............................] - ETA: 8:50 - loss: 1.0980 - accuracy: 0.3359 "
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-abac019913cc>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m history = rnn_model.fit(X_train, y_train,\n\u001b[0m\u001b[1;32m     34\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1805\u001b[0m                         ):\n\u001b[1;32m   1806\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1807\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1808\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    866\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    869\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1322\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1325\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1484\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1487\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(X_train)"
      ],
      "metadata": {
        "id": "VmTlDWPzK5Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Preparing the data\n",
        "max_words = 10000\n",
        "max_len = 100\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(train['preprocessed_text'])\n",
        "\n",
        "X_train = tokenizer.texts_to_sequences(train['preprocessed_text'])\n",
        "X_test = tokenizer.texts_to_sequences(test['preprocessed_text'])\n",
        "X_valid = tokenizer.texts_to_sequences(valid['preprocessed_text'])\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "X_valid = pad_sequences(X_valid, maxlen=max_len)\n",
        "\n",
        "y_train = to_categorical(train['label_encoded'])\n",
        "y_test = to_categorical(test['label_encoded'])\n",
        "y_valid = to_categorical(valid['label_encoded'])\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Bidirectional, LSTM, BatchNormalization\n",
        "\n",
        "# Build the model\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "#model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Bidirectional(LSTM(128)))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(128, activation='relu'))\n",
        "#model.add(Dropout(0.5))\n",
        "#model.add(Dense(3, activation='softmax'))\n",
        "#\n",
        "## Compile the model\n",
        "#model.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Build the model\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "#model.add(SimpleRNN(128))\n",
        "#model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "#model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 256, input_length=max_len))\n",
        "model.add(Bidirectional(SimpleRNN(128, return_sequences=True)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Bidirectional(SimpleRNN(128)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_valid, y_valid), epochs=10, batch_size=256)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "l5eUonVUQLJ0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}